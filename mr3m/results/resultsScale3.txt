Iteration 1, loss = 1436.74971868
Iteration 2, loss = 1294.20315675
Iteration 3, loss = 1236.23680824
Iteration 4, loss = 1199.69526401
Iteration 5, loss = 1177.85540180
Iteration 6, loss = 1165.24729328
Iteration 7, loss = 1158.65627985
Iteration 8, loss = 1155.31500860
Iteration 9, loss = 1153.77494169
Iteration 10, loss = 1153.14209770
Iteration 11, loss = 1152.91615590
Iteration 12, loss = 1152.81976904
Iteration 13, loss = 1152.77476982
Iteration 14, loss = 1152.76417094
Iteration 15, loss = 1152.77923639
Iteration 16, loss = 1152.77558113
Iteration 17, loss = 1152.78645704
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
{'ann': MLPRegressor(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, ... 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100),
       learning_rate='adaptive', learning_rate_init=0.001, max_iter=10000,
       momentum=0.9, nesterovs_momentum=True, power_t=0.5,
       random_state=None, shuffle=True, solver='adam', tol=1e-30,
       validation_fraction=0.1, verbose=True, warm_start=False), 'error': 1.5638191771636369, 'fit': -3.7768717486841297e-08, 'time': 901.2800000000001}
Iteration 1, loss = 17617.20134401
Iteration 2, loss = 16720.46968542
Iteration 3, loss = 16165.31410149
Iteration 4, loss = 15657.48069154
Iteration 5, loss = 15182.43381332
Iteration 6, loss = 14739.64050979
Iteration 7, loss = 14322.44056726
Iteration 8, loss = 13931.68269927
Iteration 9, loss = 13566.62654199
Iteration 10, loss = 13224.18888394
Iteration 11, loss = 12903.25449346
Iteration 12, loss = 12604.56510768
Iteration 13, loss = 12323.91476928
Iteration 14, loss = 12063.89084850
Iteration 15, loss = 11821.17215192
Iteration 16, loss = 11596.44101390
Iteration 17, loss = 11386.94497510
Iteration 18, loss = 11193.74132907
Iteration 19, loss = 11013.98157711
Iteration 20, loss = 10849.29197982
Iteration 21, loss = 10696.67367378
Iteration 22, loss = 10557.37969548
Iteration 23, loss = 10429.86622647
Iteration 24, loss = 10312.08343889
Iteration 25, loss = 10205.79862444
Iteration 26, loss = 10108.74917710
Iteration 27, loss = 10020.51095501
Iteration 28, loss = 9940.65060688
Iteration 29, loss = 9868.68163327
Iteration 30, loss = 9804.29540610
Iteration 31, loss = 9745.92361393
Iteration 32, loss = 9693.49979827
Iteration 33, loss = 9647.16223353
Iteration 34, loss = 9606.00906231
Iteration 35, loss = 9569.32137134
Iteration 36, loss = 9537.30028171
Iteration 37, loss = 9508.93412538
Iteration 38, loss = 9483.83820232
Iteration 39, loss = 9461.93650264
Iteration 40, loss = 9443.35204759
Iteration 41, loss = 9427.10332804
Iteration 42, loss = 9412.80069499
Iteration 43, loss = 9400.91236547
Iteration 44, loss = 9390.77179118
Iteration 45, loss = 9381.98908151
Iteration 46, loss = 9374.59396650
Iteration 47, loss = 9368.26378060
Iteration 48, loss = 9363.00761802
Iteration 49, loss = 9358.80648433
Iteration 50, loss = 9355.12497180
Iteration 51, loss = 9352.08022508
Iteration 52, loss = 9349.59410877
Iteration 53, loss = 9347.57157920
Iteration 54, loss = 9345.96854146
Iteration 55, loss = 9344.68048639
Iteration 56, loss = 9343.67357377
Iteration 57, loss = 9342.71941697
Iteration 58, loss = 9342.05432396
Iteration 59, loss = 9341.44577372
Iteration 60, loss = 9341.08549828
Iteration 61, loss = 9340.85639370
Iteration 62, loss = 9340.50132991
Iteration 63, loss = 9340.28181721
Iteration 64, loss = 9340.17635770
Iteration 65, loss = 9340.12222868
Iteration 66, loss = 9340.09378097
Iteration 67, loss = 9339.92449159
Iteration 68, loss = 9339.87486233
Iteration 69, loss = 9339.78785243
Iteration 70, loss = 9339.77440770
Iteration 71, loss = 9339.73505556
Iteratio