Iteration 1, loss = 6.53678989
Iteration 2, loss = 6.51065283
Iteration 3, loss = 6.48429015
Iteration 4, loss = 6.45702520
Iteration 5, loss = 6.42893690
Iteration 6, loss = 6.40051158
Iteration 7, loss = 6.37210160
Iteration 8, loss = 6.34414853
Iteration 9, loss = 6.31660154
Iteration 10, loss = 6.28976389
Iteration 11, loss = 6.26385153
Iteration 12, loss = 6.23868538
Iteration 13, loss = 6.21474163
Iteration 14, loss = 6.19200072
Iteration 15, loss = 6.17042666
Iteration 16, loss = 6.15003414
Iteration 17, loss = 6.13083768
Iteration 18, loss = 6.11277415
Iteration 19, loss = 6.09582237
Iteration 20, loss = 6.07976250
Iteration 21, loss = 6.06459270
Iteration 22, loss = 6.05041047
Iteration 23, loss = 6.03702877
Iteration 24, loss = 6.02429970
Iteration 25, loss = 6.01238159
Iteration 26, loss = 6.00101179
Iteration 27, loss = 5.99038617
Iteration 28, loss = 5.98023527
Iteration 29, loss = 5.97065244
Iteration 30, loss = 5.96162043
Iteration 31, loss = 5.95302386
Iteration 32, loss = 5.94490871
Iteration 33, loss = 5.93714041
Iteration 34, loss = 5.92980469
Iteration 35, loss = 5.92277656
Iteration 36, loss = 5.91608535
Iteration 37, loss = 5.90972520
Iteration 38, loss = 5.90362427
Iteration 39, loss = 5.89783061
Iteration 40, loss = 5.89217135
Iteration 41, loss = 5.88682277
Iteration 42, loss = 5.88163236
Iteration 43, loss = 5.87665456
Iteration 44, loss = 5.87190919
Iteration 45, loss = 5.86730590
Iteration 46, loss = 5.86286212
Iteration 47, loss = 5.85857515
Iteration 48, loss = 5.85445449
Iteration 49, loss = 5.85047234
Iteration 50, loss = 5.84660129
Iteration 51, loss = 5.84288651
Iteration 52, loss = 5.83925527
Iteration 53, loss = 5.83582526
Iteration 54, loss = 5.83242446
Iteration 55, loss = 5.82908141
Iteration 56, loss = 5.82592125
Iteration 57, loss = 5.82286458
Iteration 58, loss = 5.81990395
Iteration 59, loss = 5.81697710
Iteration 60, loss = 5.81412741
Iteration 61, loss = 5.81143068
Iteration 62, loss = 5.80877686
Iteration 63, loss = 5.80618747
Iteration 64, loss = 5.80366785
Iteration 65, loss = 5.80120759
Iteration 66, loss = 5.79881760
Iteration 67, loss = 5.79649400
Iteration 68, loss = 5.79424280
Iteration 69, loss = 5.79215462
Iteration 70, loss = 5.78997107
Iteration 71, loss = 5.78789603
Iteration 72, loss = 5.78590787
Iteration 73, loss = 5.78390764
Iteration 74, loss = 5.78205199
Iteration 75, loss = 5.78015828
Iteration 76, loss = 5.77834055
Iteration 77, loss = 5.77659567
Iteration 78, loss = 5.77487891
Iteration 79, loss = 5.77317803
Iteration 80, loss = 5.77164122
Iteration 81, loss = 5.76997032
Iteration 82, loss = 5.76842428
Iteration 83, loss = 5.76691669
Iteration 84, loss = 5.76544080
Iteration 85, loss = 5.76401934
Iteration 86, loss = 5.76260338
Iteration 87, loss = 5.76126036
Iteration 88, loss = 5.76002662
Iteration 89, loss = 5.75867104
Iteration 90, loss = 5.75738584
Iteration 91, loss = 5.75614566
Iteration 92, loss = 5.75493737
Iteration 93, loss = 5.75374870
Iteration 94, loss = 5.75252680
Iteration 95, loss = 5.75144684
Iteration 96, loss = 5.75032416
Iteration 97, loss = 5.74925104
Iteration 98, loss = 5.74825308
Iteration 99, loss = 5.74720909
Iteration 100, loss = 5.74620230
Iteration 101, loss = 5.74519515
Iteration 102, loss = 5.74424643
Iteration 103, loss = 5.74325332
Iteration 104, loss = 5.74231250
Iteration 105, loss = 5.74145688
Iteration 106, loss = 5.74056385
Iteration 107, loss = 5.73967337
Iteration 108, loss = 5.73882089
Iteration 109, loss = 5.73798895
Iteration 110, loss = 5.73724410
Iteration 111, loss = 5.73636619
Iteration 112, loss = 5.73557504
Iteration 113, loss = 5.73485661
Iteration 114, loss = 5.73402186
Iteration 115, loss = 5.73329892
Iteration 116, loss = 5.73254392
Iteration 117, loss = 5.73184570
Iteration 118, loss = 5.73114339
Iteration 119, loss = 5.73047765
Iteration 120, loss = 5.72974558
Iteration 121, loss = 5.72908379
Iteration 122, loss = 5.72843401
Iteration 123, loss = 5.72779280
((1,), {'error': 2.5817075017450386, 'fit': 0.019625, 'time': 153.12})
Iteration 1, loss = 6.53513651
Iteration 2, loss = 6.48072119
Iteration 3, loss = 6.41851872
Iteration 4, loss = 6.34258639
Iteration 5, loss = 6.25364262
Iteration 6, loss = 6.15667216
Iteration 7, loss = 6.06328838
Iteration 8, loss = 5.98253571
Iteration 9, loss = 5.91725384
Iteration 10, loss = 5.86745773
Iteration 11, loss = 5.82922424
Iteration 12, loss = 5.79908719
Iteration 13, loss = 5.77402564
Iteration 14, loss = 5.75256762
Iteration 15, loss = 5.73346546
Iteration 16, loss = 5.71587675
Iteration 17, loss = 5.69963918
Iteration 18, loss = 5.68453504
Iteration 19, loss = 5.67006649
Iteration 20, loss = 5.65652759
Iteration 21, loss = 5.64338507
Iteration 22, loss = 5.63085355
Iteration 23, loss = 5.61881672
Iteration 24, loss = 5.60734199
Iteration 25, loss = 5.59620566
Iteration 26, loss = 5.58573631
Iteration 27, loss = 5.57554216
Iteration 28, loss = 5.56583989
Iteration 29, loss = 5.55684586
Iteration 30, loss = 5.54787098
Iteration 31, loss = 5.53930117
Iteration 32, loss = 5.53124023
Iteration 33, loss = 5.52334924
Iteration 34, loss = 5.51560606
Iteration 35, loss = 5.50822815
Iteration 36, loss = 5.50131062
Iteration 37, loss = 5.49454221
Iteration 38, loss = 5.48796282
Iteration 39, loss = 5.48147504
Iteration 40, loss = 5.47522076
Iteration 41, loss = 5.46923919
Iteration 42, loss = 5.46350604
Iteration 43, loss = 5.45772271
Iteration 44, loss = 5.45233311
Iteration 45, loss = 5.44675699
Iteration 46, loss = 5.44170088
Iteration 47, loss = 5.43655106
Iteration 48, loss = 5.43169565
Iteration 49, loss = 5.42655109
Iteration 50, loss = 5.42190670
Iteration 51, loss = 5.41721794
Iteration 52, loss = 5.41279953
Iteration 53, loss = 5.40832702
Iteration 54, loss = 5.40392433
Iteration 55, loss = 5.39972022
Iteration 56, loss = 5.39552908
Iteration 57, loss = 5.39148020
Iteration 58, loss = 5.38748782
Iteration 59, loss = 5.38373630
Iteration 60, loss = 5.37968069
Iteration 61, loss = 5.37608826
Iteration 62, loss = 5.37243205
Iteration 63, loss = 5.36891550
Iteration 64, loss = 5.36531629
Iteration 65, loss = 5.36177080
Iteration 66, loss = 5.35833580
Iteration 67, loss = 5.35495719
Iteration 68, loss = 5.35178255
Iteration 69, loss = 5.34853085
Iteration 70, loss = 5.34543222
Iteration 71, loss = 5.34221589
Iteration 72, loss = 5.33930599
Iteration 73, loss = 5.33638869
Iteration 74, loss = 5.33339878
Iteration 75, loss = 5.33067859
Iteration 76, loss = 5.32746309
Iteration 77, loss = 5.32484595
Iteration 78, loss = 5.32209756
Iteration 79, loss = 5.31933511
Iteration 80, loss = 5.31668607
Iteration 81, loss = 5.31401328
Iteration 82, loss = 5.31149196
Iteration 83, loss = 5.30906594
Iteration 84, loss = 5.30651058
Iteration 85, loss = 5.30405845
Iteration 86, loss = 5.30145906
Iteration 87, loss = 5.29915661
Iteration 88, loss = 5.29683467
Iteration 89, loss = 5.29472032
Iteration 90, loss = 5.29211360
Iteration 91, loss = 5.28985175
Iteration 92, loss = 5.28781205
Iteration 93, loss = 5.28548116
Iteration 94, loss = 5.28336681
Iteration 95, loss = 5.28113064
Iteration 96, loss = 5.27914278
Iteration 97, loss = 5.27696226
Iteration 98, loss = 5.27496112
Iteration 99, loss = 5.27287400
Iteration 100, loss = 5.27089625
Iteration 101, loss = 5.26891630
Iteration 102, loss = 5.26689874
Iteration 103, loss = 5.26497739
Iteration 104, loss = 5.26300007
Iteration 105, loss = 5.26109980
Iteration 106, loss = 5.25919788
Iteration 107, loss = 5.25741658
Iteration 108, loss = 5.25561859
Iteration 109, loss = 5.25403021
Iteration 110, loss = 5.25208607
Iteration 111, loss = 5.25016340
Iteration 112, loss = 5.24838576
Iteration 113, loss = 5.24693488
Iteration 114, loss = 5.24485017
Iteration 115, loss = 5.24336865
Iteration 116, loss = 5.24154233
Iteration 117, loss = 5.24003226
Iteration 118, loss = 5.23834939
Iteration 119, loss = 5.23671793
Iteration 120, loss = 5.23498374
Iteration 121, loss = 5.23346426
Iteration 122, loss = 5.23206776
Iteration 123, loss = 5.23023105
Iteration 124, loss = 5.22888452
Iteration 125, loss = 5.22723005
Iteration 126, loss = 5.22575615
Iteration 127, loss = 5.22426806
Iteration 128, loss = 5.22287194
Iteration 129, loss = 5.22136953
Iteration 130, loss = 5.21997535
Iteration 131, loss = 5.21841012
Iteration 132, loss = 5.21713719
Iteration 133, loss = 5.21568919
Iteration 134, loss = 5.21420997
Iteration 135, loss = 5.21289322
Iteration 136, loss = 5.21138002
Iteration 137, loss = 5.21014982
Iteration 138, loss = 5.20875279
Iteration 139, loss = 5.20760695
Iteration 140, loss = 5.20602840
Iteration 141, loss = 5.20484835
Iteration 142, loss = 5.20350031
Iteration 143, loss = 5.20212962
Iteration 144, loss = 5.20096160
Iteration 145, loss = 5.19965760
Iteration 146, loss = 5.19840779
Iteration 147, loss = 5.19721233
Iteration 148, loss = 5.19598245
Iteration 149, loss = 5.19477426
Iteration 150, loss = 5.19354251
Iteration 151, loss = 5.19236881
Iteration 152, loss = 5.19123633
Iteration 153, loss = 5.19007292
Iteration 154, loss = 5.18876475
Iteration 155, loss = 5.18765616
Iteration 156, loss = 5.18668869
Iteration 157, loss = 5.18551554
Iteration 158, loss = 5.18429886
Iteration 159, loss = 5.18328742
Iteration 160, loss = 5.18204223
Iteration 161, loss = 5.18113466
Iteration 162, loss = 5.17991484
Iteration 163, loss = 5.17880037
Iteration 164, loss = 5.17786889
Iteration 165, loss = 5.17665382
Iteration 166, loss = 5.17568173
Iteration 167, loss = 5.17445678
Iteration 168, loss = 5.17366412
Iteration 169, loss = 5.17251805
Iteration 170, loss = 5.17158000
Iteration 171, loss = 5.17041144
Iteration 172, loss = 5.16942405
Iteration 173, loss = 5.16842568
Iteration 174, loss = 5.16746410
Iteration 175, loss = 5.16651717
Iteration 176, loss = 5.16552568
Iteration 177, loss = 5.16456571
Iteration 178, loss = 5.16342620
Iteration 179, loss = 5.16254068
Iteration 180, loss = 5.16166130
Iteration 181, loss = 5.16075631
Iteration 182, loss = 5.15987750
Iteration 183, loss = 5.15885282
Iteration 184, loss = 5.15800602
Iteration 185, loss = 5.15699636
Iteration 186, loss = 5.15616547
Iteration 187, loss = 5.15539187
Iteration 188, loss = 5.15446411
Iteration 189, loss = 5.15353222
Iteration 190, loss = 5.15262771
Iteration 191, loss = 5.15163601
Iteration 192, loss = 5.15080029
Iteration 193, loss = 5.15023466
Iteration 194, loss = 5.14927655
Iteration 195, loss = 5.14836561
Iteration 196, loss = 5.14745651
Iteration 197, loss = 5.14663656
Iteration 198, loss = 5.14586625
Iteration 199, loss = 5.14492964
Iteration 200, loss = 5.14428527
Iteration 201, loss = 5.14335277
Iteration 202, loss = 5.14252411
Iteration 203, loss = 5.14181313
Iteration 204, loss = 5.14109747
Iteration 205, loss = 5.14021097
Iteration 206, loss = 5.13941554
Iteration 207, loss = 5.13872993
Iteration 208, loss = 5.13795216
Iteration 209, loss = 5.13709998
Iteration 210, loss = 5.13639237
Iteration 211, loss = 5.13564784
Iteration 212, loss = 5.13466562
Iteration 213, loss = 5.13406814
Iteration 214, loss = 5.13343303
Iteration 215, loss = 5.13256726
Iteration 216, loss = 5.13189346
Iteration 217, loss = 5.13132641
Iteration 218, loss = 5.13062503
Iteration 219, loss = 5.12969889
Iteration 220, loss = 5.12904156
Iteration 221, loss = 5.12825163
Iteration 222, loss = 5.12765872
Iteration 223, loss = 5.12685002
Iteration 224, loss = 5.12618721
Iteration 225, loss = 5.12560811
Iteration 226, loss = 5.12475507
Iteration 227, loss = 5.12401403
Iteration 228, loss = 5.12343075
Iteration 229, loss = 5.12270091
Iteration 230, loss = 5.12197739
Iteration 231, loss = 5.12132585
Iteration 232, loss = 5.12085425
Iteration 233, loss = 5.12023799
Iteration 234, loss = 5.11948815
Iteration 235, loss = 5.11883822
Iteration 236, loss = 5.11824087
Iteration 237, loss = 5.11753799
Iteration 238, loss = 5.11687980
Iteration 239, loss = 5.11639161
Iteration 240, loss = 5.11568111
Iteration 241, loss = 5.11507456
Iteration 242, loss = 5.11436802
Iteration 243, loss = 5.11383446
Iteration 244, loss = 5.11323421
Iteration 245, loss = 5.11254033
Iteration 246, loss = 5.11206249
Iteration 247, loss = 5.11146190
Iteration 248, loss = 5.11081892
Iteration 249, loss = 5.11004633
Iteration 250, loss = 5.10951228
Iteration 251, loss = 5.10903689
Iteration 252, loss = 5.10837458
Iteration 253, loss = 5.10792571
Iteration 254, loss = 5.10714914
Iteration 255, loss = 5.10690152
Iteration 256, loss = 5.10602026
Iteration 257, loss = 5.10552791
Iteration 258, loss = 5.10501178
Iteration 259, loss = 5.10437945
Iteration 260, loss = 5.10391370
Iteration 261, loss = 5.10336885
Iteration 262, loss = 5.10288080
Iteration 263, loss = 5.10219796
Iteration 264, loss = 5.10165687
Iteration 265, loss = 5.10111891
Iteration 266, loss = 5.10055205
Iteration 267, loss = 5.10018085
Iteration 268, loss = 5.09958703
Iteration 269, loss = 5.09897650
Iteration 270, loss = 5.09848014
Iteration 271, loss = 5.09812251
Iteration 272, loss = 5.09756789
Iteration 273, loss = 5.09700220
Iteration 274, loss = 5.09648612
Iteration 275, loss = 5.09590701
Iteration 276, loss = 5.09545230
Iteration 277, loss = 5.09491850
Iteration 278, loss = 5.09446783
Iteration 279, loss = 5.09392663
Iteration 280, loss = 5.09362000
Iteration 281, loss = 5.09300265
Iteration 282, loss = 5.09241454
Iteration 283, loss = 5.09193911
Iteration 284, loss = 5.09147554
Iteration 285, loss = 5.09096098
Iteration 286, loss = 5.09054743
Iteration 287, loss = 5.09002939
Iteration 288, loss = 5.08971385
Iteration 289, loss = 5.08925906
Iteration 290, loss = 5.08871330
Iteration 291, loss = 5.08820488
Iteration 292, loss = 5.08785479
Iteration 293, loss = 5.08722608
Iteration 294, loss = 5.08669889
Iteration 295, loss = 5.08648913
Iteration 296, loss = 5.08577511
Iteration 297, loss = 5.08538244
Iteration 298, loss = 5.08482246
Iteration 299, loss = 5.08463481
Iteration 300, loss = 5.08409118
Iteration 301, loss = 5.08358338
Iteration 302, loss = 5.08324323
Iteration 303, loss = 5.08281431
Iteration 304, loss = 5.08244344
Iteration 305, loss = 5.08188455
Iteration 306, loss = 5.08144029
Iteration 307, loss = 5.08095638
Iteration 308, loss = 5.08058261
Iteration 309, loss = 5.08001416
Iteration 310, loss = 5.07972547
Iteration 311, loss = 5.07923388
Iteration 312, loss = 5.07896613
Iteration 313, loss = 5.07855093
Iteration 314, loss = 5.07814630
Iteration 315, loss = 5.07778824
Iteration 316, loss = 5.07728736
Iteration 317, loss = 5.07676938
Iteration 318, loss = 5.07646314
Iteration 319, loss = 5.07609815
Iteration 320, loss = 5.07560351
Iteration 321, loss = 5.07536592
Iteration 322, loss = 5.07479831
Iteration 323, loss = 5.07445327
Iteration 324, loss = 5.07422003
Iteration 325, loss = 5.07364471
Iteration 326, loss = 5.07319675
Iteration 327, loss = 5.07276631
Iteration 328, loss = 5.07265048
Iteration 329, loss = 5.07216541
Iteration 330, loss = 5.07187527
Iteration 331, loss = 5.07141153
Iteration 332, loss = 5.07117675
Iteration 333, loss = 5.07068869
Iteration 334, loss = 5.07018001
Iteration 335, loss = 5.06984489
Iteration 336, loss = 5.06955719
Iteration 337, loss = 5.06905913
Iteration 338, loss = 5.06875552
Iteration 339, loss = 5.06833334
Iteration 340, loss = 5.06819346
Iteration 341, loss = 5.06770868
Iteration 342, loss = 5.06730523
Iteration 343, loss = 5.06698919
Iteration 344, loss = 5.06657913
Iteration 345, loss = 5.06636483
Iteration 346, loss = 5.06587381
Iteration 347, loss = 5.06553230
Iteration 348, loss = 5.06508178
Iteration 349, loss = 5.06484407
Iteration 350, loss = 5.06445158
Iteration 351, loss = 5.06409087
Iteration 352, loss = 5.06385187
Iteration 353, loss = 5.06336919
Iteration 354, loss = 5.06311709
Iteration 355, loss = 5.06266563
Iteration 356, loss = 5.06250767
Iteration 357, loss = 5.06212047
Iteration 358, loss = 5.06167013
Iteration 359, loss = 5.06145902
Iteration 360, loss = 5.06116988
Iteration 361, loss = 5.06079481
Iteration 362, loss = 5.06060691
Iteration 363, loss = 5.05995690
Iteration 364, loss = 5.05980880
Iteration 365, loss = 5.05958526
Iteration 366, loss = 5.05900452
Iteration 367, loss = 5.05894244
Iteration 368, loss = 5.05844649
Iteration 369, loss = 5.05804798
Iteration 370, loss = 5.05788160
Iteration 371, loss = 5.05747579
Iteration 372, loss = 5.05711711
Iteration 373, loss = 5.05688555
Iteration 374, loss = 5.05655463
Iteration 375, loss = 5.05622930
Iteration 376, loss = 5.05606238
Iteration 377, loss = 5.05566863
Iteration 378, loss = 5.05532679
Iteration 379, loss = 5.05503821
Iteration 380, loss = 5.05471653
Iteration 381, loss = 5.05466812
Iteration 382, loss = 5.05420396
Iteration 383, loss = 5.05390887
Iteration 384, loss = 5.05352313
Iteration 385, loss = 5.05327195
Iteration 386, loss = 5.05287770
Iteration 387, loss = 5.05266494
Iteration 388, loss = 5.05239364
Iteration 389, loss = 5.05217040
Iteration 390, loss = 5.05184185
Iteration 391, loss = 5.05160252
Iteration 392, loss = 5.05131196
Iteration 393, loss = 5.05107490
Iteration 394, loss = 5.05056163
Iteration 395, loss = 5.05033618
Iteration 396, loss = 5.05030555
Iteration 397, loss = 5.04981791
Iteration 398, loss = 5.04953713
Iteration 399, loss = 5.04914064
Iteration 400, loss = 5.04897802
Iteration 401, loss = 5.04860503
Iteration 402, loss = 5.04840904
Iteration 403, loss = 5.04814673
Iteration 404, loss = 5.04778777
Iteration 405, loss = 5.04754744
Iteration 406, loss = 5.04718658
Iteration 407, loss = 5.04700399
Iteration 408, loss = 5.04670606
Iteration 409, loss = 5.04652985
Iteration 410, loss = 5.04618800
Iteration 411, loss = 5.04601244
Iteration 412, loss = 5.04562291
Iteration 413, loss = 5.04527210
Iteration 414, loss = 5.04514095
Iteration 415, loss = 5.04485351
Iteration 416, loss = 5.04505690
Iteration 417, loss = 5.04437899
Iteration 418, loss = 5.04408884
Iteration 419, loss = 5.04399097
Iteration 420, loss = 5.04363171
Iteration 421, loss = 5.04325657
Iteration 422, loss = 5.04294424
Iteration 423, loss = 5.04275749
Iteration 424, loss = 5.04254577
Iteration 425, loss = 5.04230045
Iteration 426, loss = 5.04218953
Iteration 427, loss = 5.04185037
Iteration 428, loss = 5.04165567
Iteration 429, loss = 5.04120271
Iteration 430, loss = 5.04120018
Iteration 431, loss = 5.04083136
Iteration 432, loss = 5.04056032
Iteration 433, loss = 5.04025408
Iteration 434, loss = 5.04002625
Iteration 435, loss = 5.03980159
Iteration 436, loss = 5.03950580
Iteration 437, loss = 5.03955642
Iteration 438, loss = 5.03911090
Iteration 439, loss = 5.03889748
Iteration 440, loss = 5.03868359
Iteration 441, loss = 5.03827024
Iteration 442, loss = 5.03807116
Iteration 443, loss = 5.03795920
Iteration 444, loss = 5.03771878
Iteration 445, loss = 5.03738525
Iteration 446, loss = 5.03721619
Iteration 447, loss = 5.03697250
Iteration 448, loss = 5.03669413
Iteration 449, loss = 5.03650071
Iteration 450, loss = 5.03636778
Iteration 451, loss = 5.03627703
Iteration 452, loss = 5.03585700
Iteration 453, loss = 5.03557511
Iteration 454, loss = 5.03532822
Iteration 455, loss = 5.03508033
Iteration 456, loss = 5.03490940
Iteration 457, loss = 5.03467964
Iteration 458, loss = 5.03436928
Iteration 459, loss = 5.03418655
Iteration 460, loss = 5.03389650
Iteration 461, loss = 5.03379871
Iteration 462, loss = 5.03367528
Iteration 463, loss = 5.03336689
Iteration 464, loss = 5.03317201
Iteration 465, loss = 5.03287082
Iteration 466, loss = 5.03268460
Iteration 467, loss = 5.03251489
Iteration 468, loss = 5.03233075
Iteration 469, loss = 5.03201828
Iteration 470, loss = 5.03174059
Iteration 471, loss = 5.03149167
Iteration 472, loss = 5.03147609
Iteration 473, loss = 5.03122101
Iteration 474, loss = 5.03087146
Iteration 475, loss = 5.03075634
Iteration 476, loss = 5.03057214
Iteration 477, loss = 5.03043924
Iteration 478, loss = 5.03018069
Iteration 479, loss = 5.02993070
Iteration 480, loss = 5.02958268
Iteration 481, loss = 5.02968842
Iteration 482, loss = 5.02928149
Iteration 483, loss = 5.02915588
Iteration 484, loss = 5.02882263
Iteration 485, loss = 5.02863483
Iteration 486, loss = 5.02868457
Iteration 487, loss = 5.02833736
Iteration 488, loss = 5.02794736
Iteration 489, loss = 5.02810248
Iteration 490, loss = 5.02771636
Iteration 491, loss = 5.02751820
Iteration 492, loss = 5.02722559
Iteration 493, loss = 5.02712948
Iteration 494, loss = 5.02693370
Iteration 495, loss = 5.02675941
Iteration 496, loss = 5.02656974
Iteration 497, loss = 5.02638446
Iteration 498, loss = 5.02614170
Iteration 499, loss = 5.02591630
Iteration 500, loss = 5.02588074
Iteration 501, loss = 5.02557091
Iteration 502, loss = 5.02540691
Iteration 503, loss = 5.02507911
Iteration 504, loss = 5.02499396
Iteration 505, loss = 5.02471073
Iteration 506, loss = 5.02463283
Iteration 507, loss = 5.02439439
Iteration 508, loss = 5.02423391
Iteration 509, loss = 5.02399370
Iteration 510, loss = 5.02390481
Iteration 511, loss = 5.02366234
Iteration 512, loss = 5.02343703
Iteration 513, loss = 5.02308758
Iteration 514, loss = 5.02312453
Iteration 515, loss = 5.02301710
Iteration 516, loss = 5.02280335
Iteration 517, loss = 5.02251804
Iteration 518, loss = 5.02225771
Iteration 519, loss = 5.02226583
Iteration 520, loss = 5.02191163
Iteration 521, loss = 5.02197934
Iteration 522, loss = 5.02176233
Iteration 523, loss = 5.02140639
Iteration 524, loss = 5.02115536
Iteration 525, loss = 5.02118947
Iteration 526, loss = 5.02089532
Iteration 527, loss = 5.02072970
Iteration 528, loss = 5.02063714
Iteration 529, loss = 5.02046028
Iteration 530, loss = 5.02008885
Iteration 531, loss = 5.02006211
Iteration 532, loss = 5.01997941
Iteration 533, loss = 5.01972885
Iteration 534, loss = 5.01949012
Iteration 535, loss = 5.01940855
Iteration 536, loss = 5.01905057
Iteration 537, loss = 5.01903860
Iteration 538, loss = 5.01888170
Iteration 539, loss = 5.01865189
Iteration 540, loss = 5.01857089
Iteration 541, loss = 5.01834984
Iteration 542, loss = 5.01808700
Iteration 543, loss = 5.01798822
Iteration 544, loss = 5.01776846
Iteration 545, loss = 5.01768104
Iteration 546, loss = 5.01755554
Iteration 547, loss = 5.01742432
Iteration 548, loss = 5.01703146
Iteration 549, loss = 5.01683716
Iteration 550, loss = 5.01683095
Iteration 551, loss = 5.01681554
Iteration 552, loss = 5.01655527
Iteration 553, loss = 5.01640216
Iteration 554, loss = 5.01603261
Iteration 555, loss = 5.01609713
Iteration 556, loss = 5.01585855
Iteration 557, loss = 5.01572572
Iteration 558, loss = 5.01558095
Iteration 559, loss = 5.01525345
Iteration 560, loss = 5.01528810
Iteration 561, loss = 5.01510613
Iteration 562, loss = 5.01488309
Iteration 563, loss = 5.01471162
Iteration 564, loss = 5.01457276
Iteration 565, loss = 5.01445639
Iteration 566, loss = 5.01418932
Iteration 567, loss = 5.01409037
Iteration 568, loss = 5.01410452
Iteration 569, loss = 5.01369510
Iteration 570, loss = 5.01356959
Iteration 571, loss = 5.01343160
Iteration 572, loss = 5.01332263
Iteration 573, loss = 5.01311458
Iteration 574, loss = 5.01311130
Iteration 575, loss = 5.01290393
Iteration 576, loss = 5.01252404
Iteration 577, loss = 5.01256131
Iteration 578, loss = 5.01254184
Iteration 579, loss = 5.01223652
Iteration 580, loss = 5.01204038
Iteration 581, loss = 5.01197811
Iteration 582, loss = 5.01160020
Iteration 583, loss = 5.01155234
Iteration 584, loss = 5.01156373
Iteration 585, loss = 5.01136313
Iteration 586, loss = 5.01119459
Iteration 587, loss = 5.01103659
Iteration 588, loss = 5.01090383
Iteration 589, loss = 5.01062442
Iteration 590, loss = 5.01048742
Iteration 591, loss = 5.01061093
Iteration 592, loss = 5.01031325
Iteration 593, loss = 5.01007265
Iteration 594, loss = 5.00996369
Iteration 595, loss = 5.00985552
Iteration 596, loss = 5.00987867
Iteration 597, loss = 5.00955719
Iteration 598, loss = 5.00944401
Iteration 599, loss = 5.00932995
Iteration 600, loss = 5.00909658
Iteration 601, loss = 5.00899886
Iteration 602, loss = 5.00884890
Iteration 603, loss = 5.00884658
Iteration 604, loss = 5.00852735
Iteration 605, loss = 5.00841414
Iteration 606, loss = 5.00825115
Iteration 607, loss = 5.00819643
Iteration 608, loss = 5.00800741
Iteration 609, loss = 5.00793278
Iteration 610, loss = 5.00772588
Iteration 611, loss = 5.00747601
Iteration 612, loss = 5.00756460
Iteration 613, loss = 5.00726777
Iteration 614, loss = 5.00729291
Iteration 615, loss = 5.00717293
Iteration 616, loss = 5.00691632
Iteration 617, loss = 5.00677230
Iteration 618, loss = 5.00677880
Iteration 619, loss = 5.00658965
Iteration 620, loss = 5.00630932
Iteration 621, loss = 5.00620822
Iteration 622, loss = 5.00609580
Iteration 623, loss = 5.00608071
Iteration 624, loss = 5.00575580
Iteration 625, loss = 5.00568618
Iteration 626, loss = 5.00562369
Iteration 627, loss = 5.00559207
Iteration 628, loss = 5.00534301
Iteration 629, loss = 5.00509958
Iteration 630, loss = 5.00505560
Iteration 631, loss = 5.00485021
Iteration 632, loss = 5.00473788
Iteration 633, loss = 5.00457461
Iteration 634, loss = 5.00452486
Iteration 635, loss = 5.00418378
Iteration 636, loss = 5.00418491
Iteration 637, loss = 5.00405291
Iteration 638, loss = 5.00384809
Iteration 639, loss = 5.00397300
Iteration 640, loss = 5.00370524
Iteration 641, loss = 5.00361865
Iteration 642, loss = 5.00340954
Iteration 643, loss = 5.00328081
Iteration 644, loss = 5.00334042
Iteration 645, loss = 5.00294594
Iteration 646, loss = 5.00296394
Iteration 647, loss = 5.00260409
Iteration 648, loss = 5.00265530
Iteration 649, loss = 5.00256216
Iteration 650, loss = 5.00242749
Iteration 651, loss = 5.00239642
Iteration 652, loss = 5.00212223
Iteration 653, loss = 5.00209716
Iteration 654, loss = 5.00196685
Iteration 655, loss = 5.00182212
Iteration 656, loss = 5.00161597
Iteration 657, loss = 5.00141822
Iteration 658, loss = 5.00140803
Iteration 659, loss = 5.00127027
Iteration 660, loss = 5.00126117
Iteration 661, loss = 5.00109895
Iteration 662, loss = 5.00104660
Iteration 663, loss = 5.00076031
Iteration 664, loss = 5.00071826
Iteration 665, loss = 5.00052781
Iteration 666, loss = 5.00047426
Iteration 667, loss = 5.00031979
Iteration 668, loss = 5.00025865
Iteration 669, loss = 5.00016975
Iteration 670, loss = 4.99995320
Iteration 671, loss = 4.99993619
Iteration 672, loss = 4.99952373
Iteration 673, loss = 4.99954156
Iteration 674, loss = 4.99944763
Iteration 675, loss = 4.99932776
Iteration 676, loss = 4.99926826
Iteration 677, loss = 4.99905155
Iteration 678, loss = 4.99884784
Iteration 679, loss = 4.99888751
Iteration 680, loss = 4.99862476
Iteration 681, loss = 4.99861727
Iteration 682, loss = 4.99843129
Iteration 683, loss = 4.99835513
Iteration 684, loss = 4.99834404
Iteration 685, loss = 4.99831643
Iteration 686, loss = 4.99811013
Iteration 687, loss = 4.99795445
Iteration 688, loss = 4.99790597
Iteration 689, loss = 4.99777697
Iteration 690, loss = 4.99749850
Iteration 691, loss = 4.99759926
Iteration 692, loss = 4.99722871
Iteration 693, loss = 4.99721765
Iteration 694, loss = 4.99715278
Iteration 695, loss = 4.99698003
Iteration 696, loss = 4.99682380
Iteration 697, loss = 4.99676796
Iteration 698, loss = 4.99667502
Iteration 699, loss = 4.99653539
Iteration 700, loss = 4.99637463
Iteration 701, loss = 4.99611375
Iteration 702, loss = 4.99620120
Iteration 703, loss = 4.99600580
Iteration 704, loss = 4.99583774
Iteration 705, loss = 4.99586808
Iteration 706, loss = 4.99583336
Iteration 707, loss = 4.99553581
Iteration 708, loss = 4.99539487
Iteration 709, loss = 4.99536063
Iteration 710, loss = 4.99521554
Iteration 711, loss = 4.99501804
Iteration 712, loss = 4.99506732
Iteration 713, loss = 4.99499637
Iteration 714, loss = 4.99482917
Iteration 715, loss = 4.99477479
Iteration 716, loss = 4.99458102
Iteration 717, loss = 4.99449031
Iteration 718, loss = 4.99436818
Iteration 719, loss = 4.99435419
Iteration 720, loss = 4.99411146
Iteration 721, loss = 4.99396267
Iteration 722, loss = 4.99387353
Iteration 723, loss = 4.99369907
Iteration 724, loss = 4.99350880
Iteration 725, loss = 4.99352137
Iteration 726, loss = 4.99359116
Iteration 727, loss = 4.99324616
Iteration 728, loss = 4.99333384
Iteration 729, loss = 4.99328004
Iteration 730, loss = 4.99309319
Iteration 731, loss = 4.99305957
Iteration 732, loss = 4.99299567
Iteration 733, loss = 4.99270727
Iteration 734, loss = 4.99260018
Iteration 735, loss = 4.99265361
Iteration 736, loss = 4.99251566
Iteration 737, loss = 4.99247651
Iteration 738, loss = 4.99219381
Iteration 739, loss = 4.99220225
Iteration 740, loss = 4.99200833
Iteration 741, loss = 4.99199753
Iteration 742, loss = 4.99179807
Iteration 743, loss = 4.99180022
Iteration 744, loss = 4.99162012
Iteration 745, loss = 4.99149394
Iteration 746, loss = 4.99140071
Iteration 747, loss = 4.99123196
Iteration 748, loss = 4.99111867
Iteration 749, loss = 4.99105821
Iteration 750, loss = 4.99105023
Iteration 751, loss = 4.99099038
Iteration 752, loss = 4.99077692
Iteration 753, loss = 4.99068337
Iteration 754, loss = 4.99045186
Iteration 755, loss = 4.99051158
Iteration 756, loss = 4.99040009
Iteration 757, loss = 4.99029627
Iteration 758, loss = 4.99022476
Iteration 759, loss = 4.98996330
Iteration 760, loss = 4.98986431
Iteration 761, loss = 4.98979846
Iteration 762, loss = 4.98978645
Iteration 763, loss = 4.98982290
Iteration 764, loss = 4.98956219
Iteration 765, loss = 4.98945517
Iteration 766, loss = 4.98936690
Iteration 767, loss = 4.98920728
Iteration 768, loss = 4.98908756
Iteration 769, loss = 4.98892956
Iteration 770, loss = 4.98913221
Iteration 771, loss = 4.98909119
Iteration 772, loss = 4.98870410
Iteration 773, loss = 4.98851683
Iteration 774, loss = 4.98854255
Iteration 775, loss = 4.98852861
Iteration 776, loss = 4.98836654
Iteration 777, loss = 4.98802613
Iteration 778, loss = 4.98820714
Iteration 779, loss = 4.98803663
Iteration 780, loss = 4.98804729
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((10,), {'error': 1.2559991229734584, 'fit': 0.047875000000000001, 'time': 965.01})
Iteration 1, loss = 6.40685341
Iteration 2, loss = 6.05017790
Iteration 3, loss = 5.78207309
Iteration 4, loss = 5.66076531
Iteration 5, loss = 5.59636162
Iteration 6, loss = 5.55438985
Iteration 7, loss = 5.52232453
Iteration 8, loss = 5.49706624
Iteration 9, loss = 5.47704041
Iteration 10, loss = 5.45986980
Iteration 11, loss = 5.44559561
Iteration 12, loss = 5.43264865
Iteration 13, loss = 5.42242829
Iteration 14, loss = 5.41140029
Iteration 15, loss = 5.40357632
Iteration 16, loss = 5.39472562
Iteration 17, loss = 5.38774841
Iteration 18, loss = 5.37984060
Iteration 19, loss = 5.37237014
Iteration 20, loss = 5.36568474
Iteration 21, loss = 5.35917024
Iteration 22, loss = 5.35278074
Iteration 23, loss = 5.34651721
Iteration 24, loss = 5.34028808
Iteration 25, loss = 5.33511507
Iteration 26, loss = 5.32815954
Iteration 27, loss = 5.32139057
Iteration 28, loss = 5.31496102
Iteration 29, loss = 5.30978394
Iteration 30, loss = 5.30363701
Iteration 31, loss = 5.29634246
Iteration 32, loss = 5.29002736
Iteration 33, loss = 5.28342506
Iteration 34, loss = 5.27630295
Iteration 35, loss = 5.26841318
Iteration 36, loss = 5.26125716
Iteration 37, loss = 5.25440475
Iteration 38, loss = 5.24776809
Iteration 39, loss = 5.23875466
Iteration 40, loss = 5.23115889
Iteration 41, loss = 5.22413616
Iteration 42, loss = 5.21511789
Iteration 43, loss = 5.20705028
Iteration 44, loss = 5.19808716
Iteration 45, loss = 5.19125041
Iteration 46, loss = 5.18176548
Iteration 47, loss = 5.17315795
Iteration 48, loss = 5.16324149
Iteration 49, loss = 5.15537426
Iteration 50, loss = 5.14482065
Iteration 51, loss = 5.13589019
Iteration 52, loss = 5.12576371
Iteration 53, loss = 5.11561196
Iteration 54, loss = 5.10713773
Iteration 55, loss = 5.09684228
Iteration 56, loss = 5.08718239
Iteration 57, loss = 5.07620683
Iteration 58, loss = 5.06648690
Iteration 59, loss = 5.05615513
Iteration 60, loss = 5.04550107
Iteration 61, loss = 5.03548232
Iteration 62, loss = 5.02336803
Iteration 63, loss = 5.01323143
Iteration 64, loss = 5.00436989
Iteration 65, loss = 4.99245699
Iteration 66, loss = 4.98084759
Iteration 67, loss = 4.97075544
Iteration 68, loss = 4.95968373
Iteration 69, loss = 4.94911895
Iteration 70, loss = 4.93826198
Iteration 71, loss = 4.92748156
Iteration 72, loss = 4.91607760
Iteration 73, loss = 4.90399220
Iteration 74, loss = 4.89454405
Iteration 75, loss = 4.88352183
Iteration 76, loss = 4.87355639
Iteration 77, loss = 4.86210384
Iteration 78, loss = 4.85066719
Iteration 79, loss = 4.83952002
Iteration 80, loss = 4.82733317
Iteration 81, loss = 4.81798270
Iteration 82, loss = 4.80650833
Iteration 83, loss = 4.79655215
Iteration 84, loss = 4.78495145
Iteration 85, loss = 4.77397775
Iteration 86, loss = 4.76341335
Iteration 87, loss = 4.75176144
Iteration 88, loss = 4.74199974
Iteration 89, loss = 4.73160479
Iteration 90, loss = 4.72017938
Iteration 91, loss = 4.71030487
Iteration 92, loss = 4.69917879
Iteration 93, loss = 4.68866801
Iteration 94, loss = 4.67763333
Iteration 95, loss = 4.66711266
Iteration 96, loss = 4.65703366
Iteration 97, loss = 4.64716085
Iteration 98, loss = 4.63635393
Iteration 99, loss = 4.62624829
Iteration 100, loss = 4.61461836
Iteration 101, loss = 4.60490164
Iteration 102, loss = 4.59475170
Iteration 103, loss = 4.58408931
Iteration 104, loss = 4.57471012
Iteration 105, loss = 4.56423478
Iteration 106, loss = 4.55414052
Iteration 107, loss = 4.54463356
Iteration 108, loss = 4.53451860
Iteration 109, loss = 4.52498266
Iteration 110, loss = 4.51383162
Iteration 111, loss = 4.50477395
Iteration 112, loss = 4.49558009
Iteration 113, loss = 4.48465167
Iteration 114, loss = 4.47544133
Iteration 115, loss = 4.46547683
Iteration 116, loss = 4.45602908
Iteration 117, loss = 4.44615192
Iteration 118, loss = 4.43786086
Iteration 119, loss = 4.42785648
Iteration 120, loss = 4.41759971
Iteration 121, loss = 4.40884884
Iteration 122, loss = 4.39942579
Iteration 123, loss = 4.38970667
Iteration 124, loss = 4.37973324
Iteration 125, loss = 4.37105496
Iteration 126, loss = 4.36208316
Iteration 127, loss = 4.35411951
Iteration 128, loss = 4.34341255
Iteration 129, loss = 4.33459955
Iteration 130, loss = 4.32568849
Iteration 131, loss = 4.31693131
Iteration 132, loss = 4.30855451
Iteration 133, loss = 4.29934252
Iteration 134, loss = 4.29187738
Iteration 135, loss = 4.28178410
Iteration 136, loss = 4.27288082
Iteration 137, loss = 4.26371236
Iteration 138, loss = 4.25529209
Iteration 139, loss = 4.24830671
Iteration 140, loss = 4.23849834
Iteration 141, loss = 4.23006952
Iteration 142, loss = 4.22126003
Iteration 143, loss = 4.21296451
Iteration 144, loss = 4.20506616
Iteration 145, loss = 4.19713350
Iteration 146, loss = 4.18788599
Iteration 147, loss = 4.18046622
Iteration 148, loss = 4.17266592
Iteration 149, loss = 4.16407749
Iteration 150, loss = 4.15620001
Iteration 151, loss = 4.14815664
Iteration 152, loss = 4.13946365
Iteration 153, loss = 4.13112391
Iteration 154, loss = 4.12370538
Iteration 155, loss = 4.11685450
Iteration 156, loss = 4.10726087
Iteration 157, loss = 4.10064220
Iteration 158, loss = 4.09219964
Iteration 159, loss = 4.08425140
Iteration 160, loss = 4.07682166
Iteration 161, loss = 4.06917368
Iteration 162, loss = 4.06157113
Iteration 163, loss = 4.05451656
Iteration 164, loss = 4.04630975
Iteration 165, loss = 4.03822503
Iteration 166, loss = 4.03200868
Iteration 167, loss = 4.02337576
Iteration 168, loss = 4.01617512
Iteration 169, loss = 4.00895029
Iteration 170, loss = 4.00166479
Iteration 171, loss = 3.99452145
Iteration 172, loss = 3.98824288
Iteration 173, loss = 3.97970286
Iteration 174, loss = 3.97326982
Iteration 175, loss = 3.96664904
Iteration 176, loss = 3.95864044
Iteration 177, loss = 3.95173715
Iteration 178, loss = 3.94458053
Iteration 179, loss = 3.93798368
Iteration 180, loss = 3.92993556
Iteration 181, loss = 3.92275644
Iteration 182, loss = 3.91668409
Iteration 183, loss = 3.90991180
Iteration 184, loss = 3.90387644
Iteration 185, loss = 3.89681085
Iteration 186, loss = 3.88901869
Iteration 187, loss = 3.88328444
Iteration 188, loss = 3.87555115
Iteration 189, loss = 3.86920532
Iteration 190, loss = 3.86179674
Iteration 191, loss = 3.85558363
Iteration 192, loss = 3.85039923
Iteration 193, loss = 3.84272321
Iteration 194, loss = 3.83661925
Iteration 195, loss = 3.83085398
Iteration 196, loss = 3.82306600
Iteration 197, loss = 3.81671815
Iteration 198, loss = 3.81046102
Iteration 199, loss = 3.80452461
Iteration 200, loss = 3.79806213
Iteration 201, loss = 3.79059868
Iteration 202, loss = 3.78595777
Iteration 203, loss = 3.77871713
Iteration 204, loss = 3.77328097
Iteration 205, loss = 3.76673456
Iteration 206, loss = 3.76061504
Iteration 207, loss = 3.75554247
Iteration 208, loss = 3.74872708
Iteration 209, loss = 3.74292656
Iteration 210, loss = 3.73546498
Iteration 211, loss = 3.73037160
Iteration 212, loss = 3.72425989
Iteration 213, loss = 3.71798126
Iteration 214, loss = 3.71262373
Iteration 215, loss = 3.70648088
Iteration 216, loss = 3.70109546
Iteration 217, loss = 3.69469583
Iteration 218, loss = 3.68915817
Iteration 219, loss = 3.68263374
Iteration 220, loss = 3.67680583
Iteration 221, loss = 3.67122925
Iteration 222, loss = 3.66633844
Iteration 223, loss = 3.66070401
Iteration 224, loss = 3.65436829
Iteration 225, loss = 3.64916733
Iteration 226, loss = 3.64334523
Iteration 227, loss = 3.63798353
Iteration 228, loss = 3.63248398
Iteration 229, loss = 3.62766943
Iteration 230, loss = 3.62193819
Iteration 231, loss = 3.61541662
Iteration 232, loss = 3.60998362
Iteration 233, loss = 3.60494468
Iteration 234, loss = 3.59888184
Iteration 235, loss = 3.59372832
Iteration 236, loss = 3.58863616
Iteration 237, loss = 3.58327759
Iteration 238, loss = 3.57803407
Iteration 239, loss = 3.57312355
Iteration 240, loss = 3.56770540
Iteration 241, loss = 3.56224308
Iteration 242, loss = 3.55737844
Iteration 243, loss = 3.55186349
Iteration 244, loss = 3.54667196
Iteration 245, loss = 3.54231682
Iteration 246, loss = 3.53611623
Iteration 247, loss = 3.53123247
Iteration 248, loss = 3.52592682
Iteration 249, loss = 3.52048617
Iteration 250, loss = 3.51506382
Iteration 251, loss = 3.51150744
Iteration 252, loss = 3.50653898
Iteration 253, loss = 3.50098234
Iteration 254, loss = 3.49690696
Iteration 255, loss = 3.49184039
Iteration 256, loss = 3.48590979
Iteration 257, loss = 3.48045572
Iteration 258, loss = 3.47596900
Iteration 259, loss = 3.47132766
Iteration 260, loss = 3.46645973
Iteration 261, loss = 3.46234553
Iteration 262, loss = 3.45645885
Iteration 263, loss = 3.45210327
Iteration 264, loss = 3.44840550
Iteration 265, loss = 3.44337308
Iteration 266, loss = 3.43783495
Iteration 267, loss = 3.43409793
Iteration 268, loss = 3.42754564
Iteration 269, loss = 3.42458382
Iteration 270, loss = 3.42036931
Iteration 271, loss = 3.41572889
Iteration 272, loss = 3.41064608
Iteration 273, loss = 3.40522658
Iteration 274, loss = 3.40145237
Iteration 275, loss = 3.39637679
Iteration 276, loss = 3.39195107
Iteration 277, loss = 3.38735903
Iteration 278, loss = 3.38316368
Iteration 279, loss = 3.37860702
Iteration 280, loss = 3.37321234
Iteration 281, loss = 3.37023881
Iteration 282, loss = 3.36605672
Iteration 283, loss = 3.36033066
Iteration 284, loss = 3.35688350
Iteration 285, loss = 3.35202991
Iteration 286, loss = 3.34848693
Iteration 287, loss = 3.34292542
Iteration 288, loss = 3.33916885
Iteration 289, loss = 3.33582196
Iteration 290, loss = 3.33143807
Iteration 291, loss = 3.32627734
Iteration 292, loss = 3.32273840
Iteration 293, loss = 3.31703868
Iteration 294, loss = 3.31467862
Iteration 295, loss = 3.30913355
Iteration 296, loss = 3.30540780
Iteration 297, loss = 3.30173641
Iteration 298, loss = 3.29767897
Iteration 299, loss = 3.29354901
Iteration 300, loss = 3.28867877
Iteration 301, loss = 3.28455232
Iteration 302, loss = 3.28181415
Iteration 303, loss = 3.27724853
Iteration 304, loss = 3.27341080
Iteration 305, loss = 3.26841238
Iteration 306, loss = 3.26465270
Iteration 307, loss = 3.26096084
Iteration 308, loss = 3.25687790
Iteration 309, loss = 3.25259278
Iteration 310, loss = 3.24886474
Iteration 311, loss = 3.24428269
Iteration 312, loss = 3.24139454
Iteration 313, loss = 3.23794711
Iteration 314, loss = 3.23353651
Iteration 315, loss = 3.22932825
Iteration 316, loss = 3.22462799
Iteration 317, loss = 3.22236115
Iteration 318, loss = 3.21770692
Iteration 319, loss = 3.21403862
Iteration 320, loss = 3.21096463
Iteration 321, loss = 3.20672885
Iteration 322, loss = 3.20242297
Iteration 323, loss = 3.19809471
Iteration 324, loss = 3.19512351
Iteration 325, loss = 3.19186149
Iteration 326, loss = 3.18869784
Iteration 327, loss = 3.18424517
Iteration 328, loss = 3.18017128
Iteration 329, loss = 3.17752339
Iteration 330, loss = 3.17288375
Iteration 331, loss = 3.16921248
Iteration 332, loss = 3.16610701
Iteration 333, loss = 3.16300232
Iteration 334, loss = 3.15935878
Iteration 335, loss = 3.15550033
Iteration 336, loss = 3.15183835
Iteration 337, loss = 3.14765103
Iteration 338, loss = 3.14469242
Iteration 339, loss = 3.14046323
Iteration 340, loss = 3.13738402
Iteration 341, loss = 3.13340221
Iteration 342, loss = 3.13005964
Iteration 343, loss = 3.12710509
Iteration 344, loss = 3.12363149
Iteration 345, loss = 3.11978821
Iteration 346, loss = 3.11692701
Iteration 347, loss = 3.11309443
Iteration 348, loss = 3.10906460
Iteration 349, loss = 3.10612846
Iteration 350, loss = 3.10306282
Iteration 351, loss = 3.09993169
Iteration 352, loss = 3.09625038
Iteration 353, loss = 3.09303977
Iteration 354, loss = 3.08920398
Iteration 355, loss = 3.08564427
Iteration 356, loss = 3.08270445
Iteration 357, loss = 3.07930086
Iteration 358, loss = 3.07653100
Iteration 359, loss = 3.07325325
Iteration 360, loss = 3.06999350
Iteration 361, loss = 3.06636445
Iteration 362, loss = 3.06378774
Iteration 363, loss = 3.06055463
Iteration 364, loss = 3.05661661
Iteration 365, loss = 3.05494206
Iteration 366, loss = 3.05088175
Iteration 367, loss = 3.04798692
Iteration 368, loss = 3.04418766
Iteration 369, loss = 3.04113723
Iteration 370, loss = 3.03821580
Iteration 371, loss = 3.03478041
Iteration 372, loss = 3.03137643
Iteration 373, loss = 3.02910113
Iteration 374, loss = 3.02619482
Iteration 375, loss = 3.02206522
Iteration 376, loss = 3.01912739
Iteration 377, loss = 3.01611053
Iteration 378, loss = 3.01237022
Iteration 379, loss = 3.00930652
Iteration 380, loss = 3.00690933
Iteration 381, loss = 3.00388796
Iteration 382, loss = 3.00014261
Iteration 383, loss = 2.99811552
Iteration 384, loss = 2.99447740
Iteration 385, loss = 2.99035880
Iteration 386, loss = 2.98849575
Iteration 387, loss = 2.98493247
Iteration 388, loss = 2.98338829
Iteration 389, loss = 2.98005133
Iteration 390, loss = 2.97724541
Iteration 391, loss = 2.97453639
Iteration 392, loss = 2.97101397
Iteration 393, loss = 2.96789036
Iteration 394, loss = 2.96489921
Iteration 395, loss = 2.96200461
Iteration 396, loss = 2.96034328
Iteration 397, loss = 2.95687762
Iteration 398, loss = 2.95378777
Iteration 399, loss = 2.95052209
Iteration 400, loss = 2.94894490
Iteration 401, loss = 2.94529264
Iteration 402, loss = 2.94170949
Iteration 403, loss = 2.93879276
Iteration 404, loss = 2.93620409
Iteration 405, loss = 2.93348699
Iteration 406, loss = 2.93119270
Iteration 407, loss = 2.92904747
Iteration 408, loss = 2.92544907
Iteration 409, loss = 2.92422527
Iteration 410, loss = 2.92055875
Iteration 411, loss = 2.91696462
Iteration 412, loss = 2.91456466
Iteration 413, loss = 2.91195194
Iteration 414, loss = 2.90912733
Iteration 415, loss = 2.90689162
Iteration 416, loss = 2.90379157
Iteration 417, loss = 2.90177070
Iteration 418, loss = 2.89901918
Iteration 419, loss = 2.89569638
Iteration 420, loss = 2.89317221
Iteration 421, loss = 2.89117276
Iteration 422, loss = 2.88839896
Iteration 423, loss = 2.88528333
Iteration 424, loss = 2.88257333
Iteration 425, loss = 2.87951136
Iteration 426, loss = 2.87731886
Iteration 427, loss = 2.87482410
Iteration 428, loss = 2.87299401
Iteration 429, loss = 2.86977807
Iteration 430, loss = 2.86632314
Iteration 431, loss = 2.86374559
Iteration 432, loss = 2.86246049
Iteration 433, loss = 2.85965726
Iteration 434, loss = 2.85695807
Iteration 435, loss = 2.85362103
Iteration 436, loss = 2.85197748
Iteration 437, loss = 2.84853617
Iteration 438, loss = 2.84648711
Iteration 439, loss = 2.84406945
Iteration 440, loss = 2.84138819
Iteration 441, loss = 2.83947990
Iteration 442, loss = 2.83635374
Iteration 443, loss = 2.83362084
Iteration 444, loss = 2.83108933
Iteration 445, loss = 2.82945703
Iteration 446, loss = 2.82713819
Iteration 447, loss = 2.82457627
Iteration 448, loss = 2.82218757
Iteration 449, loss = 2.81868254
Iteration 450, loss = 2.81721943
Iteration 451, loss = 2.81393529
Iteration 452, loss = 2.81244855
Iteration 453, loss = 2.80946504
Iteration 454, loss = 2.80741524
Iteration 455, loss = 2.80507446
Iteration 456, loss = 2.80222816
Iteration 457, loss = 2.80062441
Iteration 458, loss = 2.79762969
Iteration 459, loss = 2.79597003
Iteration 460, loss = 2.79384894
Iteration 461, loss = 2.79121703
Iteration 462, loss = 2.78869436
Iteration 463, loss = 2.78614217
Iteration 464, loss = 2.78397343
Iteration 465, loss = 2.78104782
Iteration 466, loss = 2.77913243
Iteration 467, loss = 2.77722884
Iteration 468, loss = 2.77492325
Iteration 469, loss = 2.77244432
Iteration 470, loss = 2.76973438
Iteration 471, loss = 2.76815078
Iteration 472, loss = 2.76584788
Iteration 473, loss = 2.76297322
Iteration 474, loss = 2.76084797
Iteration 475, loss = 2.75900927
Iteration 476, loss = 2.75591600
Iteration 477, loss = 2.75445590
Iteration 478, loss = 2.75196343
Iteration 479, loss = 2.75012248
Iteration 480, loss = 2.74741269
Iteration 481, loss = 2.74541127
Iteration 482, loss = 2.74319525
Iteration 483, loss = 2.74078841
Iteration 484, loss = 2.73826079
Iteration 485, loss = 2.73625770
Iteration 486, loss = 2.73522912
Iteration 487, loss = 2.73217566
Iteration 488, loss = 2.73012321
Iteration 489, loss = 2.72843525
Iteration 490, loss = 2.72658978
Iteration 491, loss = 2.72375079
Iteration 492, loss = 2.72204719
Iteration 493, loss = 2.71999930
Iteration 494, loss = 2.71699709
Iteration 495, loss = 2.71558519
Iteration 496, loss = 2.71227370
Iteration 497, loss = 2.71121594
Iteration 498, loss = 2.70912851
Iteration 499, loss = 2.70723738
Iteration 500, loss = 2.70547077
Iteration 501, loss = 2.70275171
Iteration 502, loss = 2.70111986
Iteration 503, loss = 2.69929716
Iteration 504, loss = 2.69611332
Iteration 505, loss = 2.69452057
Iteration 506, loss = 2.69254008
Iteration 507, loss = 2.68994925
Iteration 508, loss = 2.68865938
Iteration 509, loss = 2.68646550
Iteration 510, loss = 2.68371517
Iteration 511, loss = 2.68365449
Iteration 512, loss = 2.67968743
Iteration 513, loss = 2.67862050
Iteration 514, loss = 2.67619696
Iteration 515, loss = 2.67387311
Iteration 516, loss = 2.67096463
Iteration 517, loss = 2.67009994
Iteration 518, loss = 2.66802973
Iteration 519, loss = 2.66654579
Iteration 520, loss = 2.66412571
Iteration 521, loss = 2.66207343
Iteration 522, loss = 2.66045666
Iteration 523, loss = 2.65927988
Iteration 524, loss = 2.65669591
Iteration 525, loss = 2.65417893
Iteration 526, loss = 2.65208465
Iteration 527, loss = 2.65017793
Iteration 528, loss = 2.64881563
Iteration 529, loss = 2.64666438
Iteration 530, loss = 2.64488009
Iteration 531, loss = 2.64265271
Iteration 532, loss = 2.64086376
Iteration 533, loss = 2.63957694
Iteration 534, loss = 2.63733844
Iteration 535, loss = 2.63509065
Iteration 536, loss = 2.63378945
Iteration 537, loss = 2.63109593
Iteration 538, loss = 2.62937149
Iteration 539, loss = 2.62732638
Iteration 540, loss = 2.62552170
Iteration 541, loss = 2.62462495
Iteration 542, loss = 2.62225049
Iteration 543, loss = 2.62015314
Iteration 544, loss = 2.61885644
Iteration 545, loss = 2.61665054
Iteration 546, loss = 2.61484989
Iteration 547, loss = 2.61279240
Iteration 548, loss = 2.61134267
Iteration 549, loss = 2.60956509
Iteration 550, loss = 2.60698653
Iteration 551, loss = 2.60570215
Iteration 552, loss = 2.60311797
Iteration 553, loss = 2.60269123
Iteration 554, loss = 2.59953641
Iteration 555, loss = 2.59888774
Iteration 556, loss = 2.59613838
Iteration 557, loss = 2.59482880
Iteration 558, loss = 2.59254373
Iteration 559, loss = 2.59209509
Iteration 560, loss = 2.58955917
Iteration 561, loss = 2.58695325
Iteration 562, loss = 2.58609260
Iteration 563, loss = 2.58498812
Iteration 564, loss = 2.58262613
Iteration 565, loss = 2.58076521
Iteration 566, loss = 2.57854532
Iteration 567, loss = 2.57726052
Iteration 568, loss = 2.57495745
Iteration 569, loss = 2.57401676
Iteration 570, loss = 2.57227496
Iteration 571, loss = 2.57043692
Iteration 572, loss = 2.56909343
Iteration 573, loss = 2.56653312
Iteration 574, loss = 2.56591887
Iteration 575, loss = 2.56314429
Iteration 576, loss = 2.56051417
Iteration 577, loss = 2.56061614
Iteration 578, loss = 2.55890785
Iteration 579, loss = 2.55736544
Iteration 580, loss = 2.55511832
Iteration 581, loss = 2.55350576
Iteration 582, loss = 2.55231853
Iteration 583, loss = 2.55040160
Iteration 584, loss = 2.54823861
Iteration 585, loss = 2.54708732
Iteration 586, loss = 2.54399339
Iteration 587, loss = 2.54305326
Iteration 588, loss = 2.54193283
Iteration 589, loss = 2.53996137
Iteration 590, loss = 2.53873457
Iteration 591, loss = 2.53696970
Iteration 592, loss = 2.53541788
Iteration 593, loss = 2.53386593
Iteration 594, loss = 2.53204216
Iteration 595, loss = 2.53146471
Iteration 596, loss = 2.52989941
Iteration 597, loss = 2.52735978
Iteration 598, loss = 2.52561188
Iteration 599, loss = 2.52445886
Iteration 600, loss = 2.52242588
Iteration 601, loss = 2.52118037
Iteration 602, loss = 2.51933979
Iteration 603, loss = 2.51700641
Iteration 604, loss = 2.51576845
Iteration 605, loss = 2.51417218
Iteration 606, loss = 2.51269027
Iteration 607, loss = 2.51182768
Iteration 608, loss = 2.50955381
Iteration 609, loss = 2.50744891
Iteration 610, loss = 2.50653883
Iteration 611, loss = 2.50483873
Iteration 612, loss = 2.50288568
Iteration 613, loss = 2.50191318
Iteration 614, loss = 2.50067449
Iteration 615, loss = 2.49947169
Iteration 616, loss = 2.49681476
Iteration 617, loss = 2.49640107
Iteration 618, loss = 2.49423723
Iteration 619, loss = 2.49314582
Iteration 620, loss = 2.49146263
Iteration 621, loss = 2.48966647
Iteration 622, loss = 2.48902393
Iteration 623, loss = 2.48674011
Iteration 624, loss = 2.48553628
Iteration 625, loss = 2.48423684
Iteration 626, loss = 2.48213166
Iteration 627, loss = 2.48000253
Iteration 628, loss = 2.47968133
Iteration 629, loss = 2.47757335
Iteration 630, loss = 2.47550570
Iteration 631, loss = 2.47499125
Iteration 632, loss = 2.47370156
Iteration 633, loss = 2.47209115
Iteration 634, loss = 2.47049302
Iteration 635, loss = 2.46913656
Iteration 636, loss = 2.46826459
Iteration 637, loss = 2.46590524
Iteration 638, loss = 2.46412449
Iteration 639, loss = 2.46313407
Iteration 640, loss = 2.46146263
Iteration 641, loss = 2.46020204
Iteration 642, loss = 2.45866263
Iteration 643, loss = 2.45664804
Iteration 644, loss = 2.45635453
Iteration 645, loss = 2.45333291
Iteration 646, loss = 2.45329877
Iteration 647, loss = 2.45134707
Iteration 648, loss = 2.44969834
Iteration 649, loss = 2.44826822
Iteration 650, loss = 2.44643464
Iteration 651, loss = 2.44503142
Iteration 652, loss = 2.44440914
Iteration 653, loss = 2.44386993
Iteration 654, loss = 2.44145973
Iteration 655, loss = 2.44060977
Iteration 656, loss = 2.43873285
Iteration 657, loss = 2.43731582
Iteration 658, loss = 2.43555783
Iteration 659, loss = 2.43454790
Iteration 660, loss = 2.43354406
Iteration 661, loss = 2.43294663
Iteration 662, loss = 2.43090337
Iteration 663, loss = 2.42981007
Iteration 664, loss = 2.42722547
Iteration 665, loss = 2.42577368
Iteration 666, loss = 2.42545090
Iteration 667, loss = 2.42302616
Iteration 668, loss = 2.42146725
Iteration 669, loss = 2.42088628
Iteration 670, loss = 2.41913468
Iteration 671, loss = 2.41739081
Iteration 672, loss = 2.41673140
Iteration 673, loss = 2.41524144
Iteration 674, loss = 2.41367866
Iteration 675, loss = 2.41407260
Iteration 676, loss = 2.41082040
Iteration 677, loss = 2.41009022
Iteration 678, loss = 2.40947316
Iteration 679, loss = 2.40740285
Iteration 680, loss = 2.40624140
Iteration 681, loss = 2.40459055
Iteration 682, loss = 2.40342036
Iteration 683, loss = 2.40283520
Iteration 684, loss = 2.40033865
Iteration 685, loss = 2.40032717
Iteration 686, loss = 2.39810867
Iteration 687, loss = 2.39741598
Iteration 688, loss = 2.39528742
Iteration 689, loss = 2.39469716
Iteration 690, loss = 2.39292067
Iteration 691, loss = 2.39188429
Iteration 692, loss = 2.39046212
Iteration 693, loss = 2.38959309
Iteration 694, loss = 2.38760242
Iteration 695, loss = 2.38613932
Iteration 696, loss = 2.38523674
Iteration 697, loss = 2.38400239
Iteration 698, loss = 2.38197631
Iteration 699, loss = 2.38061163
Iteration 700, loss = 2.37948347
Iteration 701, loss = 2.37932558
Iteration 702, loss = 2.37785264
Iteration 703, loss = 2.37743848
Iteration 704, loss = 2.37534023
Iteration 705, loss = 2.37367227
Iteration 706, loss = 2.37259103
Iteration 707, loss = 2.37233214
Iteration 708, loss = 2.36938537
Iteration 709, loss = 2.36923421
Iteration 710, loss = 2.36781453
Iteration 711, loss = 2.36580726
Iteration 712, loss = 2.36468484
Iteration 713, loss = 2.36499176
Iteration 714, loss = 2.36247061
Iteration 715, loss = 2.36153307
Iteration 716, loss = 2.36019299
Iteration 717, loss = 2.35890201
Iteration 718, loss = 2.35820630
Iteration 719, loss = 2.35700521
Iteration 720, loss = 2.35550169
Iteration 721, loss = 2.35344309
Iteration 722, loss = 2.35245921
Iteration 723, loss = 2.35193970
Iteration 724, loss = 2.35072545
Iteration 725, loss = 2.34999024
Iteration 726, loss = 2.34816037
Iteration 727, loss = 2.34681911
Iteration 728, loss = 2.34677415
Iteration 729, loss = 2.34467835
Iteration 730, loss = 2.34388610
Iteration 731, loss = 2.34197367
Iteration 732, loss = 2.34026898
Iteration 733, loss = 2.34020848
Iteration 734, loss = 2.33908669
Iteration 735, loss = 2.33719917
Iteration 736, loss = 2.33714722
Iteration 737, loss = 2.33575465
Iteration 738, loss = 2.33450054
Iteration 739, loss = 2.33351872
Iteration 740, loss = 2.33086216
Iteration 741, loss = 2.33085655
Iteration 742, loss = 2.32912113
Iteration 743, loss = 2.32809532
Iteration 744, loss = 2.32629197
Iteration 745, loss = 2.32633950
Iteration 746, loss = 2.32332107
Iteration 747, loss = 2.32258632
Iteration 748, loss = 2.32262998
Iteration 749, loss = 2.32095383
Iteration 750, loss = 2.32079603
Iteration 751, loss = 2.31874650
Iteration 752, loss = 2.31748604
Iteration 753, loss = 2.31704630
Iteration 754, loss = 2.31586754
Iteration 755, loss = 2.31518014
Iteration 756, loss = 2.31412103
Iteration 757, loss = 2.31210286
Iteration 758, loss = 2.31101882
Iteration 759, loss = 2.31032762
Iteration 760, loss = 2.30977746
Iteration 761, loss = 2.30752654
Iteration 762, loss = 2.30709120
Iteration 763, loss = 2.30620282
Iteration 764, loss = 2.30511620
Iteration 765, loss = 2.30332510
Iteration 766, loss = 2.30223390
Iteration 767, loss = 2.30167206
Iteration 768, loss = 2.30114019
Iteration 769, loss = 2.29865957
Iteration 770, loss = 2.29798453
Iteration 771, loss = 2.29729453
Iteration 772, loss = 2.29614409
Iteration 773, loss = 2.29444928
Iteration 774, loss = 2.29417534
Iteration 775, loss = 2.29210057
Iteration 776, loss = 2.29065555
Iteration 777, loss = 2.29066010
Iteration 778, loss = 2.28847278
Iteration 779, loss = 2.28890953
Iteration 780, loss = 2.28772575
Iteration 781, loss = 2.28638698
Iteration 782, loss = 2.28535023
Iteration 783, loss = 2.28403028
Iteration 784, loss = 2.28356180
Iteration 785, loss = 2.28137140
Iteration 786, loss = 2.28100283
Iteration 787, loss = 2.28057072
Iteration 788, loss = 2.27989689
Iteration 789, loss = 2.27744714
Iteration 790, loss = 2.27682577
Iteration 791, loss = 2.27664849
Iteration 792, loss = 2.27492042
Iteration 793, loss = 2.27355212
Iteration 794, loss = 2.27185177
Iteration 795, loss = 2.27239433
Iteration 796, loss = 2.27017941
Iteration 797, loss = 2.26945898
Iteration 798, loss = 2.26793584
Iteration 799, loss = 2.26790510
Iteration 800, loss = 2.26632858
Iteration 801, loss = 2.26522197
Iteration 802, loss = 2.26364219
Iteration 803, loss = 2.26330329
Iteration 804, loss = 2.26227244
Iteration 805, loss = 2.26206666
Iteration 806, loss = 2.26013240
Iteration 807, loss = 2.25948891
Iteration 808, loss = 2.25879284
Iteration 809, loss = 2.25684579
Iteration 810, loss = 2.25601219
Iteration 811, loss = 2.25511767
Iteration 812, loss = 2.25362823
Iteration 813, loss = 2.25244547
Iteration 814, loss = 2.25248479
Iteration 815, loss = 2.25124768
Iteration 816, loss = 2.24919897
Iteration 817, loss = 2.24871414
Iteration 818, loss = 2.24841740
Iteration 819, loss = 2.24826698
Iteration 820, loss = 2.24600848
Iteration 821, loss = 2.24495808
Iteration 822, loss = 2.24418790
Iteration 823, loss = 2.24335700
Iteration 824, loss = 2.24228504
Iteration 825, loss = 2.24067469
Iteration 826, loss = 2.23962530
Iteration 827, loss = 2.23913886
Iteration 828, loss = 2.23762611
Iteration 829, loss = 2.23771587
Iteration 830, loss = 2.23666340
Iteration 831, loss = 2.23521208
Iteration 832, loss = 2.23398083
Iteration 833, loss = 2.23349309
Iteration 834, loss = 2.23278194
Iteration 835, loss = 2.23146075
Iteration 836, loss = 2.23039319
Iteration 837, loss = 2.22890585
Iteration 838, loss = 2.22880034
Iteration 839, loss = 2.22814190
Iteration 840, loss = 2.22686132
Iteration 841, loss = 2.22544370
Iteration 842, loss = 2.22494288
Iteration 843, loss = 2.22449578
Iteration 844, loss = 2.22251354
Iteration 845, loss = 2.22187079
Iteration 846, loss = 2.22082480
Iteration 847, loss = 2.22033859
Iteration 848, loss = 2.21950713
Iteration 849, loss = 2.21824558
Iteration 850, loss = 2.21818957
Iteration 851, loss = 2.21617647
Iteration 852, loss = 2.21516983
Iteration 853, loss = 2.21415898
Iteration 854, loss = 2.21458815
Iteration 855, loss = 2.21238311
Iteration 856, loss = 2.21134269
Iteration 857, loss = 2.21014980
Iteration 858, loss = 2.20977201
Iteration 859, loss = 2.20855203
Iteration 860, loss = 2.20733133
Iteration 861, loss = 2.20701812
Iteration 862, loss = 2.20590690
Iteration 863, loss = 2.20515264
Iteration 864, loss = 2.20437779
Iteration 865, loss = 2.20331183
Iteration 866, loss = 2.20318005
Iteration 867, loss = 2.20054517
Iteration 868, loss = 2.20064181
Iteration 869, loss = 2.19900885
Iteration 870, loss = 2.19834905
Iteration 871, loss = 2.19899881
Iteration 872, loss = 2.19672136
Iteration 873, loss = 2.19584312
Iteration 874, loss = 2.19527128
Iteration 875, loss = 2.19461984
Iteration 876, loss = 2.19336634
Iteration 877, loss = 2.19246657
Iteration 878, loss = 2.19140037
Iteration 879, loss = 2.19111110
Iteration 880, loss = 2.19024180
Iteration 881, loss = 2.18940052
Iteration 882, loss = 2.18836571
Iteration 883, loss = 2.18733776
Iteration 884, loss = 2.18667885
Iteration 885, loss = 2.18509401
Iteration 886, loss = 2.18502501
Iteration 887, loss = 2.18323686
Iteration 888, loss = 2.18266139
Iteration 889, loss = 2.18157318
Iteration 890, loss = 2.18118229
Iteration 891, loss = 2.17986865
Iteration 892, loss = 2.17923887
Iteration 893, loss = 2.17832376
Iteration 894, loss = 2.17832815
Iteration 895, loss = 2.17691501
Iteration 896, loss = 2.17677257
Iteration 897, loss = 2.17464133
Iteration 898, loss = 2.17450587
Iteration 899, loss = 2.17377285
Iteration 900, loss = 2.17297576
Iteration 901, loss = 2.17206849
Iteration 902, loss = 2.17094866
Iteration 903, loss = 2.16981711
Iteration 904, loss = 2.16839317
Iteration 905, loss = 2.16891039
Iteration 906, loss = 2.16733747
Iteration 907, loss = 2.16631436
Iteration 908, loss = 2.16641721
Iteration 909, loss = 2.16484234
Iteration 910, loss = 2.16293324
Iteration 911, loss = 2.16302952
Iteration 912, loss = 2.16171902
Iteration 913, loss = 2.16137902
Iteration 914, loss = 2.16032721
Iteration 915, loss = 2.15918817
Iteration 916, loss = 2.15871336
Iteration 917, loss = 2.15771112
Iteration 918, loss = 2.15792192
Iteration 919, loss = 2.15614088
Iteration 920, loss = 2.15537630
Iteration 921, loss = 2.15497496
Iteration 922, loss = 2.15389886
Iteration 923, loss = 2.15264576
Iteration 924, loss = 2.15336193
Iteration 925, loss = 2.15132555
Iteration 926, loss = 2.15021528
Iteration 927, loss = 2.14979238
Iteration 928, loss = 2.14913482
Iteration 929, loss = 2.14865903
Iteration 930, loss = 2.14695840
Iteration 931, loss = 2.14605075
Iteration 932, loss = 2.14539758
Iteration 933, loss = 2.14518016
Iteration 934, loss = 2.14448910
Iteration 935, loss = 2.14350361
Iteration 936, loss = 2.14309480
Iteration 937, loss = 2.14116362
Iteration 938, loss = 2.14109529
Iteration 939, loss = 2.13990251
Iteration 940, loss = 2.14025252
Iteration 941, loss = 2.13864787
Iteration 942, loss = 2.13774884
Iteration 943, loss = 2.13695573
Iteration 944, loss = 2.13596233
Iteration 945, loss = 2.13615276
Iteration 946, loss = 2.13445555
Iteration 947, loss = 2.13346036
Iteration 948, loss = 2.13349309
Iteration 949, loss = 2.13245167
Iteration 950, loss = 2.13224418
Iteration 951, loss = 2.13022222
Iteration 952, loss = 2.12932355
Iteration 953, loss = 2.12892930
Iteration 954, loss = 2.12813359
Iteration 955, loss = 2.12694852
Iteration 956, loss = 2.12697400
Iteration 957, loss = 2.12635673
Iteration 958, loss = 2.12514368
Iteration 959, loss = 2.12386857
Iteration 960, loss = 2.12376445
Iteration 961, loss = 2.12239062
Iteration 962, loss = 2.12196373
Iteration 963, loss = 2.12112770
Iteration 964, loss = 2.12013630
Iteration 965, loss = 2.11943832
Iteration 966, loss = 2.11860071
Iteration 967, loss = 2.11861788
Iteration 968, loss = 2.11776372
Iteration 969, loss = 2.11697818
Iteration 970, loss = 2.11530410
Iteration 971, loss = 2.11557208
Iteration 972, loss = 2.11499843
Iteration 973, loss = 2.11322320
Iteration 974, loss = 2.11374974
Iteration 975, loss = 2.11159450
Iteration 976, loss = 2.11164405
Iteration 977, loss = 2.10996999
Iteration 978, loss = 2.11049500
Iteration 979, loss = 2.10955760
Iteration 980, loss = 2.10827668
Iteration 981, loss = 2.10729690
Iteration 982, loss = 2.10657713
Iteration 983, loss = 2.10545507
Iteration 984, loss = 2.10580597
Iteration 985, loss = 2.10523793
Iteration 986, loss = 2.10333966
Iteration 987, loss = 2.10409256
Iteration 988, loss = 2.10185918
Iteration 989, loss = 2.10119931
Iteration 990, loss = 2.10123400
Iteration 991, loss = 2.10051038
Iteration 992, loss = 2.09983044
Iteration 993, loss = 2.09834264
Iteration 994, loss = 2.09791735
Iteration 995, loss = 2.09762000
Iteration 996, loss = 2.09692821
Iteration 997, loss = 2.09582532
Iteration 998, loss = 2.09512579
Iteration 999, loss = 2.09449766
Iteration 1000, loss = 2.09345636
Iteration 1001, loss = 2.09331451
Iteration 1002, loss = 2.09183920
Iteration 1003, loss = 2.09231028
Iteration 1004, loss = 2.09019505
Iteration 1005, loss = 2.09070313
Iteration 1006, loss = 2.08912875
Iteration 1007, loss = 2.08868356
Iteration 1008, loss = 2.08753068
Iteration 1009, loss = 2.08697253
Iteration 1010, loss = 2.08595992
Iteration 1011, loss = 2.08640623
Iteration 1012, loss = 2.08573249
Iteration 1013, loss = 2.08420296
Iteration 1014, loss = 2.08313342
Iteration 1015, loss = 2.08364544
Iteration 1016, loss = 2.08138513
Iteration 1017, loss = 2.08151927
Iteration 1018, loss = 2.08067203
Iteration 1019, loss = 2.08108602
Iteration 1020, loss = 2.07968476
Iteration 1021, loss = 2.07890543
Iteration 1022, loss = 2.07833048
Iteration 1023, loss = 2.07780301
Iteration 1024, loss = 2.07691333
Iteration 1025, loss = 2.07656191
Iteration 1026, loss = 2.07534000
Iteration 1027, loss = 2.07422523
Iteration 1028, loss = 2.07403703
Iteration 1029, loss = 2.07355577
Iteration 1030, loss = 2.07225880
Iteration 1031, loss = 2.07117852
Iteration 1032, loss = 2.07101852
Iteration 1033, loss = 2.07051131
Iteration 1034, loss = 2.06971204
Iteration 1035, loss = 2.07038049
Iteration 1036, loss = 2.06844478
Iteration 1037, loss = 2.06750529
Iteration 1038, loss = 2.06662066
Iteration 1039, loss = 2.06593062
Iteration 1040, loss = 2.06574901
Iteration 1041, loss = 2.06391859
Iteration 1042, loss = 2.06362050
Iteration 1043, loss = 2.06274258
Iteration 1044, loss = 2.06271300
Iteration 1045, loss = 2.06312403
Iteration 1046, loss = 2.06112387
Iteration 1047, loss = 2.06103401
Iteration 1048, loss = 2.05872385
Iteration 1049, loss = 2.06018951
Iteration 1050, loss = 2.05849372
Iteration 1051, loss = 2.05804710
Iteration 1052, loss = 2.05753194
Iteration 1053, loss = 2.05730649
Iteration 1054, loss = 2.05628903
Iteration 1055, loss = 2.05550840
Iteration 1056, loss = 2.05468956
Iteration 1057, loss = 2.05438295
Iteration 1058, loss = 2.05260097
Iteration 1059, loss = 2.05324591
Iteration 1060, loss = 2.05165534
Iteration 1061, loss = 2.05081347
Iteration 1062, loss = 2.05090183
Iteration 1063, loss = 2.04990940
Iteration 1064, loss = 2.04883640
Iteration 1065, loss = 2.04947298
Iteration 1066, loss = 2.04820616
Iteration 1067, loss = 2.04700269
Iteration 1068, loss = 2.04662065
Iteration 1069, loss = 2.04627043
Iteration 1070, loss = 2.04583779
Iteration 1071, loss = 2.04476013
Iteration 1072, loss = 2.04444870
Iteration 1073, loss = 2.04396665
Iteration 1074, loss = 2.04312345
Iteration 1075, loss = 2.04195547
Iteration 1076, loss = 2.04200497
Iteration 1077, loss = 2.04143178
Iteration 1078, loss = 2.04000225
Iteration 1079, loss = 2.03951283
Iteration 1080, loss = 2.03871479
Iteration 1081, loss = 2.03783581
Iteration 1082, loss = 2.03836203
Iteration 1083, loss = 2.03731804
Iteration 1084, loss = 2.03686396
Iteration 1085, loss = 2.03588131
Iteration 1086, loss = 2.03488955
Iteration 1087, loss = 2.03454092
Iteration 1088, loss = 2.03367394
Iteration 1089, loss = 2.03330082
Iteration 1090, loss = 2.03290900
Iteration 1091, loss = 2.03124178
Iteration 1092, loss = 2.03080667
Iteration 1093, loss = 2.03054024
Iteration 1094, loss = 2.03023757
Iteration 1095, loss = 2.02894024
Iteration 1096, loss = 2.02950724
Iteration 1097, loss = 2.02904297
Iteration 1098, loss = 2.02721087
Iteration 1099, loss = 2.02739800
Iteration 1100, loss = 2.02683602
Iteration 1101, loss = 2.02595633
Iteration 1102, loss = 2.02571593
Iteration 1103, loss = 2.02449415
Iteration 1104, loss = 2.02450597
Iteration 1105, loss = 2.02406097
Iteration 1106, loss = 2.02252323
Iteration 1107, loss = 2.02184980
Iteration 1108, loss = 2.02065089
Iteration 1109, loss = 2.02077639
Iteration 1110, loss = 2.02012438
Iteration 1111, loss = 2.01961211
Iteration 1112, loss = 2.01938764
Iteration 1113, loss = 2.01900661
Iteration 1114, loss = 2.01772876
Iteration 1115, loss = 2.01767701
Iteration 1116, loss = 2.01606598
Iteration 1117, loss = 2.01640331
Iteration 1118, loss = 2.01521959
Iteration 1119, loss = 2.01432340
Iteration 1120, loss = 2.01362139
Iteration 1121, loss = 2.01327482
Iteration 1122, loss = 2.01283951
Iteration 1123, loss = 2.01172541
Iteration 1124, loss = 2.01268084
Iteration 1125, loss = 2.01124195
Iteration 1126, loss = 2.01028177
Iteration 1127, loss = 2.01011858
Iteration 1128, loss = 2.00971845
Iteration 1129, loss = 2.00846889
Iteration 1130, loss = 2.00825602
Iteration 1131, loss = 2.00824218
Iteration 1132, loss = 2.00708933
Iteration 1133, loss = 2.00625825
Iteration 1134, loss = 2.00616564
Iteration 1135, loss = 2.00443459
Iteration 1136, loss = 2.00406505
Iteration 1137, loss = 2.00388536
Iteration 1138, loss = 2.00343154
Iteration 1139, loss = 2.00302502
Iteration 1140, loss = 2.00255106
Iteration 1141, loss = 2.00037173
Iteration 1142, loss = 2.00140438
Iteration 1143, loss = 2.00076878
Iteration 1144, loss = 1.99974896
Iteration 1145, loss = 1.99942288
Iteration 1146, loss = 1.99879809
Iteration 1147, loss = 1.99841356
Iteration 1148, loss = 1.99714815
Iteration 1149, loss = 1.99738206
Iteration 1150, loss = 1.99552000
Iteration 1151, loss = 1.99489966
Iteration 1152, loss = 1.99582749
Iteration 1153, loss = 1.99416798
Iteration 1154, loss = 1.99450518
Iteration 1155, loss = 1.99383252
Iteration 1156, loss = 1.99315361
Iteration 1157, loss = 1.99232881
Iteration 1158, loss = 1.99206940
Iteration 1159, loss = 1.99103212
Iteration 1160, loss = 1.99056960
Iteration 1161, loss = 1.99018535
Iteration 1162, loss = 1.98945826
Iteration 1163, loss = 1.98922836
Iteration 1164, loss = 1.98790463
Iteration 1165, loss = 1.98756483
Iteration 1166, loss = 1.98775647
Iteration 1167, loss = 1.98605259
Iteration 1168, loss = 1.98587783
Iteration 1169, loss = 1.98506794
Iteration 1170, loss = 1.98477321
Iteration 1171, loss = 1.98379643
Iteration 1172, loss = 1.98385606
Iteration 1173, loss = 1.98287382
Iteration 1174, loss = 1.98195724
Iteration 1175, loss = 1.98209322
Iteration 1176, loss = 1.98123866
Iteration 1177, loss = 1.98115441
Iteration 1178, loss = 1.98074221
Iteration 1179, loss = 1.98033907
Iteration 1180, loss = 1.97963559
Iteration 1181, loss = 1.97841781
Iteration 1182, loss = 1.97789212
Iteration 1183, loss = 1.97746604
Iteration 1184, loss = 1.97764589
Iteration 1185, loss = 1.97604601
Iteration 1186, loss = 1.97638511
Iteration 1187, loss = 1.97574390
Iteration 1188, loss = 1.97437076
Iteration 1189, loss = 1.97367949
Iteration 1190, loss = 1.97303163
Iteration 1191, loss = 1.97356158
Iteration 1192, loss = 1.97246826
Iteration 1193, loss = 1.97236831
Iteration 1194, loss = 1.97166550
Iteration 1195, loss = 1.97085593
Iteration 1196, loss = 1.96994783
Iteration 1197, loss = 1.96948504
Iteration 1198, loss = 1.96907206
Iteration 1199, loss = 1.96863523
Iteration 1200, loss = 1.96800128
Iteration 1201, loss = 1.96763160
Iteration 1202, loss = 1.96658855
Iteration 1203, loss = 1.96600598
Iteration 1204, loss = 1.96593642
Iteration 1205, loss = 1.96570208
Iteration 1206, loss = 1.96533398
Iteration 1207, loss = 1.96498989
Iteration 1208, loss = 1.96433842
Iteration 1209, loss = 1.96301053
Iteration 1210, loss = 1.96277606
Iteration 1211, loss = 1.96308847
Iteration 1212, loss = 1.96153695
Iteration 1213, loss = 1.96215137
Iteration 1214, loss = 1.96077126
Iteration 1215, loss = 1.96022425
Iteration 1216, loss = 1.95971559
Iteration 1217, loss = 1.95953263
Iteration 1218, loss = 1.95886307
Iteration 1219, loss = 1.95838234
Iteration 1220, loss = 1.95834977
Iteration 1221, loss = 1.95648899
Iteration 1222, loss = 1.95738778
Iteration 1223, loss = 1.95576066
Iteration 1224, loss = 1.95472122
Iteration 1225, loss = 1.95561421
Iteration 1226, loss = 1.95473236
Iteration 1227, loss = 1.95346406
Iteration 1228, loss = 1.95384766
Iteration 1229, loss = 1.95323718
Iteration 1230, loss = 1.95195177
Iteration 1231, loss = 1.95208033
Iteration 1232, loss = 1.95120911
Iteration 1233, loss = 1.95128448
Iteration 1234, loss = 1.94919512
Iteration 1235, loss = 1.94988972
Iteration 1236, loss = 1.94931938
Iteration 1237, loss = 1.94907110
Iteration 1238, loss = 1.94843339
Iteration 1239, loss = 1.94695165
Iteration 1240, loss = 1.94711698
Iteration 1241, loss = 1.94759535
Iteration 1242, loss = 1.94610788
Iteration 1243, loss = 1.94574506
Iteration 1244, loss = 1.94451810
Iteration 1245, loss = 1.94467551
Iteration 1246, loss = 1.94363653
Iteration 1247, loss = 1.94328168
Iteration 1248, loss = 1.94302085
Iteration 1249, loss = 1.94294040
Iteration 1250, loss = 1.94215434
Iteration 1251, loss = 1.94179583
Iteration 1252, loss = 1.94131268
Iteration 1253, loss = 1.94058794
Iteration 1254, loss = 1.94017042
Iteration 1255, loss = 1.93948807
Iteration 1256, loss = 1.93908216
Iteration 1257, loss = 1.93853312
Iteration 1258, loss = 1.93752822
Iteration 1259, loss = 1.93805503
Iteration 1260, loss = 1.93646630
Iteration 1261, loss = 1.93636003
Iteration 1262, loss = 1.93565772
Iteration 1263, loss = 1.93666241
Iteration 1264, loss = 1.93469825
Iteration 1265, loss = 1.93399790
Iteration 1266, loss = 1.93417498
Iteration 1267, loss = 1.93418520
Iteration 1268, loss = 1.93327521
Iteration 1269, loss = 1.93284995
Iteration 1270, loss = 1.93221684
Iteration 1271, loss = 1.93176883
Iteration 1272, loss = 1.93121566
Iteration 1273, loss = 1.92976770
Iteration 1274, loss = 1.92971842
Iteration 1275, loss = 1.92999345
Iteration 1276, loss = 1.92840675
Iteration 1277, loss = 1.92805579
Iteration 1278, loss = 1.92807933
Iteration 1279, loss = 1.92793891
Iteration 1280, loss = 1.92692512
Iteration 1281, loss = 1.92607746
Iteration 1282, loss = 1.92680901
Iteration 1283, loss = 1.92515602
Iteration 1284, loss = 1.92502236
Iteration 1285, loss = 1.92371052
Iteration 1286, loss = 1.92404921
Iteration 1287, loss = 1.92379762
Iteration 1288, loss = 1.92360714
Iteration 1289, loss = 1.92230622
Iteration 1290, loss = 1.92291351
Iteration 1291, loss = 1.92170442
Iteration 1292, loss = 1.92083032
Iteration 1293, loss = 1.92077524
Iteration 1294, loss = 1.92006611
Iteration 1295, loss = 1.91960402
Iteration 1296, loss = 1.91903189
Iteration 1297, loss = 1.91900997
Iteration 1298, loss = 1.91890017
Iteration 1299, loss = 1.91761077
Iteration 1300, loss = 1.91711457
Iteration 1301, loss = 1.91648322
Iteration 1302, loss = 1.91643551
Iteration 1303, loss = 1.91602690
Iteration 1304, loss = 1.91540323
Iteration 1305, loss = 1.91400428
Iteration 1306, loss = 1.91422576
Iteration 1307, loss = 1.91349140
Iteration 1308, loss = 1.91334077
Iteration 1309, loss = 1.91252391
Iteration 1310, loss = 1.91268313
Iteration 1311, loss = 1.91292670
Iteration 1312, loss = 1.91216421
Iteration 1313, loss = 1.91114463
Iteration 1314, loss = 1.91034186
Iteration 1315, loss = 1.90993061
Iteration 1316, loss = 1.90812365
Iteration 1317, loss = 1.90882374
Iteration 1318, loss = 1.90845663
Iteration 1319, loss = 1.90884496
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100,), {'error': 0.95756559315695677, 'fit': 0.54549999999999998, 'time': 2140.92})
Iteration 1, loss = 6.12451651
Iteration 2, loss = 5.62793756
Iteration 3, loss = 5.54617332
Iteration 4, loss = 5.51475730
Iteration 5, loss = 5.49135880
Iteration 6, loss = 5.48022651
Iteration 7, loss = 5.47194487
Iteration 8, loss = 5.46545001
Iteration 9, loss = 5.46071184
Iteration 10, loss = 5.45552246
Iteration 11, loss = 5.45285926
Iteration 12, loss = 5.44762345
Iteration 13, loss = 5.44421793
Iteration 14, loss = 5.44380878
Iteration 15, loss = 5.43949921
Iteration 16, loss = 5.43901413
Iteration 17, loss = 5.43595097
Iteration 18, loss = 5.43405124
Iteration 19, loss = 5.43223735
Iteration 20, loss = 5.42977326
Iteration 21, loss = 5.43049874
Iteration 22, loss = 5.42622483
Iteration 23, loss = 5.42440185
Iteration 24, loss = 5.42282652
Iteration 25, loss = 5.42202197
Iteration 26, loss = 5.41808325
Iteration 27, loss = 5.41754107
Iteration 28, loss = 5.41549613
Iteration 29, loss = 5.41504039
Iteration 30, loss = 5.41577082
Iteration 31, loss = 5.41175999
Iteration 32, loss = 5.40997206
Iteration 33, loss = 5.40867339
Iteration 34, loss = 5.40812606
Iteration 35, loss = 5.40690985
Iteration 36, loss = 5.40297225
Iteration 37, loss = 5.40292248
Iteration 38, loss = 5.40005555
Iteration 39, loss = 5.40174147
Iteration 40, loss = 5.39989994
Iteration 41, loss = 5.39588502
Iteration 42, loss = 5.39568217
Iteration 43, loss = 5.39306674
Iteration 44, loss = 5.39212333
Iteration 45, loss = 5.38926301
Iteration 46, loss = 5.38973563
Iteration 47, loss = 5.38681677
Iteration 48, loss = 5.38593740
Iteration 49, loss = 5.38650848
Iteration 50, loss = 5.38283379
Iteration 51, loss = 5.38108718
Iteration 52, loss = 5.37953793
Iteration 53, loss = 5.37819616
Iteration 54, loss = 5.37625820
Iteration 55, loss = 5.37761184
Iteration 56, loss = 5.37349112
Iteration 57, loss = 5.37242096
Iteration 58, loss = 5.36874767
Iteration 59, loss = 5.36729729
Iteration 60, loss = 5.36652339
Iteration 61, loss = 5.36250242
Iteration 62, loss = 5.36348368
Iteration 63, loss = 5.35840851
Iteration 64, loss = 5.35649978
Iteration 65, loss = 5.35350794
Iteration 66, loss = 5.35220568
Iteration 67, loss = 5.34871529
Iteration 68, loss = 5.34495700
Iteration 69, loss = 5.34433938
Iteration 70, loss = 5.34022213
Iteration 71, loss = 5.33818044
Iteration 72, loss = 5.33539347
Iteration 73, loss = 5.33192938
Iteration 74, loss = 5.32655539
Iteration 75, loss = 5.32469431
Iteration 76, loss = 5.32037249
Iteration 77, loss = 5.31538919
Iteration 78, loss = 5.31239024
Iteration 79, loss = 5.30573607
Iteration 80, loss = 5.30236065
Iteration 81, loss = 5.29900715
Iteration 82, loss = 5.29364019
Iteration 83, loss = 5.28857473
Iteration 84, loss = 5.28346588
Iteration 85, loss = 5.27905555
Iteration 86, loss = 5.27246953
Iteration 87, loss = 5.26985155
Iteration 88, loss = 5.26390722
Iteration 89, loss = 5.25657517
Iteration 90, loss = 5.25259973
Iteration 91, loss = 5.24636067
Iteration 92, loss = 5.24150485
Iteration 93, loss = 5.23418108
Iteration 94, loss = 5.22716804
Iteration 95, loss = 5.22089814
Iteration 96, loss = 5.21368710
Iteration 97, loss = 5.20808577
Iteration 98, loss = 5.20273334
Iteration 99, loss = 5.19479665
Iteration 100, loss = 5.18997432
Iteration 101, loss = 5.18520061
Iteration 102, loss = 5.17674449
Iteration 103, loss = 5.16734590
Iteration 104, loss = 5.16348078
Iteration 105, loss = 5.15629951
Iteration 106, loss = 5.14770908
Iteration 107, loss = 5.14238983
Iteration 108, loss = 5.13604095
Iteration 109, loss = 5.12850254
Iteration 110, loss = 5.12011133
Iteration 111, loss = 5.11341604
Iteration 112, loss = 5.10900483
Iteration 113, loss = 5.10018491
Iteration 114, loss = 5.09390896
Iteration 115, loss = 5.08674300
Iteration 116, loss = 5.07907883
Iteration 117, loss = 5.07267534
Iteration 118, loss = 5.06493135
Iteration 119, loss = 5.05713582
Iteration 120, loss = 5.05018771
Iteration 121, loss = 5.04363315
Iteration 122, loss = 5.03614123
Iteration 123, loss = 5.02888917
Iteration 124, loss = 5.02046281
Iteration 125, loss = 5.01230099
Iteration 126, loss = 5.00396534
Iteration 127, loss = 4.99798621
Iteration 128, loss = 4.99120168
Iteration 129, loss = 4.98210802
Iteration 130, loss = 4.97612949
Iteration 131, loss = 4.96856787
Iteration 132, loss = 4.95871610
Iteration 133, loss = 4.95174969
Iteration 134, loss = 4.94308158
Iteration 135, loss = 4.93730536
Iteration 136, loss = 4.92984684
Iteration 137, loss = 4.92112298
Iteration 138, loss = 4.91598286
Iteration 139, loss = 4.90601396
Iteration 140, loss = 4.90064685
Iteration 141, loss = 4.89204217
Iteration 142, loss = 4.88443402
Iteration 143, loss = 4.87389826
Iteration 144, loss = 4.86766768
Iteration 145, loss = 4.86205730
Iteration 146, loss = 4.85445974
Iteration 147, loss = 4.84425020
Iteration 148, loss = 4.83559787
Iteration 149, loss = 4.82739708
Iteration 150, loss = 4.82111412
Iteration 151, loss = 4.81182876
Iteration 152, loss = 4.80238758
Iteration 153, loss = 4.79893048
Iteration 154, loss = 4.78875228
Iteration 155, loss = 4.78040818
Iteration 156, loss = 4.77231079
Iteration 157, loss = 4.76478591
Iteration 158, loss = 4.75601641
Iteration 159, loss = 4.74756347
Iteration 160, loss = 4.73873177
Iteration 161, loss = 4.73226229
Iteration 162, loss = 4.72483971
Iteration 163, loss = 4.71458472
Iteration 164, loss = 4.70994192
Iteration 165, loss = 4.69990561
Iteration 166, loss = 4.69268712
Iteration 167, loss = 4.68579440
Iteration 168, loss = 4.67669274
Iteration 169, loss = 4.66844809
Iteration 170, loss = 4.66125783
Iteration 171, loss = 4.65305455
Iteration 172, loss = 4.64536689
Iteration 173, loss = 4.63553980
Iteration 174, loss = 4.62680406
Iteration 175, loss = 4.61908441
Iteration 176, loss = 4.61178320
Iteration 177, loss = 4.60306069
Iteration 178, loss = 4.59575013
Iteration 179, loss = 4.58875184
Iteration 180, loss = 4.57773821
Iteration 181, loss = 4.56978639
Iteration 182, loss = 4.56002620
Iteration 183, loss = 4.55498024
Iteration 184, loss = 4.54749081
Iteration 185, loss = 4.53626359
Iteration 186, loss = 4.53200491
Iteration 187, loss = 4.52372801
Iteration 188, loss = 4.51342264
Iteration 189, loss = 4.50983204
Iteration 190, loss = 4.49982976
Iteration 191, loss = 4.49025992
Iteration 192, loss = 4.48125579
Iteration 193, loss = 4.47472504
Iteration 194, loss = 4.46989292
Iteration 195, loss = 4.45642621
Iteration 196, loss = 4.45052711
Iteration 197, loss = 4.44259610
Iteration 198, loss = 4.43245365
Iteration 199, loss = 4.42666032
Iteration 200, loss = 4.41975727
Iteration 201, loss = 4.40988931
Iteration 202, loss = 4.40351260
Iteration 203, loss = 4.39547318
Iteration 204, loss = 4.38722280
Iteration 205, loss = 4.37960051
Iteration 206, loss = 4.37237719
Iteration 207, loss = 4.36316199
Iteration 208, loss = 4.35821900
Iteration 209, loss = 4.34856898
Iteration 210, loss = 4.34050878
Iteration 211, loss = 4.33292215
Iteration 212, loss = 4.32348716
Iteration 213, loss = 4.31722915
Iteration 214, loss = 4.31033864
Iteration 215, loss = 4.30105430
Iteration 216, loss = 4.29459584
Iteration 217, loss = 4.28746544
Iteration 218, loss = 4.27862036
Iteration 219, loss = 4.27155433
Iteration 220, loss = 4.26420006
Iteration 221, loss = 4.25547534
Iteration 222, loss = 4.24899514
Iteration 223, loss = 4.24174066
Iteration 224, loss = 4.23341647
Iteration 225, loss = 4.22587169
Iteration 226, loss = 4.21722314
Iteration 227, loss = 4.21180264
Iteration 228, loss = 4.20534696
Iteration 229, loss = 4.19824042
Iteration 230, loss = 4.18778272
Iteration 231, loss = 4.17967130
Iteration 232, loss = 4.17173857
Iteration 233, loss = 4.16533590
Iteration 234, loss = 4.15749103
Iteration 235, loss = 4.14897012
Iteration 236, loss = 4.14433653
Iteration 237, loss = 4.13752688
Iteration 238, loss = 4.12932268
Iteration 239, loss = 4.12233760
Iteration 240, loss = 4.11470635
Iteration 241, loss = 4.10994002
Iteration 242, loss = 4.10213473
Iteration 243, loss = 4.09300340
Iteration 244, loss = 4.08812362
Iteration 245, loss = 4.08095381
Iteration 246, loss = 4.07090932
Iteration 247, loss = 4.06551770
Iteration 248, loss = 4.05843739
Iteration 249, loss = 4.04791465
Iteration 250, loss = 4.04304978
Iteration 251, loss = 4.03639156
Iteration 252, loss = 4.02714547
Iteration 253, loss = 4.02097723
Iteration 254, loss = 4.01528201
Iteration 255, loss = 4.00896157
Iteration 256, loss = 4.00288701
Iteration 257, loss = 3.99657777
Iteration 258, loss = 3.98828438
Iteration 259, loss = 3.97969233
Iteration 260, loss = 3.97364274
Iteration 261, loss = 3.96727891
Iteration 262, loss = 3.95984045
Iteration 263, loss = 3.95438513
Iteration 264, loss = 3.94830126
Iteration 265, loss = 3.94012191
Iteration 266, loss = 3.93290491
Iteration 267, loss = 3.92486426
Iteration 268, loss = 3.92040842
Iteration 269, loss = 3.91387077
Iteration 270, loss = 3.90757867
Iteration 271, loss = 3.89904009
Iteration 272, loss = 3.89207683
Iteration 273, loss = 3.88869219
Iteration 274, loss = 3.87972505
Iteration 275, loss = 3.87326993
Iteration 276, loss = 3.86750778
Iteration 277, loss = 3.86032674
Iteration 278, loss = 3.85293485
Iteration 279, loss = 3.84746111
Iteration 280, loss = 3.83786557
Iteration 281, loss = 3.83621044
Iteration 282, loss = 3.82895557
Iteration 283, loss = 3.82345842
Iteration 284, loss = 3.81396019
Iteration 285, loss = 3.81093074
Iteration 286, loss = 3.80393481
Iteration 287, loss = 3.79436260
Iteration 288, loss = 3.78862704
Iteration 289, loss = 3.78303801
Iteration 290, loss = 3.77821063
Iteration 291, loss = 3.77125151
Iteration 292, loss = 3.76520283
Iteration 293, loss = 3.75973081
Iteration 294, loss = 3.75252560
Iteration 295, loss = 3.74874821
Iteration 296, loss = 3.74163440
Iteration 297, loss = 3.73417021
Iteration 298, loss = 3.72872430
Iteration 299, loss = 3.72332075
Iteration 300, loss = 3.71522485
Iteration 301, loss = 3.71019005
Iteration 302, loss = 3.70599796
Iteration 303, loss = 3.69974737
Iteration 304, loss = 3.69148200
Iteration 305, loss = 3.68475329
Iteration 306, loss = 3.68065195
Iteration 307, loss = 3.67316416
Iteration 308, loss = 3.66753577
Iteration 309, loss = 3.66176597
Iteration 310, loss = 3.65463541
Iteration 311, loss = 3.65028743
Iteration 312, loss = 3.64385969
Iteration 313, loss = 3.63880266
Iteration 314, loss = 3.63313026
Iteration 315, loss = 3.62724708
Iteration 316, loss = 3.62033684
Iteration 317, loss = 3.61819293
Iteration 318, loss = 3.60926723
Iteration 319, loss = 3.60296108
Iteration 320, loss = 3.59559802
Iteration 321, loss = 3.59227987
Iteration 322, loss = 3.58577853
Iteration 323, loss = 3.58085897
Iteration 324, loss = 3.57606986
Iteration 325, loss = 3.56818656
Iteration 326, loss = 3.56503935
Iteration 327, loss = 3.55800803
Iteration 328, loss = 3.55195038
Iteration 329, loss = 3.54837213
Iteration 330, loss = 3.54428653
Iteration 331, loss = 3.53677857
Iteration 332, loss = 3.53176706
Iteration 333, loss = 3.52489101
Iteration 334, loss = 3.51871746
Iteration 335, loss = 3.51414959
Iteration 336, loss = 3.50844518
Iteration 337, loss = 3.50394105
Iteration 338, loss = 3.49680353
Iteration 339, loss = 3.49183129
Iteration 340, loss = 3.48719886
Iteration 341, loss = 3.48200873
Iteration 342, loss = 3.47750273
Iteration 343, loss = 3.47119622
Iteration 344, loss = 3.46680202
Iteration 345, loss = 3.45920003
Iteration 346, loss = 3.45470249
Iteration 347, loss = 3.45096180
Iteration 348, loss = 3.44341605
Iteration 349, loss = 3.43945673
Iteration 350, loss = 3.43340575
Iteration 351, loss = 3.43039254
Iteration 352, loss = 3.42581203
Iteration 353, loss = 3.41788874
Iteration 354, loss = 3.41250236
Iteration 355, loss = 3.40542527
Iteration 356, loss = 3.40339374
Iteration 357, loss = 3.39739149
Iteration 358, loss = 3.39313513
Iteration 359, loss = 3.38811130
Iteration 360, loss = 3.38457176
Iteration 361, loss = 3.37696962
Iteration 362, loss = 3.37322122
Iteration 363, loss = 3.36706656
Iteration 364, loss = 3.36315332
Iteration 365, loss = 3.35866427
Iteration 366, loss = 3.35172916
Iteration 367, loss = 3.34737787
Iteration 368, loss = 3.34377149
Iteration 369, loss = 3.33845643
Iteration 370, loss = 3.33284845
Iteration 371, loss = 3.32880279
Iteration 372, loss = 3.32465464
Iteration 373, loss = 3.31826167
Iteration 374, loss = 3.31265239
Iteration 375, loss = 3.31053684
Iteration 376, loss = 3.30593840
Iteration 377, loss = 3.29882649
Iteration 378, loss = 3.29606072
Iteration 379, loss = 3.28958860
Iteration 380, loss = 3.28579531
Iteration 381, loss = 3.27867980
Iteration 382, loss = 3.27485725
Iteration 383, loss = 3.26842965
Iteration 384, loss = 3.26737729
Iteration 385, loss = 3.26110447
Iteration 386, loss = 3.25375855
Iteration 387, loss = 3.25094350
Iteration 388, loss = 3.24779016
Iteration 389, loss = 3.24612309
Iteration 390, loss = 3.23779805
Iteration 391, loss = 3.23261120
Iteration 392, loss = 3.23048791
Iteration 393, loss = 3.22384226
Iteration 394, loss = 3.21949776
Iteration 395, loss = 3.21171128
Iteration 396, loss = 3.20873599
Iteration 397, loss = 3.20624497
Iteration 398, loss = 3.19972274
Iteration 399, loss = 3.19613117
Iteration 400, loss = 3.19095999
Iteration 401, loss = 3.18694957
Iteration 402, loss = 3.18201258
Iteration 403, loss = 3.17880430
Iteration 404, loss = 3.17686467
Iteration 405, loss = 3.17187901
Iteration 406, loss = 3.16475481
Iteration 407, loss = 3.16077875
Iteration 408, loss = 3.15594840
Iteration 409, loss = 3.15341537
Iteration 410, loss = 3.14818670
Iteration 411, loss = 3.14540763
Iteration 412, loss = 3.13907033
Iteration 413, loss = 3.13602819
Iteration 414, loss = 3.13001956
Iteration 415, loss = 3.12500414
Iteration 416, loss = 3.12332883
Iteration 417, loss = 3.11758367
Iteration 418, loss = 3.11418074
Iteration 419, loss = 3.10913044
Iteration 420, loss = 3.10479625
Iteration 421, loss = 3.10267083
Iteration 422, loss = 3.09689570
Iteration 423, loss = 3.09244379
Iteration 424, loss = 3.08860897
Iteration 425, loss = 3.08427004
Iteration 426, loss = 3.08041368
Iteration 427, loss = 3.07603876
Iteration 428, loss = 3.07082908
Iteration 429, loss = 3.06751165
Iteration 430, loss = 3.06490127
Iteration 431, loss = 3.05780694
Iteration 432, loss = 3.05368114
Iteration 433, loss = 3.05013637
Iteration 434, loss = 3.04702607
Iteration 435, loss = 3.04334258
Iteration 436, loss = 3.03924262
Iteration 437, loss = 3.03385443
Iteration 438, loss = 3.03022848
Iteration 439, loss = 3.02514809
Iteration 440, loss = 3.02192662
Iteration 441, loss = 3.02031091
Iteration 442, loss = 3.01511285
Iteration 443, loss = 3.00949725
Iteration 444, loss = 3.00762851
Iteration 445, loss = 3.00431366
Iteration 446, loss = 3.00015748
Iteration 447, loss = 2.99518856
Iteration 448, loss = 2.98774057
Iteration 449, loss = 2.98738544
Iteration 450, loss = 2.98106645
Iteration 451, loss = 2.97771993
Iteration 452, loss = 2.97533428
Iteration 453, loss = 2.97141004
Iteration 454, loss = 2.96867353
Iteration 455, loss = 2.96517731
Iteration 456, loss = 2.96009470
Iteration 457, loss = 2.95733374
Iteration 458, loss = 2.95092389
Iteration 459, loss = 2.94757540
Iteration 460, loss = 2.94442856
Iteration 461, loss = 2.94260358
Iteration 462, loss = 2.94120988
Iteration 463, loss = 2.93238204
Iteration 464, loss = 2.92852971
Iteration 465, loss = 2.92426563
Iteration 466, loss = 2.92067119
Iteration 467, loss = 2.91745492
Iteration 468, loss = 2.91344319
Iteration 469, loss = 2.90806807
Iteration 470, loss = 2.90814935
Iteration 471, loss = 2.90415501
Iteration 472, loss = 2.89880728
Iteration 473, loss = 2.89546023
Iteration 474, loss = 2.89116138
Iteration 475, loss = 2.88673377
Iteration 476, loss = 2.88162957
Iteration 477, loss = 2.88203021
Iteration 478, loss = 2.87684715
Iteration 479, loss = 2.87322164
Iteration 480, loss = 2.86806245
Iteration 481, loss = 2.86930230
Iteration 482, loss = 2.86297637
Iteration 483, loss = 2.85951934
Iteration 484, loss = 2.85631924
Iteration 485, loss = 2.85114807
Iteration 486, loss = 2.84897019
Iteration 487, loss = 2.84539026
Iteration 488, loss = 2.84341713
Iteration 489, loss = 2.84050707
Iteration 490, loss = 2.83373156
Iteration 491, loss = 2.83067444
Iteration 492, loss = 2.82674183
Iteration 493, loss = 2.82482560
Iteration 494, loss = 2.81943610
Iteration 495, loss = 2.81623907
Iteration 496, loss = 2.81339045
Iteration 497, loss = 2.80971476
Iteration 498, loss = 2.80849020
Iteration 499, loss = 2.80416312
Iteration 500, loss = 2.80245858
Iteration 501, loss = 2.79516425
Iteration 502, loss = 2.79251616
Iteration 503, loss = 2.78992373
Iteration 504, loss = 2.78431272
Iteration 505, loss = 2.78232599
Iteration 506, loss = 2.77935937
Iteration 507, loss = 2.77353560
Iteration 508, loss = 2.77248103
Iteration 509, loss = 2.76872846
Iteration 510, loss = 2.76716951
Iteration 511, loss = 2.76165615
Iteration 512, loss = 2.75970924
Iteration 513, loss = 2.75429316
Iteration 514, loss = 2.75264502
Iteration 515, loss = 2.74936823
Iteration 516, loss = 2.74372818
Iteration 517, loss = 2.74272726
Iteration 518, loss = 2.73912684
Iteration 519, loss = 2.73484267
Iteration 520, loss = 2.73098344
Iteration 521, loss = 2.72867132
Iteration 522, loss = 2.72532567
Iteration 523, loss = 2.72255452
Iteration 524, loss = 2.71955843
Iteration 525, loss = 2.71640220
Iteration 526, loss = 2.71189909
Iteration 527, loss = 2.70955847
Iteration 528, loss = 2.70382005
Iteration 529, loss = 2.70313722
Iteration 530, loss = 2.69782156
Iteration 531, loss = 2.69784975
Iteration 532, loss = 2.69227023
Iteration 533, loss = 2.68836849
Iteration 534, loss = 2.68609998
Iteration 535, loss = 2.68246268
Iteration 536, loss = 2.67905437
Iteration 537, loss = 2.67894903
Iteration 538, loss = 2.67270462
Iteration 539, loss = 2.66688790
Iteration 540, loss = 2.66421198
Iteration 541, loss = 2.66304634
Iteration 542, loss = 2.65941591
Iteration 543, loss = 2.65795331
Iteration 544, loss = 2.65254212
Iteration 545, loss = 2.65062603
Iteration 546, loss = 2.64772725
Iteration 547, loss = 2.64553580
Iteration 548, loss = 2.64475603
Iteration 549, loss = 2.63909334
Iteration 550, loss = 2.63516621
Iteration 551, loss = 2.63414782
Iteration 552, loss = 2.62842890
Iteration 553, loss = 2.62745988
Iteration 554, loss = 2.62275540
Iteration 555, loss = 2.61870365
Iteration 556, loss = 2.61718656
Iteration 557, loss = 2.61378021
Iteration 558, loss = 2.61023656
Iteration 559, loss = 2.60874060
Iteration 560, loss = 2.60535700
Iteration 561, loss = 2.60355721
Iteration 562, loss = 2.59831069
Iteration 563, loss = 2.59641272
Iteration 564, loss = 2.59334580
Iteration 565, loss = 2.58930785
Iteration 566, loss = 2.58858282
Iteration 567, loss = 2.58501941
Iteration 568, loss = 2.58161995
Iteration 569, loss = 2.57903502
Iteration 570, loss = 2.57651515
Iteration 571, loss = 2.57201820
Iteration 572, loss = 2.56795155
Iteration 573, loss = 2.56707870
Iteration 574, loss = 2.56299324
Iteration 575, loss = 2.56249118
Iteration 576, loss = 2.55743889
Iteration 577, loss = 2.55446555
Iteration 578, loss = 2.55244591
Iteration 579, loss = 2.54887918
Iteration 580, loss = 2.54670477
Iteration 581, loss = 2.54494082
Iteration 582, loss = 2.54193148
Iteration 583, loss = 2.53990783
Iteration 584, loss = 2.53552249
Iteration 585, loss = 2.53450288
Iteration 586, loss = 2.52895727
Iteration 587, loss = 2.52411395
Iteration 588, loss = 2.52460763
Iteration 589, loss = 2.51989498
Iteration 590, loss = 2.51758536
Iteration 591, loss = 2.51504171
Iteration 592, loss = 2.51216140
Iteration 593, loss = 2.50618101
Iteration 594, loss = 2.50492530
Iteration 595, loss = 2.50381488
Iteration 596, loss = 2.50073417
Iteration 597, loss = 2.49887536
Iteration 598, loss = 2.49528456
Iteration 599, loss = 2.49208123
Iteration 600, loss = 2.48977716
Iteration 601, loss = 2.48654103
Iteration 602, loss = 2.48254366
Iteration 603, loss = 2.48005079
Iteration 604, loss = 2.47818796
Iteration 605, loss = 2.47471692
Iteration 606, loss = 2.47034631
Iteration 607, loss = 2.47180460
Iteration 608, loss = 2.46710313
Iteration 609, loss = 2.46420578
Iteration 610, loss = 2.46175401
Iteration 611, loss = 2.46017952
Iteration 612, loss = 2.45366819
Iteration 613, loss = 2.45403147
Iteration 614, loss = 2.45099024
Iteration 615, loss = 2.44695763
Iteration 616, loss = 2.44436011
Iteration 617, loss = 2.44346767
Iteration 618, loss = 2.44019277
Iteration 619, loss = 2.43907692
Iteration 620, loss = 2.43505057
Iteration 621, loss = 2.43010522
Iteration 622, loss = 2.42924363
Iteration 623, loss = 2.42822282
Iteration 624, loss = 2.42478492
Iteration 625, loss = 2.41929772
Iteration 626, loss = 2.41818594
Iteration 627, loss = 2.41668572
Iteration 628, loss = 2.41502487
Iteration 629, loss = 2.41162175
Iteration 630, loss = 2.40684931
Iteration 631, loss = 2.40655424
Iteration 632, loss = 2.40460673
Iteration 633, loss = 2.39920777
Iteration 634, loss = 2.39693327
Iteration 635, loss = 2.39553456
Iteration 636, loss = 2.39259090
Iteration 637, loss = 2.39002610
Iteration 638, loss = 2.38521826
Iteration 639, loss = 2.38541339
Iteration 640, loss = 2.38142414
Iteration 641, loss = 2.37848360
Iteration 642, loss = 2.37413049
Iteration 643, loss = 2.37394475
Iteration 644, loss = 2.37047143
Iteration 645, loss = 2.36861269
Iteration 646, loss = 2.36514872
Iteration 647, loss = 2.36383807
Iteration 648, loss = 2.36016110
Iteration 649, loss = 2.35983843
Iteration 650, loss = 2.35572254
Iteration 651, loss = 2.35305848
Iteration 652, loss = 2.35119171
Iteration 653, loss = 2.34882674
Iteration 654, loss = 2.34842375
Iteration 655, loss = 2.34261463
Iteration 656, loss = 2.34042119
Iteration 657, loss = 2.33798912
Iteration 658, loss = 2.33387210
Iteration 659, loss = 2.33292007
Iteration 660, loss = 2.33182055
Iteration 661, loss = 2.32851491
Iteration 662, loss = 2.32472766
Iteration 663, loss = 2.32390203
Iteration 664, loss = 2.32223909
Iteration 665, loss = 2.31707883
Iteration 666, loss = 2.31744792
Iteration 667, loss = 2.31347669
Iteration 668, loss = 2.31022855
Iteration 669, loss = 2.30687302
Iteration 670, loss = 2.30678070
Iteration 671, loss = 2.30036327
Iteration 672, loss = 2.30173377
Iteration 673, loss = 2.29778147
Iteration 674, loss = 2.29785648
Iteration 675, loss = 2.29267196
Iteration 676, loss = 2.29210889
Iteration 677, loss = 2.28910315
Iteration 678, loss = 2.28730448
Iteration 679, loss = 2.28407132
Iteration 680, loss = 2.28126079
Iteration 681, loss = 2.27996629
Iteration 682, loss = 2.27691129
Iteration 683, loss = 2.27604128
Iteration 684, loss = 2.27198070
Iteration 685, loss = 2.27112449
Iteration 686, loss = 2.26357136
Iteration 687, loss = 2.26348397
Iteration 688, loss = 2.26244189
Iteration 689, loss = 2.26056192
Iteration 690, loss = 2.25659073
Iteration 691, loss = 2.25321296
Iteration 692, loss = 2.25163425
Iteration 693, loss = 2.25004174
Iteration 694, loss = 2.24698329
Iteration 695, loss = 2.24658868
Iteration 696, loss = 2.24081214
Iteration 697, loss = 2.23892289
Iteration 698, loss = 2.23957420
Iteration 699, loss = 2.23429762
Iteration 700, loss = 2.23410510
Iteration 701, loss = 2.23192100
Iteration 702, loss = 2.22993884
Iteration 703, loss = 2.22529591
Iteration 704, loss = 2.22424989
Iteration 705, loss = 2.22082264
Iteration 706, loss = 2.22011137
Iteration 707, loss = 2.21706969
Iteration 708, loss = 2.21504399
Iteration 709, loss = 2.20988389
Iteration 710, loss = 2.20775779
Iteration 711, loss = 2.20849787
Iteration 712, loss = 2.20434935
Iteration 713, loss = 2.20126870
Iteration 714, loss = 2.20339040
Iteration 715, loss = 2.19875502
Iteration 716, loss = 2.19662926
Iteration 717, loss = 2.19442972
Iteration 718, loss = 2.18986959
Iteration 719, loss = 2.18953100
Iteration 720, loss = 2.18649505
Iteration 721, loss = 2.18483084
Iteration 722, loss = 2.18262497
Iteration 723, loss = 2.18009886
Iteration 724, loss = 2.17738135
Iteration 725, loss = 2.17471038
Iteration 726, loss = 2.17301478
Iteration 727, loss = 2.17135586
Iteration 728, loss = 2.16952699
Iteration 729, loss = 2.16814527
Iteration 730, loss = 2.16595123
Iteration 731, loss = 2.16141323
Iteration 732, loss = 2.15913652
Iteration 733, loss = 2.15688912
Iteration 734, loss = 2.15480188
Iteration 735, loss = 2.15303059
Iteration 736, loss = 2.15241482
Iteration 737, loss = 2.14831499
Iteration 738, loss = 2.14485021
Iteration 739, loss = 2.14579467
Iteration 740, loss = 2.14203812
Iteration 741, loss = 2.13806423
Iteration 742, loss = 2.13711538
Iteration 743, loss = 2.13637579
Iteration 744, loss = 2.13330134
Iteration 745, loss = 2.12960856
Iteration 746, loss = 2.13022764
Iteration 747, loss = 2.12863000
Iteration 748, loss = 2.12318902
Iteration 749, loss = 2.12230178
Iteration 750, loss = 2.12096169
Iteration 751, loss = 2.11831009
Iteration 752, loss = 2.11441183
Iteration 753, loss = 2.11343877
Iteration 754, loss = 2.11110303
Iteration 755, loss = 2.10893157
Iteration 756, loss = 2.10517278
Iteration 757, loss = 2.10397930
Iteration 758, loss = 2.10367136
Iteration 759, loss = 2.10093452
Iteration 760, loss = 2.09896349
Iteration 761, loss = 2.09612289
Iteration 762, loss = 2.09437306
Iteration 763, loss = 2.09357909
Iteration 764, loss = 2.09104764
Iteration 765, loss = 2.08700378
Iteration 766, loss = 2.08638654
Iteration 767, loss = 2.08441878
Iteration 768, loss = 2.08285401
Iteration 769, loss = 2.07757419
Iteration 770, loss = 2.07786416
Iteration 771, loss = 2.07651803
Iteration 772, loss = 2.07287429
Iteration 773, loss = 2.07335115
Iteration 774, loss = 2.06963009
Iteration 775, loss = 2.06524197
Iteration 776, loss = 2.06433287
Iteration 777, loss = 2.06185740
Iteration 778, loss = 2.05936514
Iteration 779, loss = 2.05751916
Iteration 780, loss = 2.05434998
Iteration 781, loss = 2.05360160
Iteration 782, loss = 2.04835007
Iteration 783, loss = 2.05010810
Iteration 784, loss = 2.04800542
Iteration 785, loss = 2.04526870
Iteration 786, loss = 2.04438795
Iteration 787, loss = 2.04064820
Iteration 788, loss = 2.03870072
Iteration 789, loss = 2.03734150
Iteration 790, loss = 2.03525819
Iteration 791, loss = 2.03440686
Iteration 792, loss = 2.03314224
Iteration 793, loss = 2.02821345
Iteration 794, loss = 2.02837331
Iteration 795, loss = 2.02617988
Iteration 796, loss = 2.02305734
Iteration 797, loss = 2.02125967
Iteration 798, loss = 2.01820513
Iteration 799, loss = 2.01693266
Iteration 800, loss = 2.01531974
Iteration 801, loss = 2.01217200
Iteration 802, loss = 2.00820243
Iteration 803, loss = 2.00717546
Iteration 804, loss = 2.00563635
Iteration 805, loss = 2.00619775
Iteration 806, loss = 2.00332516
Iteration 807, loss = 1.99943146
Iteration 808, loss = 1.99944987
Iteration 809, loss = 1.99460822
Iteration 810, loss = 1.99355212
Iteration 811, loss = 1.99188571
Iteration 812, loss = 1.98830845
Iteration 813, loss = 1.98953379
Iteration 814, loss = 1.98520318
Iteration 815, loss = 1.98363430
Iteration 816, loss = 1.98093774
Iteration 817, loss = 1.98147175
Iteration 818, loss = 1.97966647
Iteration 819, loss = 1.97584612
Iteration 820, loss = 1.97510025
Iteration 821, loss = 1.97119115
Iteration 822, loss = 1.97070994
Iteration 823, loss = 1.96733126
Iteration 824, loss = 1.96640853
Iteration 825, loss = 1.96369380
Iteration 826, loss = 1.96289520
Iteration 827, loss = 1.96019591
Iteration 828, loss = 1.95724619
Iteration 829, loss = 1.95443918
Iteration 830, loss = 1.95377133
Iteration 831, loss = 1.95003536
Iteration 832, loss = 1.95109098
Iteration 833, loss = 1.95089320
Iteration 834, loss = 1.94509732
Iteration 835, loss = 1.94446778
Iteration 836, loss = 1.94196607
Iteration 837, loss = 1.93823486
Iteration 838, loss = 1.93817706
Iteration 839, loss = 1.93801529
Iteration 840, loss = 1.93598635
Iteration 841, loss = 1.93312929
Iteration 842, loss = 1.93001468
Iteration 843, loss = 1.92641473
Iteration 844, loss = 1.92636912
Iteration 845, loss = 1.92129642
Iteration 846, loss = 1.92331772
Iteration 847, loss = 1.92031740
Iteration 848, loss = 1.91945930
Iteration 849, loss = 1.91491359
Iteration 850, loss = 1.91495357
Iteration 851, loss = 1.91377466
Iteration 852, loss = 1.91003016
Iteration 853, loss = 1.90753452
Iteration 854, loss = 1.90549914
Iteration 855, loss = 1.90419551
Iteration 856, loss = 1.90151380
Iteration 857, loss = 1.90328575
Iteration 858, loss = 1.90083613
Iteration 859, loss = 1.89746104
Iteration 860, loss = 1.89365521
Iteration 861, loss = 1.89424106
Iteration 862, loss = 1.89093222
Iteration 863, loss = 1.89008855
Iteration 864, loss = 1.88601409
Iteration 865, loss = 1.88560818
Iteration 866, loss = 1.88281341
Iteration 867, loss = 1.88148439
Iteration 868, loss = 1.88036988
Iteration 869, loss = 1.87691956
Iteration 870, loss = 1.87781204
Iteration 871, loss = 1.87373191
Iteration 872, loss = 1.87231390
Iteration 873, loss = 1.86966866
Iteration 874, loss = 1.87021834
Iteration 875, loss = 1.86575900
Iteration 876, loss = 1.86446419
Iteration 877, loss = 1.86346742
Iteration 878, loss = 1.86147788
Iteration 879, loss = 1.86019407
Iteration 880, loss = 1.85471924
Iteration 881, loss = 1.85515307
Iteration 882, loss = 1.85465177
Iteration 883, loss = 1.85139863
Iteration 884, loss = 1.84736489
Iteration 885, loss = 1.84761897
Iteration 886, loss = 1.84607639
Iteration 887, loss = 1.84276119
Iteration 888, loss = 1.84264449
Iteration 889, loss = 1.84073258
Iteration 890, loss = 1.83840092
Iteration 891, loss = 1.83599578
Iteration 892, loss = 1.83748201
Iteration 893, loss = 1.83108176
Iteration 894, loss = 1.83081600
Iteration 895, loss = 1.83021168
Iteration 896, loss = 1.83114771
Iteration 897, loss = 1.82498298
Iteration 898, loss = 1.82412952
Iteration 899, loss = 1.82215609
Iteration 900, loss = 1.82300682
Iteration 901, loss = 1.82063473
Iteration 902, loss = 1.81598586
Iteration 903, loss = 1.81465251
Iteration 904, loss = 1.81437598
Iteration 905, loss = 1.80957439
Iteration 906, loss = 1.80979916
Iteration 907, loss = 1.80713530
Iteration 908, loss = 1.80373060
Iteration 909, loss = 1.80253734
Iteration 910, loss = 1.80400451
Iteration 911, loss = 1.79917373
Iteration 912, loss = 1.79470223
Iteration 913, loss = 1.79504659
Iteration 914, loss = 1.79324324
Iteration 915, loss = 1.79358023
Iteration 916, loss = 1.78973554
Iteration 917, loss = 1.78809707
Iteration 918, loss = 1.78414771
Iteration 919, loss = 1.78544008
Iteration 920, loss = 1.78357537
Iteration 921, loss = 1.78235224
Iteration 922, loss = 1.77927592
Iteration 923, loss = 1.78046548
Iteration 924, loss = 1.77802207
Iteration 925, loss = 1.77513465
Iteration 926, loss = 1.77455757
Iteration 927, loss = 1.77101047
Iteration 928, loss = 1.77083920
Iteration 929, loss = 1.76681331
Iteration 930, loss = 1.76611117
Iteration 931, loss = 1.76165725
Iteration 932, loss = 1.76225988
Iteration 933, loss = 1.75955197
Iteration 934, loss = 1.75731626
Iteration 935, loss = 1.75759356
Iteration 936, loss = 1.75469206
Iteration 937, loss = 1.75207910
Iteration 938, loss = 1.74898145
Iteration 939, loss = 1.75076568
Iteration 940, loss = 1.74648657
Iteration 941, loss = 1.74395123
Iteration 942, loss = 1.74237579
Iteration 943, loss = 1.74456005
Iteration 944, loss = 1.73974008
Iteration 945, loss = 1.73873644
Iteration 946, loss = 1.73694033
Iteration 947, loss = 1.73553631
Iteration 948, loss = 1.73255305
Iteration 949, loss = 1.73117248
Iteration 950, loss = 1.72674870
Iteration 951, loss = 1.72961718
Iteration 952, loss = 1.72498768
Iteration 953, loss = 1.72562424
Iteration 954, loss = 1.72456314
Iteration 955, loss = 1.72003703
Iteration 956, loss = 1.72036890
Iteration 957, loss = 1.71711636
Iteration 958, loss = 1.71425998
Iteration 959, loss = 1.71287113
Iteration 960, loss = 1.71146284
Iteration 961, loss = 1.70837840
Iteration 962, loss = 1.70732179
Iteration 963, loss = 1.70677326
Iteration 964, loss = 1.70486475
Iteration 965, loss = 1.70374169
Iteration 966, loss = 1.70101772
Iteration 967, loss = 1.69962883
Iteration 968, loss = 1.69861523
Iteration 969, loss = 1.69573887
Iteration 970, loss = 1.69363834
Iteration 971, loss = 1.69242153
Iteration 972, loss = 1.68896720
Iteration 973, loss = 1.68827063
Iteration 974, loss = 1.68549216
Iteration 975, loss = 1.68744662
Iteration 976, loss = 1.68321218
Iteration 977, loss = 1.68056693
Iteration 978, loss = 1.67769504
Iteration 979, loss = 1.67751183
Iteration 980, loss = 1.67778700
Iteration 981, loss = 1.67681862
Iteration 982, loss = 1.67568760
Iteration 983, loss = 1.67186797
Iteration 984, loss = 1.67089989
Iteration 985, loss = 1.66707129
Iteration 986, loss = 1.66485211
Iteration 987, loss = 1.66453299
Iteration 988, loss = 1.66266416
Iteration 989, loss = 1.65980249
Iteration 990, loss = 1.65977427
Iteration 991, loss = 1.65692370
Iteration 992, loss = 1.65610981
Iteration 993, loss = 1.65572574
Iteration 994, loss = 1.65144095
Iteration 995, loss = 1.64960213
Iteration 996, loss = 1.64840907
Iteration 997, loss = 1.64904447
Iteration 998, loss = 1.64402407
Iteration 999, loss = 1.64412739
Iteration 1000, loss = 1.64177083
Iteration 1001, loss = 1.64164914
Iteration 1002, loss = 1.63613847
Iteration 1003, loss = 1.63800796
Iteration 1004, loss = 1.63609242
Iteration 1005, loss = 1.63395845
Iteration 1006, loss = 1.63226957
Iteration 1007, loss = 1.63198907
Iteration 1008, loss = 1.62933237
Iteration 1009, loss = 1.62711985
Iteration 1010, loss = 1.62490664
Iteration 1011, loss = 1.62340010
Iteration 1012, loss = 1.62266878
Iteration 1013, loss = 1.62124316
Iteration 1014, loss = 1.61681193
Iteration 1015, loss = 1.61648775
Iteration 1016, loss = 1.61569565
Iteration 1017, loss = 1.61691584
Iteration 1018, loss = 1.61084216
Iteration 1019, loss = 1.61162154
Iteration 1020, loss = 1.60751940
Iteration 1021, loss = 1.60512852
Iteration 1022, loss = 1.60387009
Iteration 1023, loss = 1.60184215
Iteration 1024, loss = 1.60199712
Iteration 1025, loss = 1.59733792
Iteration 1026, loss = 1.59811061
Iteration 1027, loss = 1.59538762
Iteration 1028, loss = 1.59393441
Iteration 1029, loss = 1.59301460
Iteration 1030, loss = 1.59078528
Iteration 1031, loss = 1.58932711
Iteration 1032, loss = 1.58705833
Iteration 1033, loss = 1.58785512
Iteration 1034, loss = 1.58546840
Iteration 1035, loss = 1.58326141
Iteration 1036, loss = 1.58006041
Iteration 1037, loss = 1.58060720
Iteration 1038, loss = 1.57801578
Iteration 1039, loss = 1.57595552
Iteration 1040, loss = 1.57424686
Iteration 1041, loss = 1.57248259
Iteration 1042, loss = 1.57203233
Iteration 1043, loss = 1.56875090
Iteration 1044, loss = 1.56632152
Iteration 1045, loss = 1.56523530
Iteration 1046, loss = 1.56531426
Iteration 1047, loss = 1.56526147
Iteration 1048, loss = 1.56124421
Iteration 1049, loss = 1.55991208
Iteration 1050, loss = 1.55907181
Iteration 1051, loss = 1.55656774
Iteration 1052, loss = 1.55595761
Iteration 1053, loss = 1.55175834
Iteration 1054, loss = 1.55218366
Iteration 1055, loss = 1.54800630
Iteration 1056, loss = 1.54626432
Iteration 1057, loss = 1.54511949
Iteration 1058, loss = 1.54208450
Iteration 1059, loss = 1.54146826
Iteration 1060, loss = 1.54161440
Iteration 1061, loss = 1.54095188
Iteration 1062, loss = 1.53937025
Iteration 1063, loss = 1.53548389
Iteration 1064, loss = 1.53492850
Iteration 1065, loss = 1.53163771
Iteration 1066, loss = 1.53119631
Iteration 1067, loss = 1.52969626
Iteration 1068, loss = 1.52890100
Iteration 1069, loss = 1.52759553
Iteration 1070, loss = 1.52543351
Iteration 1071, loss = 1.52270670
Iteration 1072, loss = 1.52172332
Iteration 1073, loss = 1.51979508
Iteration 1074, loss = 1.51597778
Iteration 1075, loss = 1.51874388
Iteration 1076, loss = 1.51637934
Iteration 1077, loss = 1.51153955
Iteration 1078, loss = 1.51211064
Iteration 1079, loss = 1.50941535
Iteration 1080, loss = 1.50888507
Iteration 1081, loss = 1.50806004
Iteration 1082, loss = 1.50620849
Iteration 1083, loss = 1.50312366
Iteration 1084, loss = 1.50337261
Iteration 1085, loss = 1.49980703
Iteration 1086, loss = 1.49822888
Iteration 1087, loss = 1.49622118
Iteration 1088, loss = 1.49517604
Iteration 1089, loss = 1.49310802
Iteration 1090, loss = 1.49225230
Iteration 1091, loss = 1.49114253
Iteration 1092, loss = 1.48712438
Iteration 1093, loss = 1.48666433
Iteration 1094, loss = 1.48467542
Iteration 1095, loss = 1.48412006
Iteration 1096, loss = 1.48219455
Iteration 1097, loss = 1.48098893
Iteration 1098, loss = 1.47983452
Iteration 1099, loss = 1.47658080
Iteration 1100, loss = 1.47741798
Iteration 1101, loss = 1.47477673
Iteration 1102, loss = 1.47389868
Iteration 1103, loss = 1.47285987
Iteration 1104, loss = 1.47109622
Iteration 1105, loss = 1.46796646
Iteration 1106, loss = 1.46620609
Iteration 1107, loss = 1.46349532
Iteration 1108, loss = 1.46309014
Iteration 1109, loss = 1.46235733
Iteration 1110, loss = 1.46127408
Iteration 1111, loss = 1.45933048
Iteration 1112, loss = 1.45594438
Iteration 1113, loss = 1.45473854
Iteration 1114, loss = 1.45314007
Iteration 1115, loss = 1.45326123
Iteration 1116, loss = 1.45126176
Iteration 1117, loss = 1.44792047
Iteration 1118, loss = 1.44787932
Iteration 1119, loss = 1.44515982
Iteration 1120, loss = 1.44621228
Iteration 1121, loss = 1.44165765
Iteration 1122, loss = 1.44394435
Iteration 1123, loss = 1.43820337
Iteration 1124, loss = 1.43882549
Iteration 1125, loss = 1.43502092
Iteration 1126, loss = 1.43446048
Iteration 1127, loss = 1.43424977
Iteration 1128, loss = 1.43128635
Iteration 1129, loss = 1.42923824
Iteration 1130, loss = 1.42876014
Iteration 1131, loss = 1.42538773
Iteration 1132, loss = 1.42544094
Iteration 1133, loss = 1.42380131
Iteration 1134, loss = 1.42115201
Iteration 1135, loss = 1.42247052
Iteration 1136, loss = 1.41784237
Iteration 1137, loss = 1.41777558
Iteration 1138, loss = 1.41552074
Iteration 1139, loss = 1.41691232
Iteration 1140, loss = 1.41162921
Iteration 1141, loss = 1.41175713
Iteration 1142, loss = 1.40955382
Iteration 1143, loss = 1.40900508
Iteration 1144, loss = 1.40616935
Iteration 1145, loss = 1.40536327
Iteration 1146, loss = 1.40362940
Iteration 1147, loss = 1.40095793
Iteration 1148, loss = 1.40026439
Iteration 1149, loss = 1.39728734
Iteration 1150, loss = 1.39674897
Iteration 1151, loss = 1.39353707
Iteration 1152, loss = 1.39498041
Iteration 1153, loss = 1.39264738
Iteration 1154, loss = 1.39022222
Iteration 1155, loss = 1.38827356
Iteration 1156, loss = 1.38907018
Iteration 1157, loss = 1.38788075
Iteration 1158, loss = 1.38596392
Iteration 1159, loss = 1.38271269
Iteration 1160, loss = 1.38165495
Iteration 1161, loss = 1.38009873
Iteration 1162, loss = 1.37904941
Iteration 1163, loss = 1.37930694
Iteration 1164, loss = 1.37526242
Iteration 1165, loss = 1.37629829
Iteration 1166, loss = 1.37305874
Iteration 1167, loss = 1.37201488
Iteration 1168, loss = 1.36848197
Iteration 1169, loss = 1.36678518
Iteration 1170, loss = 1.36798512
Iteration 1171, loss = 1.36571201
Iteration 1172, loss = 1.36304602
Iteration 1173, loss = 1.36122020
Iteration 1174, loss = 1.35925167
Iteration 1175, loss = 1.35942975
Iteration 1176, loss = 1.35812236
Iteration 1177, loss = 1.35641625
Iteration 1178, loss = 1.35572312
Iteration 1179, loss = 1.35179423
Iteration 1180, loss = 1.35006943
Iteration 1181, loss = 1.34878707
Iteration 1182, loss = 1.34906598
Iteration 1183, loss = 1.34915603
Iteration 1184, loss = 1.34420405
Iteration 1185, loss = 1.34455598
Iteration 1186, loss = 1.34285592
Iteration 1187, loss = 1.34148775
Iteration 1188, loss = 1.33897160
Iteration 1189, loss = 1.33732513
Iteration 1190, loss = 1.33591960
Iteration 1191, loss = 1.33675113
Iteration 1192, loss = 1.33339176
Iteration 1193, loss = 1.33023273
Iteration 1194, loss = 1.33187630
Iteration 1195, loss = 1.32827533
Iteration 1196, loss = 1.32791024
Iteration 1197, loss = 1.32445686
Iteration 1198, loss = 1.32497408
Iteration 1199, loss = 1.32449676
Iteration 1200, loss = 1.32379116
Iteration 1201, loss = 1.32070297
Iteration 1202, loss = 1.31786842
Iteration 1203, loss = 1.31795759
Iteration 1204, loss = 1.31439527
Iteration 1205, loss = 1.31334322
Iteration 1206, loss = 1.31221304
Iteration 1207, loss = 1.31259726
Iteration 1208, loss = 1.30971994
Iteration 1209, loss = 1.30816007
Iteration 1210, loss = 1.30856780
Iteration 1211, loss = 1.30621650
Iteration 1212, loss = 1.30354035
Iteration 1213, loss = 1.30477273
Iteration 1214, loss = 1.30265116
Iteration 1215, loss = 1.29988602
Iteration 1216, loss = 1.29669132
Iteration 1217, loss = 1.29703358
Iteration 1218, loss = 1.29394875
Iteration 1219, loss = 1.29306899
Iteration 1220, loss = 1.29167360
Iteration 1221, loss = 1.29109664
Iteration 1222, loss = 1.28971931
Iteration 1223, loss = 1.29152208
Iteration 1224, loss = 1.28773531
Iteration 1225, loss = 1.28477619
Iteration 1226, loss = 1.28439832
Iteration 1227, loss = 1.28188552
Iteration 1228, loss = 1.28209476
Iteration 1229, loss = 1.28153523
Iteration 1230, loss = 1.27819495
Iteration 1231, loss = 1.27601360
Iteration 1232, loss = 1.27380497
Iteration 1233, loss = 1.27278744
Iteration 1234, loss = 1.27354822
Iteration 1235, loss = 1.27455135
Iteration 1236, loss = 1.26993850
Iteration 1237, loss = 1.26813578
Iteration 1238, loss = 1.26861449
Iteration 1239, loss = 1.26675714
Iteration 1240, loss = 1.26479458
Iteration 1241, loss = 1.26171125
Iteration 1242, loss = 1.26021920
Iteration 1243, loss = 1.26033290
Iteration 1244, loss = 1.25662208
Iteration 1245, loss = 1.25489362
Iteration 1246, loss = 1.25594066
Iteration 1247, loss = 1.25393084
Iteration 1248, loss = 1.25480755
Iteration 1249, loss = 1.24913253
Iteration 1250, loss = 1.24989967
Iteration 1251, loss = 1.24869407
Iteration 1252, loss = 1.24782624
Iteration 1253, loss = 1.24580924
Iteration 1254, loss = 1.24478367
Iteration 1255, loss = 1.24265110
Iteration 1256, loss = 1.23993893
Iteration 1257, loss = 1.23944566
Iteration 1258, loss = 1.23873992
Iteration 1259, loss = 1.23688227
Iteration 1260, loss = 1.23363325
Iteration 1261, loss = 1.23641753
Iteration 1262, loss = 1.23197473
Iteration 1263, loss = 1.23139904
Iteration 1264, loss = 1.22924690
Iteration 1265, loss = 1.22781098
Iteration 1266, loss = 1.22651757
Iteration 1267, loss = 1.22571451
Iteration 1268, loss = 1.22611442
Iteration 1269, loss = 1.22413699
Iteration 1270, loss = 1.22156048
Iteration 1271, loss = 1.21955715
Iteration 1272, loss = 1.21749270
Iteration 1273, loss = 1.21911684
Iteration 1274, loss = 1.21640976
Iteration 1275, loss = 1.21527886
Iteration 1276, loss = 1.21381578
Iteration 1277, loss = 1.21390582
Iteration 1278, loss = 1.20934492
Iteration 1279, loss = 1.20950093
Iteration 1280, loss = 1.20625019
Iteration 1281, loss = 1.20476864
Iteration 1282, loss = 1.20251836
Iteration 1283, loss = 1.20299473
Iteration 1284, loss = 1.20100460
Iteration 1285, loss = 1.20185027
Iteration 1286, loss = 1.19801266
Iteration 1287, loss = 1.19761470
Iteration 1288, loss = 1.19651603
Iteration 1289, loss = 1.19474973
Iteration 1290, loss = 1.19445194
Iteration 1291, loss = 1.19325949
Iteration 1292, loss = 1.19079190
Iteration 1293, loss = 1.19029443
Iteration 1294, loss = 1.19129435
Iteration 1295, loss = 1.18680150
Iteration 1296, loss = 1.18681984
Iteration 1297, loss = 1.18366377
Iteration 1298, loss = 1.18519205
Iteration 1299, loss = 1.17968835
Iteration 1300, loss = 1.17831005
Iteration 1301, loss = 1.17898766
Iteration 1302, loss = 1.17798047
Iteration 1303, loss = 1.17349352
Iteration 1304, loss = 1.17499036
Iteration 1305, loss = 1.17371606
Iteration 1306, loss = 1.17067128
Iteration 1307, loss = 1.16926599
Iteration 1308, loss = 1.17125001
Iteration 1309, loss = 1.16900949
Iteration 1310, loss = 1.16848100
Iteration 1311, loss = 1.16460560
Iteration 1312, loss = 1.16463463
Iteration 1313, loss = 1.16426518
Iteration 1314, loss = 1.16055205
Iteration 1315, loss = 1.16015508
Iteration 1316, loss = 1.15792817
Iteration 1317, loss = 1.15826408
Iteration 1318, loss = 1.15741025
Iteration 1319, loss = 1.15576748
Iteration 1320, loss = 1.15455247
Iteration 1321, loss = 1.15108688
Iteration 1322, loss = 1.15252578
Iteration 1323, loss = 1.15015083
Iteration 1324, loss = 1.14774695
Iteration 1325, loss = 1.14671296
Iteration 1326, loss = 1.14561471
Iteration 1327, loss = 1.14565520
Iteration 1328, loss = 1.14255576
Iteration 1329, loss = 1.14221546
Iteration 1330, loss = 1.14070980
Iteration 1331, loss = 1.14002422
Iteration 1332, loss = 1.13840129
Iteration 1333, loss = 1.13684524
Iteration 1334, loss = 1.13785065
Iteration 1335, loss = 1.13401547
Iteration 1336, loss = 1.13548231
Iteration 1337, loss = 1.13084340
Iteration 1338, loss = 1.12922784
Iteration 1339, loss = 1.13010902
Iteration 1340, loss = 1.12624600
Iteration 1341, loss = 1.12617194
Iteration 1342, loss = 1.12461110
Iteration 1343, loss = 1.12369555
Iteration 1344, loss = 1.12194013
Iteration 1345, loss = 1.12254429
Iteration 1346, loss = 1.12198728
Iteration 1347, loss = 1.11913973
Iteration 1348, loss = 1.11572747
Iteration 1349, loss = 1.11812443
Iteration 1350, loss = 1.11436513
Iteration 1351, loss = 1.11458404
Iteration 1352, loss = 1.11380721
Iteration 1353, loss = 1.11021010
Iteration 1354, loss = 1.10996594
Iteration 1355, loss = 1.10882177
Iteration 1356, loss = 1.10623422
Iteration 1357, loss = 1.10846029
Iteration 1358, loss = 1.10623586
Iteration 1359, loss = 1.10339695
Iteration 1360, loss = 1.10293884
Iteration 1361, loss = 1.09940972
Iteration 1362, loss = 1.09847085
Iteration 1363, loss = 1.09615984
Iteration 1364, loss = 1.09663658
Iteration 1365, loss = 1.09532007
Iteration 1366, loss = 1.09530748
Iteration 1367, loss = 1.09228364
Iteration 1368, loss = 1.09148290
Iteration 1369, loss = 1.08993942
Iteration 1370, loss = 1.08946078
Iteration 1371, loss = 1.08809384
Iteration 1372, loss = 1.08535804
Iteration 1373, loss = 1.08528721
Iteration 1374, loss = 1.08306177
Iteration 1375, loss = 1.08282019
Iteration 1376, loss = 1.08054543
Iteration 1377, loss = 1.08034244
Iteration 1378, loss = 1.07811061
Iteration 1379, loss = 1.07703336
Iteration 1380, loss = 1.07755357
Iteration 1381, loss = 1.07765709
Iteration 1382, loss = 1.07616270
Iteration 1383, loss = 1.07376190
Iteration 1384, loss = 1.07221216
Iteration 1385, loss = 1.07170966
Iteration 1386, loss = 1.06880052
Iteration 1387, loss = 1.06641135
Iteration 1388, loss = 1.06838421
Iteration 1389, loss = 1.06698285
Iteration 1390, loss = 1.06532993
Iteration 1391, loss = 1.06299583
Iteration 1392, loss = 1.06059655
Iteration 1393, loss = 1.05927294
Iteration 1394, loss = 1.05902037
Iteration 1395, loss = 1.05676872
Iteration 1396, loss = 1.05766849
Iteration 1397, loss = 1.05503563
Iteration 1398, loss = 1.05330385
Iteration 1399, loss = 1.05577663
Iteration 1400, loss = 1.05342527
Iteration 1401, loss = 1.05162442
Iteration 1402, loss = 1.05079990
Iteration 1403, loss = 1.04744451
Iteration 1404, loss = 1.04759739
Iteration 1405, loss = 1.04663202
Iteration 1406, loss = 1.04385950
Iteration 1407, loss = 1.04362595
Iteration 1408, loss = 1.04127254
Iteration 1409, loss = 1.04100398
Iteration 1410, loss = 1.04095742
Iteration 1411, loss = 1.03648846
Iteration 1412, loss = 1.03721622
Iteration 1413, loss = 1.03917479
Iteration 1414, loss = 1.03285045
Iteration 1415, loss = 1.03403254
Iteration 1416, loss = 1.03580646
Iteration 1417, loss = 1.03291188
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((1000,), {'error': 0.98527696777840024, 'fit': 0.79037500000000005, 'time': 8105.880000000001})
Iteration 1, loss = 6.53212480
Iteration 2, loss = 6.49599716
Iteration 3, loss = 6.46159466
Iteration 4, loss = 6.42871776
Iteration 5, loss = 6.39742285
Iteration 6, loss = 6.36797124
Iteration 7, loss = 6.34014815
Iteration 8, loss = 6.31395310
Iteration 9, loss = 6.28958156
Iteration 10, loss = 6.26678056
Iteration 11, loss = 6.24540711
Iteration 12, loss = 6.22558917
Iteration 13, loss = 6.20685327
Iteration 14, loss = 6.18919970
Iteration 15, loss = 6.17251331
Iteration 16, loss = 6.15662549
Iteration 17, loss = 6.14131266
Iteration 18, loss = 6.12657236
Iteration 19, loss = 6.11228034
Iteration 20, loss = 6.09851280
Iteration 21, loss = 6.08517015
Iteration 22, loss = 6.07244935
Iteration 23, loss = 6.06022749
Iteration 24, loss = 6.04847758
Iteration 25, loss = 6.03732575
Iteration 26, loss = 6.02667343
Iteration 27, loss = 6.01644904
Iteration 28, loss = 6.00676968
Iteration 29, loss = 5.99748644
Iteration 30, loss = 5.98855615
Iteration 31, loss = 5.98015755
Iteration 32, loss = 5.97207402
Iteration 33, loss = 5.96432670
Iteration 34, loss = 5.95697315
Iteration 35, loss = 5.94986209
Iteration 36, loss = 5.94310620
Iteration 37, loss = 5.93664870
Iteration 38, loss = 5.93037373
Iteration 39, loss = 5.92442675
Iteration 40, loss = 5.91872142
Iteration 41, loss = 5.91314005
Iteration 42, loss = 5.90786913
Iteration 43, loss = 5.90277237
Iteration 44, loss = 5.89785495
Iteration 45, loss = 5.89313968
Iteration 46, loss = 5.88859578
Iteration 47, loss = 5.88415462
Iteration 48, loss = 5.87993039
Iteration 49, loss = 5.87570405
Iteration 50, loss = 5.87165069
Iteration 51, loss = 5.86776902
Iteration 52, loss = 5.86400891
Iteration 53, loss = 5.86037556
Iteration 54, loss = 5.85670767
Iteration 55, loss = 5.85332814
Iteration 56, loss = 5.84991234
Iteration 57, loss = 5.84660524
Iteration 58, loss = 5.84339286
Iteration 59, loss = 5.84029153
Iteration 60, loss = 5.83726045
Iteration 61, loss = 5.83422887
Iteration 62, loss = 5.83137726
Iteration 63, loss = 5.82857611
Iteration 64, loss = 5.82578515
Iteration 65, loss = 5.82310838
Iteration 66, loss = 5.82051473
Iteration 67, loss = 5.81791932
Iteration 68, loss = 5.81541328
Iteration 69, loss = 5.81300858
Iteration 70, loss = 5.81066450
Iteration 71, loss = 5.80830171
Iteration 72, loss = 5.80609846
Iteration 73, loss = 5.80390957
Iteration 74, loss = 5.80169050
Iteration 75, loss = 5.79955567
Iteration 76, loss = 5.79755787
Iteration 77, loss = 5.79553186
Iteration 78, loss = 5.79355314
Iteration 79, loss = 5.79168505
Iteration 80, loss = 5.78977529
Iteration 81, loss = 5.78799787
Iteration 82, loss = 5.78620280
Iteration 83, loss = 5.78448756
Iteration 84, loss = 5.78281489
Iteration 85, loss = 5.78117648
Iteration 86, loss = 5.77955033
Iteration 87, loss = 5.77799917
Iteration 88, loss = 5.77649042
Iteration 89, loss = 5.77497408
Iteration 90, loss = 5.77343980
Iteration 91, loss = 5.77206203
Iteration 92, loss = 5.77063160
Iteration 93, loss = 5.76931595
Iteration 94, loss = 5.76800418
Iteration 95, loss = 5.76660205
Iteration 96, loss = 5.76536612
Iteration 97, loss = 5.76412262
Iteration 98, loss = 5.76288719
Iteration 99, loss = 5.76165238
Iteration 100, loss = 5.76059367
Iteration 101, loss = 5.75938934
Iteration 102, loss = 5.75828877
Iteration 103, loss = 5.75716829
Iteration 104, loss = 5.75611260
Iteration 105, loss = 5.75502099
Iteration 106, loss = 5.75396444
Iteration 107, loss = 5.75294500
Iteration 108, loss = 5.75196585
Iteration 109, loss = 5.75096502
Iteration 110, loss = 5.75002053
Iteration 111, loss = 5.74903728
Iteration 112, loss = 5.74817636
Iteration 113, loss = 5.74724895
Iteration 114, loss = 5.74632297
Iteration 115, loss = 5.74558427
Iteration 116, loss = 5.74467015
Iteration 117, loss = 5.74380200
Iteration 118, loss = 5.74299875
Iteration 119, loss = 5.74212091
Iteration 120, loss = 5.74129317
Iteration 121, loss = 5.74060790
Iteration 122, loss = 5.73975373
Iteration 123, loss = 5.73904074
Iteration 124, loss = 5.73830469
Iteration 125, loss = 5.73763312
Iteration 126, loss = 5.73679256
Iteration 127, loss = 5.73609533
Iteration 128, loss = 5.73538714
Iteration 129, loss = 5.73488087
Iteration 130, loss = 5.73402801
Iteration 131, loss = 5.73343909
Iteration 132, loss = 5.73273888
Iteration 133, loss = 5.73207401
Iteration 134, loss = 5.73143565
Iteration 135, loss = 5.73086552
Iteration 136, loss = 5.73023675
Iteration 137, loss = 5.72964236
Iteration 138, loss = 5.72897901
Iteration 139, loss = 5.72843489
Iteration 140, loss = 5.72788274
Iteration 141, loss = 5.72729003
Iteration 142, loss = 5.72667287
Iteration 143, loss = 5.72624912
Iteration 144, loss = 5.72561657
Iteration 145, loss = 5.72513777
Iteration 146, loss = 5.72457765
Iteration 147, loss = 5.72409516
Iteration 148, loss = 5.72360691
Iteration 149, loss = 5.72301910
Iteration 150, loss = 5.72252013
Iteration 151, loss = 5.72197964
Iteration 152, loss = 5.72151756
Iteration 153, loss = 5.72109558
Iteration 154, loss = 5.72069615
Iteration 155, loss = 5.72008679
Iteration 156, loss = 5.71962008
Iteration 157, loss = 5.71916748
Iteration 158, loss = 5.71875283
Iteration 159, loss = 5.71847151
Iteration 160, loss = 5.71796246
Iteration 161, loss = 5.71746173
Iteration 162, loss = 5.71704380
Iteration 163, loss = 5.71663819
Iteration 164, loss = 5.71616286
Iteration 165, loss = 5.71577307
Iteration 166, loss = 5.71533059
Iteration 167, loss = 5.71495114
Iteration 168, loss = 5.71464662
Iteration 169, loss = 5.71433301
Iteration 170, loss = 5.71386158
Iteration 171, loss = 5.71341846
Iteration 172, loss = 5.71305648
Iteration 173, loss = 5.71268011
Iteration 174, loss = 5.71235515
Iteration 175, loss = 5.71198020
Iteration 176, loss = 5.71154200
Iteration 177, loss = 5.71121682
Iteration 178, loss = 5.71104406
Iteration 179, loss = 5.71056670
Iteration 180, loss = 5.71013481
Iteration 181, loss = 5.70983780
Iteration 182, loss = 5.70946648
Iteration 183, loss = 5.70917858
Iteration 184, loss = 5.70879087
Iteration 185, loss = 5.70851392
Iteration 186, loss = 5.70815972
Iteration 187, loss = 5.70781846
Iteration 188, loss = 5.70750770
Iteration 189, loss = 5.70713587
Iteration 190, loss = 5.70689308
Iteration 191, loss = 5.70656126
Iteration 192, loss = 5.70627932
Iteration 193, loss = 5.70592970
Iteration 194, loss = 5.70564933
Iteration 195, loss = 5.70536781
Iteration 196, loss = 5.70515829
Iteration 197, loss = 5.70471977
Iteration 198, loss = 5.70448914
Iteration 199, loss = 5.70430920
Iteration 200, loss = 5.70396787
Iteration 201, loss = 5.70381089
Iteration 202, loss = 5.70334385
Iteration 203, loss = 5.70324104
Iteration 204, loss = 5.70281096
Iteration 205, loss = 5.70268832
Iteration 206, loss = 5.70238394
Iteration 207, loss = 5.70204850
Iteration 208, loss = 5.70186983
Iteration 209, loss = 5.70147898
Iteration 210, loss = 5.70134357
Iteration 211, loss = 5.70114370
Iteration 212, loss = 5.70075728
Iteration 213, loss = 5.70057617
Iteration 214, loss = 5.70028333
Iteration 215, loss = 5.70003023
Iteration 216, loss = 5.69980282
Iteration 217, loss = 5.69955557
Iteration 218, loss = 5.69935887
Iteration 219, loss = 5.69903373
Iteration 220, loss = 5.69883166
Iteration 221, loss = 5.69861187
Iteration 222, loss = 5.69836838
Iteration 223, loss = 5.69800581
Iteration 224, loss = 5.69786064
Iteration 225, loss = 5.69757444
Iteration 226, loss = 5.69767768
Iteration 227, loss = 5.69710884
Iteration 228, loss = 5.69704140
Iteration 229, loss = 5.69670706
Iteration 230, loss = 5.69653579
Iteration 231, loss = 5.69629803
Iteration 232, loss = 5.69606206
Iteration 233, loss = 5.69580571
Iteration 234, loss = 5.69565544
Iteration 235, loss = 5.69532598
Iteration 236, loss = 5.69521945
Iteration 237, loss = 5.69501327
Iteration 238, loss = 5.69487914
Iteration 239, loss = 5.69456356
Iteration 240, loss = 5.69441182
Iteration 241, loss = 5.69409781
Iteration 242, loss = 5.69401157
Iteration 243, loss = 5.69389658
Iteration 244, loss = 5.69360114
Iteration 245, loss = 5.69341609
Iteration 246, loss = 5.69316755
Iteration 247, loss = 5.69289196
Iteration 248, loss = 5.69277104
Iteration 249, loss = 5.69266563
Iteration 250, loss = 5.69241318
Iteration 251, loss = 5.69228892
Iteration 252, loss = 5.69200113
Iteration 253, loss = 5.69178164
Iteration 254, loss = 5.69161246
Iteration 255, loss = 5.69145788
Iteration 256, loss = 5.69128260
Iteration 257, loss = 5.69104686
Iteration 258, loss = 5.69090198
Iteration 259, loss = 5.69070550
Iteration 260, loss = 5.69065878
Iteration 261, loss = 5.69033552
Iteration 262, loss = 5.69024588
Iteration 263, loss = 5.68997951
Iteration 264, loss = 5.68972617
Iteration 265, loss = 5.68963561
Iteration 266, loss = 5.68945072
Iteration 267, loss = 5.68936365
Iteration 268, loss = 5.68917696
Iteration 269, loss = 5.68892955
Iteration 270, loss = 5.68879567
Iteration 271, loss = 5.68858496
Iteration 272, loss = 5.68851095
Iteration 273, loss = 5.68832614
Iteration 274, loss = 5.68828886
Iteration 275, loss = 5.68797402
Iteration 276, loss = 5.68787125
Iteration 277, loss = 5.68764955
Iteration 278, loss = 5.68762314
Iteration 279, loss = 5.68739630
Iteration 280, loss = 5.68724132
Iteration 281, loss = 5.68703096
Iteration 282, loss = 5.68695294
Iteration 283, loss = 5.68671001
Iteration 284, loss = 5.68645788
Iteration 285, loss = 5.68641103
Iteration 286, loss = 5.68622376
Iteration 287, loss = 5.68606097
Iteration 288, loss = 5.68598539
Iteration 289, loss = 5.68578870
Iteration 290, loss = 5.68549804
Iteration 291, loss = 5.68544324
Iteration 292, loss = 5.68546051
Iteration 293, loss = 5.68512508
Iteration 294, loss = 5.68494155
Iteration 295, loss = 5.68483964
Iteration 296, loss = 5.68479637
Iteration 297, loss = 5.68461638
Iteration 298, loss = 5.68443516
Iteration 299, loss = 5.68434755
Iteration 300, loss = 5.68417121
Iteration 301, loss = 5.68401166
Iteration 302, loss = 5.68381154
Iteration 303, loss = 5.68373102
Iteration 304, loss = 5.68382425
Iteration 305, loss = 5.68354576
Iteration 306, loss = 5.68345197
Iteration 307, loss = 5.68325693
Iteration 308, loss = 5.68305059
Iteration 309, loss = 5.68298517
Iteration 310, loss = 5.68278327
Iteration 311, loss = 5.68262714
Iteration 312, loss = 5.68249483
Iteration 313, loss = 5.68228222
Iteration 314, loss = 5.68241353
Iteration 315, loss = 5.68222703
Iteration 316, loss = 5.68212919
Iteration 317, loss = 5.68192831
Iteration 318, loss = 5.68185851
Iteration 319, loss = 5.68163222
Iteration 320, loss = 5.68161491
Iteration 321, loss = 5.68146131
Iteration 322, loss = 5.68124232
Iteration 323, loss = 5.68112701
Iteration 324, loss = 5.68103589
Iteration 325, loss = 5.68086365
Iteration 326, loss = 5.68074025
Iteration 327, loss = 5.68061822
Iteration 328, loss = 5.68043435
Iteration 329, loss = 5.68032613
Iteration 330, loss = 5.68045461
Iteration 331, loss = 5.68010299
Iteration 332, loss = 5.67999612
Iteration 333, loss = 5.67998129
Iteration 334, loss = 5.67969332
Iteration 335, loss = 5.67970020
Iteration 336, loss = 5.67956354
Iteration 337, loss = 5.67946019
Iteration 338, loss = 5.67946872
Iteration 339, loss = 5.67923875
Iteration 340, loss = 5.67904515
Iteration 341, loss = 5.67902708
Iteration 342, loss = 5.67883773
Iteration 343, loss = 5.67868548
Iteration 344, loss = 5.67868651
Iteration 345, loss = 5.67879213
Iteration 346, loss = 5.67845169
Iteration 347, loss = 5.67828747
Iteration 348, loss = 5.67820798
Iteration 349, loss = 5.67813018
Iteration 350, loss = 5.67806855
Iteration 351, loss = 5.67786332
Iteration 352, loss = 5.67782177
Iteration 353, loss = 5.67768196
Iteration 354, loss = 5.67747634
Iteration 355, loss = 5.67742483
Iteration 356, loss = 5.67729971
Iteration 357, loss = 5.67725055
Iteration 358, loss = 5.67718957
Iteration 359, loss = 5.67706892
Iteration 360, loss = 5.67689861
Iteration 361, loss = 5.67661558
Iteration 362, loss = 5.67665931
Iteration 363, loss = 5.67659686
Iteration 364, loss = 5.67649025
Iteration 365, loss = 5.67643736
Iteration 366, loss = 5.67626027
Iteration 367, loss = 5.67617731
Iteration 368, loss = 5.67600619
Iteration 369, loss = 5.67595436
Iteration 370, loss = 5.67599601
Iteration 371, loss = 5.67581886
Iteration 372, loss = 5.67559785
Iteration 373, loss = 5.67567523
Iteration 374, loss = 5.67554566
Iteration 375, loss = 5.67536184
Iteration 376, loss = 5.67531981
Iteration 377, loss = 5.67523191
Iteration 378, loss = 5.67519087
Iteration 379, loss = 5.67512069
Iteration 380, loss = 5.67531678
Iteration 381, loss = 5.67479749
Iteration 382, loss = 5.67474367
Iteration 383, loss = 5.67460060
Iteration 384, loss = 5.67458227
Iteration 385, loss = 5.67429470
Iteration 386, loss = 5.67429651
Iteration 387, loss = 5.67423696
Iteration 388, loss = 5.67406130
Iteration 389, loss = 5.67417490
Iteration 390, loss = 5.67401232
Iteration 391, loss = 5.67382847
Iteration 392, loss = 5.67374419
Iteration 393, loss = 5.67371341
Iteration 394, loss = 5.67369856
Iteration 395, loss = 5.67360819
Iteration 396, loss = 5.67344246
Iteration 397, loss = 5.67323475
Iteration 398, loss = 5.67325282
Iteration 399, loss = 5.67329718
Iteration 400, loss = 5.67301165
Iteration 401, loss = 5.67289852
Iteration 402, loss = 5.67305189
Iteration 403, loss = 5.67304519
Iteration 404, loss = 5.67269448
Iteration 405, loss = 5.67261199
Iteration 406, loss = 5.67253196
Iteration 407, loss = 5.67235209
Iteration 408, loss = 5.67252440
Iteration 409, loss = 5.67237403
Iteration 410, loss = 5.67216090
Iteration 411, loss = 5.67220023
Iteration 412, loss = 5.67218530
Iteration 413, loss = 5.67226964
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((1, 1), {'error': 2.1529508507712314, 'fit': 0.021874999999999999, 'time': 486.0})
Iteration 1, loss = 6.52362328
Iteration 2, loss = 6.44436274
Iteration 3, loss = 6.33124152
Iteration 4, loss = 6.19482713
Iteration 5, loss = 6.07029103
Iteration 6, loss = 5.97851741
Iteration 7, loss = 5.91479746
Iteration 8, loss = 5.87019695
Iteration 9, loss = 5.83568955
Iteration 10, loss = 5.80638141
Iteration 11, loss = 5.78102278
Iteration 12, loss = 5.75723200
Iteration 13, loss = 5.73514632
Iteration 14, loss = 5.71370497
Iteration 15, loss = 5.69350420
Iteration 16, loss = 5.67422116
Iteration 17, loss = 5.65549070
Iteration 18, loss = 5.63747890
Iteration 19, loss = 5.62091865
Iteration 20, loss = 5.60475656
Iteration 21, loss = 5.58966441
Iteration 22, loss = 5.57524259
Iteration 23, loss = 5.56225254
Iteration 24, loss = 5.54999245
Iteration 25, loss = 5.53781836
Iteration 26, loss = 5.52656508
Iteration 27, loss = 5.51602637
Iteration 28, loss = 5.50577062
Iteration 29, loss = 5.49610245
Iteration 30, loss = 5.48679094
Iteration 31, loss = 5.47807932
Iteration 32, loss = 5.46984203
Iteration 33, loss = 5.46158256
Iteration 34, loss = 5.45395626
Iteration 35, loss = 5.44616959
Iteration 36, loss = 5.43927652
Iteration 37, loss = 5.43246107
Iteration 38, loss = 5.42590057
Iteration 39, loss = 5.41985551
Iteration 40, loss = 5.41391454
Iteration 41, loss = 5.40821679
Iteration 42, loss = 5.40238514
Iteration 43, loss = 5.39731539
Iteration 44, loss = 5.39191259
Iteration 45, loss = 5.38682883
Iteration 46, loss = 5.38194878
Iteration 47, loss = 5.37716424
Iteration 48, loss = 5.37267534
Iteration 49, loss = 5.36816009
Iteration 50, loss = 5.36375770
Iteration 51, loss = 5.35932382
Iteration 52, loss = 5.35530381
Iteration 53, loss = 5.35110361
Iteration 54, loss = 5.34706099
Iteration 55, loss = 5.34309173
Iteration 56, loss = 5.33943062
Iteration 57, loss = 5.33575578
Iteration 58, loss = 5.33188599
Iteration 59, loss = 5.32831893
Iteration 60, loss = 5.32494146
Iteration 61, loss = 5.32141495
Iteration 62, loss = 5.31785106
Iteration 63, loss = 5.31440742
Iteration 64, loss = 5.31124071
Iteration 65, loss = 5.30771056
Iteration 66, loss = 5.30433691
Iteration 67, loss = 5.30099027
Iteration 68, loss = 5.29792294
Iteration 69, loss = 5.29449013
Iteration 70, loss = 5.29116981
Iteration 71, loss = 5.28788943
Iteration 72, loss = 5.28459541
Iteration 73, loss = 5.28141716
Iteration 74, loss = 5.27807797
Iteration 75, loss = 5.27481123
Iteration 76, loss = 5.27171206
Iteration 77, loss = 5.26850716
Iteration 78, loss = 5.26523395
Iteration 79, loss = 5.26194764
Iteration 80, loss = 5.25892265
Iteration 81, loss = 5.25575915
Iteration 82, loss = 5.25248519
Iteration 83, loss = 5.24939852
Iteration 84, loss = 5.24622556
Iteration 85, loss = 5.24316053
Iteration 86, loss = 5.24033196
Iteration 87, loss = 5.23718524
Iteration 88, loss = 5.23431811
Iteration 89, loss = 5.23137132
Iteration 90, loss = 5.22888735
Iteration 91, loss = 5.22589433
Iteration 92, loss = 5.22298633
Iteration 93, loss = 5.22012315
Iteration 94, loss = 5.21765167
Iteration 95, loss = 5.21481213
Iteration 96, loss = 5.21194738
Iteration 97, loss = 5.20938868
Iteration 98, loss = 5.20692252
Iteration 99, loss = 5.20439253
Iteration 100, loss = 5.20177607
Iteration 101, loss = 5.19931217
Iteration 102, loss = 5.19670698
Iteration 103, loss = 5.19449936
Iteration 104, loss = 5.19180840
Iteration 105, loss = 5.18963067
Iteration 106, loss = 5.18714487
Iteration 107, loss = 5.18494283
Iteration 108, loss = 5.18240416
Iteration 109, loss = 5.18030615
Iteration 110, loss = 5.17832171
Iteration 111, loss = 5.17574395
Iteration 112, loss = 5.17355385
Iteration 113, loss = 5.17135627
Iteration 114, loss = 5.16914426
Iteration 115, loss = 5.16702642
Iteration 116, loss = 5.16502573
Iteration 117, loss = 5.16296146
Iteration 118, loss = 5.16098054
Iteration 119, loss = 5.15876044
Iteration 120, loss = 5.15692213
Iteration 121, loss = 5.15486944
Iteration 122, loss = 5.15296009
Iteration 123, loss = 5.15122412
Iteration 124, loss = 5.14909600
Iteration 125, loss = 5.14721473
Iteration 126, loss = 5.14522770
Iteration 127, loss = 5.14356420
Iteration 128, loss = 5.14145560
Iteration 129, loss = 5.13950700
Iteration 130, loss = 5.13764449
Iteration 131, loss = 5.13597489
Iteration 132, loss = 5.13411144
Iteration 133, loss = 5.13258412
Iteration 134, loss = 5.13066281
Iteration 135, loss = 5.12906445
Iteration 136, loss = 5.12687507
Iteration 137, loss = 5.12526791
Iteration 138, loss = 5.12351736
Iteration 139, loss = 5.12217670
Iteration 140, loss = 5.12023971
Iteration 141, loss = 5.11881397
Iteration 142, loss = 5.11716930
Iteration 143, loss = 5.11541114
Iteration 144, loss = 5.11386198
Iteration 145, loss = 5.11201280
Iteration 146, loss = 5.11056101
Iteration 147, loss = 5.10908223
Iteration 148, loss = 5.10769533
Iteration 149, loss = 5.10596032
Iteration 150, loss = 5.10434298
Iteration 151, loss = 5.10296240
Iteration 152, loss = 5.10162834
Iteration 153, loss = 5.09997312
Iteration 154, loss = 5.09864664
Iteration 155, loss = 5.09700654
Iteration 156, loss = 5.09566529
Iteration 157, loss = 5.09410537
Iteration 158, loss = 5.09254987
Iteration 159, loss = 5.09102688
Iteration 160, loss = 5.08952959
Iteration 161, loss = 5.08850002
Iteration 162, loss = 5.08719682
Iteration 163, loss = 5.08549176
Iteration 164, loss = 5.08409335
Iteration 165, loss = 5.08286620
Iteration 166, loss = 5.08182644
Iteration 167, loss = 5.08019673
Iteration 168, loss = 5.07902701
Iteration 169, loss = 5.07750858
Iteration 170, loss = 5.07601762
Iteration 171, loss = 5.07492458
Iteration 172, loss = 5.07352538
Iteration 173, loss = 5.07228426
Iteration 174, loss = 5.07103482
Iteration 175, loss = 5.06956695
Iteration 176, loss = 5.06849438
Iteration 177, loss = 5.06750921
Iteration 178, loss = 5.06600460
Iteration 179, loss = 5.06481662
Iteration 180, loss = 5.06368118
Iteration 181, loss = 5.06252819
Iteration 182, loss = 5.06118407
Iteration 183, loss = 5.05987288
Iteration 184, loss = 5.05881363
Iteration 185, loss = 5.05751509
Iteration 186, loss = 5.05633015
Iteration 187, loss = 5.05514254
Iteration 188, loss = 5.05406955
Iteration 189, loss = 5.05271893
Iteration 190, loss = 5.05190683
Iteration 191, loss = 5.05046059
Iteration 192, loss = 5.04927812
Iteration 193, loss = 5.04797940
Iteration 194, loss = 5.04709114
Iteration 195, loss = 5.04587175
Iteration 196, loss = 5.04482789
Iteration 197, loss = 5.04375457
Iteration 198, loss = 5.04267383
Iteration 199, loss = 5.04149988
Iteration 200, loss = 5.04056810
Iteration 201, loss = 5.03906702
Iteration 202, loss = 5.03818275
Iteration 203, loss = 5.03726346
Iteration 204, loss = 5.03599529
Iteration 205, loss = 5.03501020
Iteration 206, loss = 5.03383097
Iteration 207, loss = 5.03286789
Iteration 208, loss = 5.03181211
Iteration 209, loss = 5.03101530
Iteration 210, loss = 5.02970093
Iteration 211, loss = 5.02872543
Iteration 212, loss = 5.02747211
Iteration 213, loss = 5.02666074
Iteration 214, loss = 5.02567927
Iteration 215, loss = 5.02453968
Iteration 216, loss = 5.02349622
Iteration 217, loss = 5.02244176
Iteration 218, loss = 5.02146036
Iteration 219, loss = 5.02074584
Iteration 220, loss = 5.01965777
Iteration 221, loss = 5.01859814
Iteration 222, loss = 5.01765868
Iteration 223, loss = 5.01645541
Iteration 224, loss = 5.01569592
Iteration 225, loss = 5.01452512
Iteration 226, loss = 5.01382499
Iteration 227, loss = 5.01278872
Iteration 228, loss = 5.01174739
Iteration 229, loss = 5.01107647
Iteration 230, loss = 5.01024248
Iteration 231, loss = 5.00930760
Iteration 232, loss = 5.00801492
Iteration 233, loss = 5.00718136
Iteration 234, loss = 5.00611354
Iteration 235, loss = 5.00526770
Iteration 236, loss = 5.00435709
Iteration 237, loss = 5.00366611
Iteration 238, loss = 5.00266251
Iteration 239, loss = 5.00167507
Iteration 240, loss = 5.00076480
Iteration 241, loss = 4.99967255
Iteration 242, loss = 4.99904763
Iteration 243, loss = 4.99837339
Iteration 244, loss = 4.99735322
Iteration 245, loss = 4.99634103
Iteration 246, loss = 4.99540065
Iteration 247, loss = 4.99467937
Iteration 248, loss = 4.99396384
Iteration 249, loss = 4.99282264
Iteration 250, loss = 4.99189314
Iteration 251, loss = 4.99084783
Iteration 252, loss = 4.99038243
Iteration 253, loss = 4.98948014
Iteration 254, loss = 4.98847632
Iteration 255, loss = 4.98758424
Iteration 256, loss = 4.98693033
Iteration 257, loss = 4.98602262
Iteration 258, loss = 4.98524577
Iteration 259, loss = 4.98444626
Iteration 260, loss = 4.98335292
Iteration 261, loss = 4.98271219
Iteration 262, loss = 4.98176960
Iteration 263, loss = 4.98115645
Iteration 264, loss = 4.98019452
Iteration 265, loss = 4.97920137
Iteration 266, loss = 4.97859802
Iteration 267, loss = 4.97767220
Iteration 268, loss = 4.97671458
Iteration 269, loss = 4.97574588
Iteration 270, loss = 4.97527411
Iteration 271, loss = 4.97449063
Iteration 272, loss = 4.97341609
Iteration 273, loss = 4.97287031
Iteration 274, loss = 4.97158993
Iteration 275, loss = 4.97133291
Iteration 276, loss = 4.97044457
Iteration 277, loss = 4.96991135
Iteration 278, loss = 4.96889913
Iteration 279, loss = 4.96781822
Iteration 280, loss = 4.96718557
Iteration 281, loss = 4.96679199
Iteration 282, loss = 4.96563439
Iteration 283, loss = 4.96490055
Iteration 284, loss = 4.96380063
Iteration 285, loss = 4.96338348
Iteration 286, loss = 4.96270165
Iteration 287, loss = 4.96187867
Iteration 288, loss = 4.96083076
Iteration 289, loss = 4.96009591
Iteration 290, loss = 4.95935799
Iteration 291, loss = 4.95851561
Iteration 292, loss = 4.95769516
Iteration 293, loss = 4.95713085
Iteration 294, loss = 4.95622818
Iteration 295, loss = 4.95539443
Iteration 296, loss = 4.95489283
Iteration 297, loss = 4.95381391
Iteration 298, loss = 4.95332413
Iteration 299, loss = 4.95226407
Iteration 300, loss = 4.95182094
Iteration 301, loss = 4.95075570
Iteration 302, loss = 4.94988322
Iteration 303, loss = 4.94939571
Iteration 304, loss = 4.94859365
Iteration 305, loss = 4.94811982
Iteration 306, loss = 4.94693932
Iteration 307, loss = 4.94631712
Iteration 308, loss = 4.94561020
Iteration 309, loss = 4.94469264
Iteration 310, loss = 4.94415720
Iteration 311, loss = 4.94334284
Iteration 312, loss = 4.94235817
Iteration 313, loss = 4.94168256
Iteration 314, loss = 4.94109453
Iteration 315, loss = 4.94013302
Iteration 316, loss = 4.93953200
Iteration 317, loss = 4.93882714
Iteration 318, loss = 4.93794010
Iteration 319, loss = 4.93717153
Iteration 320, loss = 4.93643383
Iteration 321, loss = 4.93560903
Iteration 322, loss = 4.93479081
Iteration 323, loss = 4.93400094
Iteration 324, loss = 4.93331506
Iteration 325, loss = 4.93238224
Iteration 326, loss = 4.93174577
Iteration 327, loss = 4.93099633
Iteration 328, loss = 4.93039952
Iteration 329, loss = 4.92936471
Iteration 330, loss = 4.92879206
Iteration 331, loss = 4.92820389
Iteration 332, loss = 4.92734406
Iteration 333, loss = 4.92628253
Iteration 334, loss = 4.92560774
Iteration 335, loss = 4.92511687
Iteration 336, loss = 4.92417412
Iteration 337, loss = 4.92377784
Iteration 338, loss = 4.92243781
Iteration 339, loss = 4.92217258
Iteration 340, loss = 4.92129066
Iteration 341, loss = 4.92011113
Iteration 342, loss = 4.91959351
Iteration 343, loss = 4.91902981
Iteration 344, loss = 4.91800036
Iteration 345, loss = 4.91737625
Iteration 346, loss = 4.91643182
Iteration 347, loss = 4.91580645
Iteration 348, loss = 4.91504890
Iteration 349, loss = 4.91441222
Iteration 350, loss = 4.91385691
Iteration 351, loss = 4.91298484
Iteration 352, loss = 4.91192394
Iteration 353, loss = 4.91128860
Iteration 354, loss = 4.91028920
Iteration 355, loss = 4.90993630
Iteration 356, loss = 4.90921055
Iteration 357, loss = 4.90828913
Iteration 358, loss = 4.90756632
Iteration 359, loss = 4.90660780
Iteration 360, loss = 4.90618406
Iteration 361, loss = 4.90533398
Iteration 362, loss = 4.90446872
Iteration 363, loss = 4.90423702
Iteration 364, loss = 4.90292679
Iteration 365, loss = 4.90237350
Iteration 366, loss = 4.90151351
Iteration 367, loss = 4.90100550
Iteration 368, loss = 4.90007780
Iteration 369, loss = 4.89934841
Iteration 370, loss = 4.89892064
Iteration 371, loss = 4.89818178
Iteration 372, loss = 4.89715095
Iteration 373, loss = 4.89688170
Iteration 374, loss = 4.89550757
Iteration 375, loss = 4.89507973
Iteration 376, loss = 4.89443430
Iteration 377, loss = 4.89359463
Iteration 378, loss = 4.89265085
Iteration 379, loss = 4.89203734
Iteration 380, loss = 4.89145299
Iteration 381, loss = 4.89045041
Iteration 382, loss = 4.89004771
Iteration 383, loss = 4.88935539
Iteration 384, loss = 4.88842520
Iteration 385, loss = 4.88818747
Iteration 386, loss = 4.88726801
Iteration 387, loss = 4.88669164
Iteration 388, loss = 4.88597415
Iteration 389, loss = 4.88510997
Iteration 390, loss = 4.88419214
Iteration 391, loss = 4.88370588
Iteration 392, loss = 4.88303218
Iteration 393, loss = 4.88262434
Iteration 394, loss = 4.88174750
Iteration 395, loss = 4.88118158
Iteration 396, loss = 4.88032655
Iteration 397, loss = 4.87984073
Iteration 398, loss = 4.87923106
Iteration 399, loss = 4.87850645
Iteration 400, loss = 4.87781288
Iteration 401, loss = 4.87711113
Iteration 402, loss = 4.87672829
Iteration 403, loss = 4.87591301
Iteration 404, loss = 4.87529556
Iteration 405, loss = 4.87466642
Iteration 406, loss = 4.87422990
Iteration 407, loss = 4.87343876
Iteration 408, loss = 4.87287324
Iteration 409, loss = 4.87204129
Iteration 410, loss = 4.87144034
Iteration 411, loss = 4.87088206
Iteration 412, loss = 4.87014202
Iteration 413, loss = 4.86951646
Iteration 414, loss = 4.86900746
Iteration 415, loss = 4.86829099
Iteration 416, loss = 4.86789729
Iteration 417, loss = 4.86723107
Iteration 418, loss = 4.86632494
Iteration 419, loss = 4.86563288
Iteration 420, loss = 4.86560801
Iteration 421, loss = 4.86494804
Iteration 422, loss = 4.86380375
Iteration 423, loss = 4.86343613
Iteration 424, loss = 4.86307159
Iteration 425, loss = 4.86218915
Iteration 426, loss = 4.86170555
Iteration 427, loss = 4.86107815
Iteration 428, loss = 4.86073578
Iteration 429, loss = 4.85997233
Iteration 430, loss = 4.85956192
Iteration 431, loss = 4.85873365
Iteration 432, loss = 4.85827253
Iteration 433, loss = 4.85787201
Iteration 434, loss = 4.85698861
Iteration 435, loss = 4.85667966
Iteration 436, loss = 4.85599004
Iteration 437, loss = 4.85540470
Iteration 438, loss = 4.85480946
Iteration 439, loss = 4.85406338
Iteration 440, loss = 4.85389877
Iteration 441, loss = 4.85315469
Iteration 442, loss = 4.85263603
Iteration 443, loss = 4.85223567
Iteration 444, loss = 4.85152168
Iteration 445, loss = 4.85072515
Iteration 446, loss = 4.85070183
Iteration 447, loss = 4.85017392
Iteration 448, loss = 4.84934695
Iteration 449, loss = 4.84887137
Iteration 450, loss = 4.84848042
Iteration 451, loss = 4.84744537
Iteration 452, loss = 4.84728565
Iteration 453, loss = 4.84683995
Iteration 454, loss = 4.84650841
Iteration 455, loss = 4.84599408
Iteration 456, loss = 4.84534657
Iteration 457, loss = 4.84481982
Iteration 458, loss = 4.84447334
Iteration 459, loss = 4.84372161
Iteration 460, loss = 4.84308341
Iteration 461, loss = 4.84284265
Iteration 462, loss = 4.84248667
Iteration 463, loss = 4.84193322
Iteration 464, loss = 4.84143921
Iteration 465, loss = 4.84090023
Iteration 466, loss = 4.84078444
Iteration 467, loss = 4.84000119
Iteration 468, loss = 4.83934452
Iteration 469, loss = 4.83879454
Iteration 470, loss = 4.83842828
Iteration 471, loss = 4.83822030
Iteration 472, loss = 4.83747530
Iteration 473, loss = 4.83686015
Iteration 474, loss = 4.83678302
Iteration 475, loss = 4.83624244
Iteration 476, loss = 4.83567737
Iteration 477, loss = 4.83532417
Iteration 478, loss = 4.83462079
Iteration 479, loss = 4.83408825
Iteration 480, loss = 4.83373637
Iteration 481, loss = 4.83346644
Iteration 482, loss = 4.83259676
Iteration 483, loss = 4.83248500
Iteration 484, loss = 4.83178222
Iteration 485, loss = 4.83174187
Iteration 486, loss = 4.83095401
Iteration 487, loss = 4.83063118
Iteration 488, loss = 4.83013121
Iteration 489, loss = 4.82976972
Iteration 490, loss = 4.82968279
Iteration 491, loss = 4.82877170
Iteration 492, loss = 4.82840231
Iteration 493, loss = 4.82770954
Iteration 494, loss = 4.82745255
Iteration 495, loss = 4.82679723
Iteration 496, loss = 4.82636696
Iteration 497, loss = 4.82618215
Iteration 498, loss = 4.82552411
Iteration 499, loss = 4.82534297
Iteration 500, loss = 4.82486659
Iteration 501, loss = 4.82477122
Iteration 502, loss = 4.82402081
Iteration 503, loss = 4.82367856
Iteration 504, loss = 4.82336000
Iteration 505, loss = 4.82256191
Iteration 506, loss = 4.82244686
Iteration 507, loss = 4.82186724
Iteration 508, loss = 4.82153494
Iteration 509, loss = 4.82072891
Iteration 510, loss = 4.82082373
Iteration 511, loss = 4.82031182
Iteration 512, loss = 4.81994366
Iteration 513, loss = 4.81965803
Iteration 514, loss = 4.81913891
Iteration 515, loss = 4.81848440
Iteration 516, loss = 4.81847482
Iteration 517, loss = 4.81809957
Iteration 518, loss = 4.81735077
Iteration 519, loss = 4.81700305
Iteration 520, loss = 4.81695197
Iteration 521, loss = 4.81633621
Iteration 522, loss = 4.81568460
Iteration 523, loss = 4.81537813
Iteration 524, loss = 4.81515401
Iteration 525, loss = 4.81466755
Iteration 526, loss = 4.81445830
Iteration 527, loss = 4.81438044
Iteration 528, loss = 4.81349121
Iteration 529, loss = 4.81312941
Iteration 530, loss = 4.81267670
Iteration 531, loss = 4.81232998
Iteration 532, loss = 4.81194745
Iteration 533, loss = 4.81192334
Iteration 534, loss = 4.81123369
Iteration 535, loss = 4.81075752
Iteration 536, loss = 4.81029810
Iteration 537, loss = 4.81010367
Iteration 538, loss = 4.80998895
Iteration 539, loss = 4.80991572
Iteration 540, loss = 4.80902547
Iteration 541, loss = 4.80867915
Iteration 542, loss = 4.80821869
Iteration 543, loss = 4.80773053
Iteration 544, loss = 4.80785244
Iteration 545, loss = 4.80725981
Iteration 546, loss = 4.80681178
Iteration 547, loss = 4.80664864
Iteration 548, loss = 4.80619674
Iteration 549, loss = 4.80581691
Iteration 550, loss = 4.80535558
Iteration 551, loss = 4.80511153
Iteration 552, loss = 4.80436366
Iteration 553, loss = 4.80440396
Iteration 554, loss = 4.80420905
Iteration 555, loss = 4.80355451
Iteration 556, loss = 4.80330457
Iteration 557, loss = 4.80299338
Iteration 558, loss = 4.80225021
Iteration 559, loss = 4.80221594
Iteration 560, loss = 4.80185077
Iteration 561, loss = 4.80157498
Iteration 562, loss = 4.80183840
Iteration 563, loss = 4.80070596
Iteration 564, loss = 4.80050948
Iteration 565, loss = 4.80017287
Iteration 566, loss = 4.79996688
Iteration 567, loss = 4.79940779
Iteration 568, loss = 4.79892699
Iteration 569, loss = 4.79909031
Iteration 570, loss = 4.79872202
Iteration 571, loss = 4.79838832
Iteration 572, loss = 4.79777296
Iteration 573, loss = 4.79748691
Iteration 574, loss = 4.79698289
Iteration 575, loss = 4.79661789
Iteration 576, loss = 4.79611925
Iteration 577, loss = 4.79594828
Iteration 578, loss = 4.79577232
Iteration 579, loss = 4.79523406
Iteration 580, loss = 4.79568807
Iteration 581, loss = 4.79465805
Iteration 582, loss = 4.79431688
Iteration 583, loss = 4.79428935
Iteration 584, loss = 4.79366242
Iteration 585, loss = 4.79326523
Iteration 586, loss = 4.79305018
Iteration 587, loss = 4.79285287
Iteration 588, loss = 4.79288126
Iteration 589, loss = 4.79278198
Iteration 590, loss = 4.79217444
Iteration 591, loss = 4.79187905
Iteration 592, loss = 4.79135384
Iteration 593, loss = 4.79108725
Iteration 594, loss = 4.79052979
Iteration 595, loss = 4.79056859
Iteration 596, loss = 4.79033612
Iteration 597, loss = 4.78969102
Iteration 598, loss = 4.78925822
Iteration 599, loss = 4.78945405
Iteration 600, loss = 4.78864785
Iteration 601, loss = 4.78849748
Iteration 602, loss = 4.78841500
Iteration 603, loss = 4.78792318
Iteration 604, loss = 4.78780151
Iteration 605, loss = 4.78734824
Iteration 606, loss = 4.78725077
Iteration 607, loss = 4.78708719
Iteration 608, loss = 4.78620019
Iteration 609, loss = 4.78610984
Iteration 610, loss = 4.78595940
Iteration 611, loss = 4.78557368
Iteration 612, loss = 4.78551328
Iteration 613, loss = 4.78489343
Iteration 614, loss = 4.78456997
Iteration 615, loss = 4.78450582
Iteration 616, loss = 4.78428289
Iteration 617, loss = 4.78345041
Iteration 618, loss = 4.78353774
Iteration 619, loss = 4.78331498
Iteration 620, loss = 4.78301093
Iteration 621, loss = 4.78244143
Iteration 622, loss = 4.78230401
Iteration 623, loss = 4.78162075
Iteration 624, loss = 4.78164637
Iteration 625, loss = 4.78187379
Iteration 626, loss = 4.78082863
Iteration 627, loss = 4.78088136
Iteration 628, loss = 4.78089336
Iteration 629, loss = 4.78048935
Iteration 630, loss = 4.77991335
Iteration 631, loss = 4.77956263
Iteration 632, loss = 4.77966131
Iteration 633, loss = 4.77914124
Iteration 634, loss = 4.77935757
Iteration 635, loss = 4.77870245
Iteration 636, loss = 4.77845158
Iteration 637, loss = 4.77815743
Iteration 638, loss = 4.77790085
Iteration 639, loss = 4.77775220
Iteration 640, loss = 4.77737682
Iteration 641, loss = 4.77730553
Iteration 642, loss = 4.77675703
Iteration 643, loss = 4.77621342
Iteration 644, loss = 4.77631747
Iteration 645, loss = 4.77606383
Iteration 646, loss = 4.77561364
Iteration 647, loss = 4.77516274
Iteration 648, loss = 4.77518910
Iteration 649, loss = 4.77501942
Iteration 650, loss = 4.77476294
Iteration 651, loss = 4.77435164
Iteration 652, loss = 4.77383413
Iteration 653, loss = 4.77364583
Iteration 654, loss = 4.77355663
Iteration 655, loss = 4.77339750
Iteration 656, loss = 4.77306548
Iteration 657, loss = 4.77314558
Iteration 658, loss = 4.77263796
Iteration 659, loss = 4.77227996
Iteration 660, loss = 4.77197956
Iteration 661, loss = 4.77174477
Iteration 662, loss = 4.77134785
Iteration 663, loss = 4.77144352
Iteration 664, loss = 4.77088850
Iteration 665, loss = 4.77136768
Iteration 666, loss = 4.77043225
Iteration 667, loss = 4.76997315
Iteration 668, loss = 4.76973230
Iteration 669, loss = 4.76945871
Iteration 670, loss = 4.76938438
Iteration 671, loss = 4.76931244
Iteration 672, loss = 4.76899184
Iteration 673, loss = 4.76876275
Iteration 674, loss = 4.76840256
Iteration 675, loss = 4.76811490
Iteration 676, loss = 4.76790280
Iteration 677, loss = 4.76786452
Iteration 678, loss = 4.76758787
Iteration 679, loss = 4.76699631
Iteration 680, loss = 4.76680337
Iteration 681, loss = 4.76629789
Iteration 682, loss = 4.76619464
Iteration 683, loss = 4.76617067
Iteration 684, loss = 4.76591268
Iteration 685, loss = 4.76542395
Iteration 686, loss = 4.76591207
Iteration 687, loss = 4.76555348
Iteration 688, loss = 4.76489575
Iteration 689, loss = 4.76439787
Iteration 690, loss = 4.76479846
Iteration 691, loss = 4.76453555
Iteration 692, loss = 4.76381352
Iteration 693, loss = 4.76332455
Iteration 694, loss = 4.76320577
Iteration 695, loss = 4.76323608
Iteration 696, loss = 4.76290186
Iteration 697, loss = 4.76277726
Iteration 698, loss = 4.76288912
Iteration 699, loss = 4.76235574
Iteration 700, loss = 4.76241566
Iteration 701, loss = 4.76207480
Iteration 702, loss = 4.76163703
Iteration 703, loss = 4.76115793
Iteration 704, loss = 4.76127814
Iteration 705, loss = 4.76068084
Iteration 706, loss = 4.76069096
Iteration 707, loss = 4.76030263
Iteration 708, loss = 4.76038780
Iteration 709, loss = 4.75985842
Iteration 710, loss = 4.75953769
Iteration 711, loss = 4.75942608
Iteration 712, loss = 4.75981393
Iteration 713, loss = 4.75911025
Iteration 714, loss = 4.75868322
Iteration 715, loss = 4.75874208
Iteration 716, loss = 4.75812354
Iteration 717, loss = 4.75819645
Iteration 718, loss = 4.75816805
Iteration 719, loss = 4.75794758
Iteration 720, loss = 4.75756193
Iteration 721, loss = 4.75684587
Iteration 722, loss = 4.75747713
Iteration 723, loss = 4.75680155
Iteration 724, loss = 4.75618316
Iteration 725, loss = 4.75589784
Iteration 726, loss = 4.75599775
Iteration 727, loss = 4.75620402
Iteration 728, loss = 4.75528634
Iteration 729, loss = 4.75565527
Iteration 730, loss = 4.75490023
Iteration 731, loss = 4.75467932
Iteration 732, loss = 4.75457601
Iteration 733, loss = 4.75449037
Iteration 734, loss = 4.75432058
Iteration 735, loss = 4.75406696
Iteration 736, loss = 4.75414157
Iteration 737, loss = 4.75329051
Iteration 738, loss = 4.75345806
Iteration 739, loss = 4.75325209
Iteration 740, loss = 4.75288225
Iteration 741, loss = 4.75294548
Iteration 742, loss = 4.75226189
Iteration 743, loss = 4.75238323
Iteration 744, loss = 4.75164291
Iteration 745, loss = 4.75196520
Iteration 746, loss = 4.75185546
Iteration 747, loss = 4.75141240
Iteration 748, loss = 4.75170528
Iteration 749, loss = 4.75078127
Iteration 750, loss = 4.75057570
Iteration 751, loss = 4.75044977
Iteration 752, loss = 4.75039579
Iteration 753, loss = 4.75015161
Iteration 754, loss = 4.75024362
Iteration 755, loss = 4.74965205
Iteration 756, loss = 4.74934842
Iteration 757, loss = 4.74955138
Iteration 758, loss = 4.74905110
Iteration 759, loss = 4.74888190
Iteration 760, loss = 4.74850116
Iteration 761, loss = 4.74852451
Iteration 762, loss = 4.74840116
Iteration 763, loss = 4.74786085
Iteration 764, loss = 4.74736647
Iteration 765, loss = 4.74787878
Iteration 766, loss = 4.74758934
Iteration 767, loss = 4.74710887
Iteration 768, loss = 4.74698769
Iteration 769, loss = 4.74677309
Iteration 770, loss = 4.74685895
Iteration 771, loss = 4.74609534
Iteration 772, loss = 4.74608798
Iteration 773, loss = 4.74600761
Iteration 774, loss = 4.74613204
Iteration 775, loss = 4.74541735
Iteration 776, loss = 4.74508323
Iteration 777, loss = 4.74512028
Iteration 778, loss = 4.74531117
Iteration 779, loss = 4.74491941
Iteration 780, loss = 4.74505256
Iteration 781, loss = 4.74420639
Iteration 782, loss = 4.74421340
Iteration 783, loss = 4.74373904
Iteration 784, loss = 4.74371670
Iteration 785, loss = 4.74354363
Iteration 786, loss = 4.74328337
Iteration 787, loss = 4.74319431
Iteration 788, loss = 4.74280933
Iteration 789, loss = 4.74251997
Iteration 790, loss = 4.74224670
Iteration 791, loss = 4.74230370
Iteration 792, loss = 4.74207090
Iteration 793, loss = 4.74200807
Iteration 794, loss = 4.74182058
Iteration 795, loss = 4.74183950
Iteration 796, loss = 4.74178375
Iteration 797, loss = 4.74152541
Iteration 798, loss = 4.74147139
Iteration 799, loss = 4.74045908
Iteration 800, loss = 4.74067535
Iteration 801, loss = 4.74031499
Iteration 802, loss = 4.74034532
Iteration 803, loss = 4.74024267
Iteration 804, loss = 4.73966175
Iteration 805, loss = 4.73963713
Iteration 806, loss = 4.73958327
Iteration 807, loss = 4.73926025
Iteration 808, loss = 4.73969778
Iteration 809, loss = 4.73861364
Iteration 810, loss = 4.73867805
Iteration 811, loss = 4.73863263
Iteration 812, loss = 4.73840733
Iteration 813, loss = 4.73794846
Iteration 814, loss = 4.73805363
Iteration 815, loss = 4.73762741
Iteration 816, loss = 4.73768879
Iteration 817, loss = 4.73795575
Iteration 818, loss = 4.73717298
Iteration 819, loss = 4.73746286
Iteration 820, loss = 4.73648952
Iteration 821, loss = 4.73685275
Iteration 822, loss = 4.73651741
Iteration 823, loss = 4.73627900
Iteration 824, loss = 4.73601109
Iteration 825, loss = 4.73610760
Iteration 826, loss = 4.73608698
Iteration 827, loss = 4.73547937
Iteration 828, loss = 4.73558010
Iteration 829, loss = 4.73513865
Iteration 830, loss = 4.73530620
Iteration 831, loss = 4.73486810
Iteration 832, loss = 4.73464691
Iteration 833, loss = 4.73441904
Iteration 834, loss = 4.73424478
Iteration 835, loss = 4.73409875
Iteration 836, loss = 4.73366925
Iteration 837, loss = 4.73368154
Iteration 838, loss = 4.73356682
Iteration 839, loss = 4.73322327
Iteration 840, loss = 4.73286242
Iteration 841, loss = 4.73301292
Iteration 842, loss = 4.73277863
Iteration 843, loss = 4.73211275
Iteration 844, loss = 4.73210638
Iteration 845, loss = 4.73195627
Iteration 846, loss = 4.73220769
Iteration 847, loss = 4.73173484
Iteration 848, loss = 4.73161206
Iteration 849, loss = 4.73140111
Iteration 850, loss = 4.73147061
Iteration 851, loss = 4.73143600
Iteration 852, loss = 4.73133295
Iteration 853, loss = 4.73047917
Iteration 854, loss = 4.73037757
Iteration 855, loss = 4.73048911
Iteration 856, loss = 4.73019816
Iteration 857, loss = 4.73007750
Iteration 858, loss = 4.72964749
Iteration 859, loss = 4.72954518
Iteration 860, loss = 4.72941250
Iteration 861, loss = 4.72930333
Iteration 862, loss = 4.72921200
Iteration 863, loss = 4.72938594
Iteration 864, loss = 4.72871478
Iteration 865, loss = 4.72859658
Iteration 866, loss = 4.72824454
Iteration 867, loss = 4.72850905
Iteration 868, loss = 4.72789303
Iteration 869, loss = 4.72804349
Iteration 870, loss = 4.72811933
Iteration 871, loss = 4.72777552
Iteration 872, loss = 4.72749274
Iteration 873, loss = 4.72725837
Iteration 874, loss = 4.72717733
Iteration 875, loss = 4.72688895
Iteration 876, loss = 4.72662347
Iteration 877, loss = 4.72717919
Iteration 878, loss = 4.72648074
Iteration 879, loss = 4.72628238
Iteration 880, loss = 4.72619085
Iteration 881, loss = 4.72583524
Iteration 882, loss = 4.72577100
Iteration 883, loss = 4.72551749
Iteration 884, loss = 4.72570307
Iteration 885, loss = 4.72507384
Iteration 886, loss = 4.72509062
Iteration 887, loss = 4.72488923
Iteration 888, loss = 4.72465302
Iteration 889, loss = 4.72465722
Iteration 890, loss = 4.72435758
Iteration 891, loss = 4.72427314
Iteration 892, loss = 4.72416873
Iteration 893, loss = 4.72419200
Iteration 894, loss = 4.72396662
Iteration 895, loss = 4.72380178
Iteration 896, loss = 4.72339512
Iteration 897, loss = 4.72347099
Iteration 898, loss = 4.72302034
Iteration 899, loss = 4.72298808
Iteration 900, loss = 4.72277084
Iteration 901, loss = 4.72264271
Iteration 902, loss = 4.72328698
Iteration 903, loss = 4.72293249
Iteration 904, loss = 4.72216970
Iteration 905, loss = 4.72196073
Iteration 906, loss = 4.72180801
Iteration 907, loss = 4.72177474
Iteration 908, loss = 4.72137293
Iteration 909, loss = 4.72160607
Iteration 910, loss = 4.72109542
Iteration 911, loss = 4.72080468
Iteration 912, loss = 4.72073866
Iteration 913, loss = 4.72056426
Iteration 914, loss = 4.72071323
Iteration 915, loss = 4.72042988
Iteration 916, loss = 4.72068098
Iteration 917, loss = 4.72005497
Iteration 918, loss = 4.72014219
Iteration 919, loss = 4.71995033
Iteration 920, loss = 4.71954901
Iteration 921, loss = 4.71930226
Iteration 922, loss = 4.71904021
Iteration 923, loss = 4.71922753
Iteration 924, loss = 4.71969145
Iteration 925, loss = 4.71889132
Iteration 926, loss = 4.71884299
Iteration 927, loss = 4.71868339
Iteration 928, loss = 4.71812497
Iteration 929, loss = 4.71806387
Iteration 930, loss = 4.71824724
Iteration 931, loss = 4.71763345
Iteration 932, loss = 4.71803528
Iteration 933, loss = 4.71747659
Iteration 934, loss = 4.71727598
Iteration 935, loss = 4.71734862
Iteration 936, loss = 4.71707319
Iteration 937, loss = 4.71635800
Iteration 938, loss = 4.71639140
Iteration 939, loss = 4.71650347
Iteration 940, loss = 4.71623833
Iteration 941, loss = 4.71653355
Iteration 942, loss = 4.71568883
Iteration 943, loss = 4.71617177
Iteration 944, loss = 4.71556286
Iteration 945, loss = 4.71531434
Iteration 946, loss = 4.71539211
Iteration 947, loss = 4.71590024
Iteration 948, loss = 4.71514738
Iteration 949, loss = 4.71530876
Iteration 950, loss = 4.71459452
Iteration 951, loss = 4.71442495
Iteration 952, loss = 4.71494023
Iteration 953, loss = 4.71506671
Iteration 954, loss = 4.71419665
Iteration 955, loss = 4.71396405
Iteration 956, loss = 4.71416146
Iteration 957, loss = 4.71359634
Iteration 958, loss = 4.71379557
Iteration 959, loss = 4.71349530
Iteration 960, loss = 4.71336243
Iteration 961, loss = 4.71313157
Iteration 962, loss = 4.71334471
Iteration 963, loss = 4.71268054
Iteration 964, loss = 4.71282579
Iteration 965, loss = 4.71268286
Iteration 966, loss = 4.71284230
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((10, 10), {'error': 0.86184723908418026, 'fit': 0.048000000000000001, 'time': 1215.0})
Iteration 1, loss = 6.35762134
Iteration 2, loss = 5.81394343
Iteration 3, loss = 5.66474390
Iteration 4, loss = 5.58992066
Iteration 5, loss = 5.53548998
Iteration 6, loss = 5.48936949
Iteration 7, loss = 5.45064413
Iteration 8, loss = 5.41539256
Iteration 9, loss = 5.38254611
Iteration 10, loss = 5.35084335
Iteration 11, loss = 5.32029388
Iteration 12, loss = 5.29313807
Iteration 13, loss = 5.26523222
Iteration 14, loss = 5.24073538
Iteration 15, loss = 5.21428548
Iteration 16, loss = 5.18966597
Iteration 17, loss = 5.16591104
Iteration 18, loss = 5.14215732
Iteration 19, loss = 5.12035695
Iteration 20, loss = 5.09510196
Iteration 21, loss = 5.07286282
Iteration 22, loss = 5.04842272
Iteration 23, loss = 5.02596283
Iteration 24, loss = 5.00368660
Iteration 25, loss = 4.98201901
Iteration 26, loss = 4.95944889
Iteration 27, loss = 4.93545808
Iteration 28, loss = 4.91358877
Iteration 29, loss = 4.89042063
Iteration 30, loss = 4.86939604
Iteration 31, loss = 4.84688187
Iteration 32, loss = 4.82481968
Iteration 33, loss = 4.80234192
Iteration 34, loss = 4.78082257
Iteration 35, loss = 4.75930465
Iteration 36, loss = 4.73777334
Iteration 37, loss = 4.71702692
Iteration 38, loss = 4.69569443
Iteration 39, loss = 4.67438637
Iteration 40, loss = 4.65266715
Iteration 41, loss = 4.63132658
Iteration 42, loss = 4.61097020
Iteration 43, loss = 4.59007570
Iteration 44, loss = 4.57018879
Iteration 45, loss = 4.55052141
Iteration 46, loss = 4.53118754
Iteration 47, loss = 4.50966118
Iteration 48, loss = 4.49007473
Iteration 49, loss = 4.47046157
Iteration 50, loss = 4.45231307
Iteration 51, loss = 4.43276978
Iteration 52, loss = 4.41437932
Iteration 53, loss = 4.39513122
Iteration 54, loss = 4.37676973
Iteration 55, loss = 4.35847065
Iteration 56, loss = 4.34009955
Iteration 57, loss = 4.32090138
Iteration 58, loss = 4.30257580
Iteration 59, loss = 4.28428779
Iteration 60, loss = 4.26750956
Iteration 61, loss = 4.25078416
Iteration 62, loss = 4.23351012
Iteration 63, loss = 4.21490740
Iteration 64, loss = 4.19901106
Iteration 65, loss = 4.18092357
Iteration 66, loss = 4.16528160
Iteration 67, loss = 4.14885517
Iteration 68, loss = 4.13284427
Iteration 69, loss = 4.11516836
Iteration 70, loss = 4.09876199
Iteration 71, loss = 4.08382860
Iteration 72, loss = 4.06744454
Iteration 73, loss = 4.05085609
Iteration 74, loss = 4.03598063
Iteration 75, loss = 4.02002339
Iteration 76, loss = 4.00516006
Iteration 77, loss = 3.99024546
Iteration 78, loss = 3.97552583
Iteration 79, loss = 3.95998867
Iteration 80, loss = 3.94381062
Iteration 81, loss = 3.93058271
Iteration 82, loss = 3.91544239
Iteration 83, loss = 3.90114580
Iteration 84, loss = 3.88629922
Iteration 85, loss = 3.87237407
Iteration 86, loss = 3.85898486
Iteration 87, loss = 3.84522325
Iteration 88, loss = 3.83178048
Iteration 89, loss = 3.81740831
Iteration 90, loss = 3.80457079
Iteration 91, loss = 3.79060001
Iteration 92, loss = 3.77756997
Iteration 93, loss = 3.76530998
Iteration 94, loss = 3.75123786
Iteration 95, loss = 3.73701264
Iteration 96, loss = 3.72481086
Iteration 97, loss = 3.71224445
Iteration 98, loss = 3.69960769
Iteration 99, loss = 3.68610153
Iteration 100, loss = 3.67415974
Iteration 101, loss = 3.66076567
Iteration 102, loss = 3.64774927
Iteration 103, loss = 3.63679422
Iteration 104, loss = 3.62522983
Iteration 105, loss = 3.61215576
Iteration 106, loss = 3.60088196
Iteration 107, loss = 3.58916057
Iteration 108, loss = 3.57666472
Iteration 109, loss = 3.56395703
Iteration 110, loss = 3.55456974
Iteration 111, loss = 3.54243687
Iteration 112, loss = 3.52995735
Iteration 113, loss = 3.51975246
Iteration 114, loss = 3.50789077
Iteration 115, loss = 3.49638640
Iteration 116, loss = 3.48525549
Iteration 117, loss = 3.47481536
Iteration 118, loss = 3.46310312
Iteration 119, loss = 3.45283286
Iteration 120, loss = 3.44304832
Iteration 121, loss = 3.43130688
Iteration 122, loss = 3.42071535
Iteration 123, loss = 3.41007186
Iteration 124, loss = 3.40054141
Iteration 125, loss = 3.39043398
Iteration 126, loss = 3.37742742
Iteration 127, loss = 3.36884009
Iteration 128, loss = 3.35792422
Iteration 129, loss = 3.34826244
Iteration 130, loss = 3.33833060
Iteration 131, loss = 3.32868821
Iteration 132, loss = 3.31771539
Iteration 133, loss = 3.30796384
Iteration 134, loss = 3.29931900
Iteration 135, loss = 3.28850984
Iteration 136, loss = 3.27862711
Iteration 137, loss = 3.26943506
Iteration 138, loss = 3.25902938
Iteration 139, loss = 3.25043601
Iteration 140, loss = 3.24079568
Iteration 141, loss = 3.23135487
Iteration 142, loss = 3.22259272
Iteration 143, loss = 3.21286128
Iteration 144, loss = 3.20473690
Iteration 145, loss = 3.19458371
Iteration 146, loss = 3.18572373
Iteration 147, loss = 3.17655795
Iteration 148, loss = 3.16806373
Iteration 149, loss = 3.15849354
Iteration 150, loss = 3.15029048
Iteration 151, loss = 3.14147796
Iteration 152, loss = 3.13286344
Iteration 153, loss = 3.12297906
Iteration 154, loss = 3.11527192
Iteration 155, loss = 3.10554011
Iteration 156, loss = 3.09666343
Iteration 157, loss = 3.08873338
Iteration 158, loss = 3.08056823
Iteration 159, loss = 3.07199967
Iteration 160, loss = 3.06350935
Iteration 161, loss = 3.05511112
Iteration 162, loss = 3.04796348
Iteration 163, loss = 3.03949956
Iteration 164, loss = 3.03078671
Iteration 165, loss = 3.02335332
Iteration 166, loss = 3.01502307
Iteration 167, loss = 3.00640637
Iteration 168, loss = 2.99956346
Iteration 169, loss = 2.99136929
Iteration 170, loss = 2.98403324
Iteration 171, loss = 2.97557007
Iteration 172, loss = 2.96794049
Iteration 173, loss = 2.95972813
Iteration 174, loss = 2.95109810
Iteration 175, loss = 2.94388727
Iteration 176, loss = 2.93682734
Iteration 177, loss = 2.92882801
Iteration 178, loss = 2.92128013
Iteration 179, loss = 2.91501624
Iteration 180, loss = 2.90681774
Iteration 181, loss = 2.89986361
Iteration 182, loss = 2.89239503
Iteration 183, loss = 2.88494626
Iteration 184, loss = 2.87691961
Iteration 185, loss = 2.87062158
Iteration 186, loss = 2.86268993
Iteration 187, loss = 2.85461188
Iteration 188, loss = 2.84712067
Iteration 189, loss = 2.84051793
Iteration 190, loss = 2.83391001
Iteration 191, loss = 2.82639327
Iteration 192, loss = 2.81905205
Iteration 193, loss = 2.81276749
Iteration 194, loss = 2.80592165
Iteration 195, loss = 2.79815694
Iteration 196, loss = 2.79288148
Iteration 197, loss = 2.78679422
Iteration 198, loss = 2.77775677
Iteration 199, loss = 2.77149832
Iteration 200, loss = 2.76524582
Iteration 201, loss = 2.75877069
Iteration 202, loss = 2.75151443
Iteration 203, loss = 2.74510021
Iteration 204, loss = 2.73771384
Iteration 205, loss = 2.73256606
Iteration 206, loss = 2.72323145
Iteration 207, loss = 2.71857133
Iteration 208, loss = 2.71243341
Iteration 209, loss = 2.70514927
Iteration 210, loss = 2.69919197
Iteration 211, loss = 2.69236032
Iteration 212, loss = 2.68651472
Iteration 213, loss = 2.68091411
Iteration 214, loss = 2.67347449
Iteration 215, loss = 2.66760189
Iteration 216, loss = 2.66146146
Iteration 217, loss = 2.65543367
Iteration 218, loss = 2.64833896
Iteration 219, loss = 2.64238888
Iteration 220, loss = 2.63630819
Iteration 221, loss = 2.63035124
Iteration 222, loss = 2.62482576
Iteration 223, loss = 2.61876194
Iteration 224, loss = 2.61203995
Iteration 225, loss = 2.60574768
Iteration 226, loss = 2.60064978
Iteration 227, loss = 2.59429903
Iteration 228, loss = 2.58919692
Iteration 229, loss = 2.58291588
Iteration 230, loss = 2.57635151
Iteration 231, loss = 2.57075545
Iteration 232, loss = 2.56618340
Iteration 233, loss = 2.55890746
Iteration 234, loss = 2.55386720
Iteration 235, loss = 2.54723022
Iteration 236, loss = 2.54384402
Iteration 237, loss = 2.53611850
Iteration 238, loss = 2.53178032
Iteration 239, loss = 2.52492821
Iteration 240, loss = 2.51902789
Iteration 241, loss = 2.51386481
Iteration 242, loss = 2.51004385
Iteration 243, loss = 2.50347582
Iteration 244, loss = 2.49862116
Iteration 245, loss = 2.49205459
Iteration 246, loss = 2.48599815
Iteration 247, loss = 2.48030944
Iteration 248, loss = 2.47548184
Iteration 249, loss = 2.47158035
Iteration 250, loss = 2.46529586
Iteration 251, loss = 2.46039079
Iteration 252, loss = 2.45461396
Iteration 253, loss = 2.45037632
Iteration 254, loss = 2.44468207
Iteration 255, loss = 2.43868743
Iteration 256, loss = 2.43336552
Iteration 257, loss = 2.42925081
Iteration 258, loss = 2.42239425
Iteration 259, loss = 2.41960158
Iteration 260, loss = 2.41319540
Iteration 261, loss = 2.40773329
Iteration 262, loss = 2.40237538
Iteration 263, loss = 2.39694548
Iteration 264, loss = 2.39083535
Iteration 265, loss = 2.38771703
Iteration 266, loss = 2.38303698
Iteration 267, loss = 2.37762598
Iteration 268, loss = 2.37386602
Iteration 269, loss = 2.36774080
Iteration 270, loss = 2.36253674
Iteration 271, loss = 2.35894435
Iteration 272, loss = 2.35315140
Iteration 273, loss = 2.34870458
Iteration 274, loss = 2.34353306
Iteration 275, loss = 2.33889621
Iteration 276, loss = 2.33451152
Iteration 277, loss = 2.32968882
Iteration 278, loss = 2.32471380
Iteration 279, loss = 2.31993116
Iteration 280, loss = 2.31564936
Iteration 281, loss = 2.31060293
Iteration 282, loss = 2.30633320
Iteration 283, loss = 2.30259806
Iteration 284, loss = 2.29660338
Iteration 285, loss = 2.29208781
Iteration 286, loss = 2.28856675
Iteration 287, loss = 2.28314317
Iteration 288, loss = 2.27743822
Iteration 289, loss = 2.27281773
Iteration 290, loss = 2.26890057
Iteration 291, loss = 2.26463261
Iteration 292, loss = 2.26113671
Iteration 293, loss = 2.25622892
Iteration 294, loss = 2.25156528
Iteration 295, loss = 2.24604382
Iteration 296, loss = 2.24170174
Iteration 297, loss = 2.23767111
Iteration 298, loss = 2.23216017
Iteration 299, loss = 2.22930309
Iteration 300, loss = 2.22487449
Iteration 301, loss = 2.22073628
Iteration 302, loss = 2.21680479
Iteration 303, loss = 2.21147239
Iteration 304, loss = 2.20799788
Iteration 305, loss = 2.20257311
Iteration 306, loss = 2.19963673
Iteration 307, loss = 2.19480031
Iteration 308, loss = 2.18956225
Iteration 309, loss = 2.18630973
Iteration 310, loss = 2.18198177
Iteration 311, loss = 2.17804812
Iteration 312, loss = 2.17449287
Iteration 313, loss = 2.16987945
Iteration 314, loss = 2.16534092
Iteration 315, loss = 2.16120416
Iteration 316, loss = 2.15700689
Iteration 317, loss = 2.15222404
Iteration 318, loss = 2.14970514
Iteration 319, loss = 2.14523874
Iteration 320, loss = 2.14023308
Iteration 321, loss = 2.13781357
Iteration 322, loss = 2.13274244
Iteration 323, loss = 2.12912620
Iteration 324, loss = 2.12557493
Iteration 325, loss = 2.11976429
Iteration 326, loss = 2.11699366
Iteration 327, loss = 2.11213800
Iteration 328, loss = 2.10997648
Iteration 329, loss = 2.10576923
Iteration 330, loss = 2.10093955
Iteration 331, loss = 2.09793363
Iteration 332, loss = 2.09350994
Iteration 333, loss = 2.08966283
Iteration 334, loss = 2.08642307
Iteration 335, loss = 2.08229355
Iteration 336, loss = 2.07891336
Iteration 337, loss = 2.07364282
Iteration 338, loss = 2.07023841
Iteration 339, loss = 2.06687251
Iteration 340, loss = 2.06315530
Iteration 341, loss = 2.05994702
Iteration 342, loss = 2.05501139
Iteration 343, loss = 2.05141630
Iteration 344, loss = 2.04842620
Iteration 345, loss = 2.04462064
Iteration 346, loss = 2.03973280
Iteration 347, loss = 2.03698239
Iteration 348, loss = 2.03404259
Iteration 349, loss = 2.03001270
Iteration 350, loss = 2.02428883
Iteration 351, loss = 2.02107493
Iteration 352, loss = 2.01855421
Iteration 353, loss = 2.01560777
Iteration 354, loss = 2.01062911
Iteration 355, loss = 2.00779267
Iteration 356, loss = 2.00309052
Iteration 357, loss = 2.00116023
Iteration 358, loss = 1.99772634
Iteration 359, loss = 1.99315839
Iteration 360, loss = 1.98952842
Iteration 361, loss = 1.98549095
Iteration 362, loss = 1.98320423
Iteration 363, loss = 1.97978280
Iteration 364, loss = 1.97491529
Iteration 365, loss = 1.97292435
Iteration 366, loss = 1.96713914
Iteration 367, loss = 1.96550471
Iteration 368, loss = 1.96131318
Iteration 369, loss = 1.95804830
Iteration 370, loss = 1.95499757
Iteration 371, loss = 1.95184308
Iteration 372, loss = 1.94950521
Iteration 373, loss = 1.94575426
Iteration 374, loss = 1.94243202
Iteration 375, loss = 1.93802423
Iteration 376, loss = 1.93424320
Iteration 377, loss = 1.93186569
Iteration 378, loss = 1.92743209
Iteration 379, loss = 1.92486029
Iteration 380, loss = 1.92143649
Iteration 381, loss = 1.91827502
Iteration 382, loss = 1.91473008
Iteration 383, loss = 1.91108561
Iteration 384, loss = 1.90843172
Iteration 385, loss = 1.90585715
Iteration 386, loss = 1.90184523
Iteration 387, loss = 1.89817813
Iteration 388, loss = 1.89591634
Iteration 389, loss = 1.89233702
Iteration 390, loss = 1.88993153
Iteration 391, loss = 1.88636045
Iteration 392, loss = 1.88349262
Iteration 393, loss = 1.87935465
Iteration 394, loss = 1.87607544
Iteration 395, loss = 1.87460605
Iteration 396, loss = 1.86967231
Iteration 397, loss = 1.86690071
Iteration 398, loss = 1.86358553
Iteration 399, loss = 1.86055795
Iteration 400, loss = 1.85885188
Iteration 401, loss = 1.85403108
Iteration 402, loss = 1.85179482
Iteration 403, loss = 1.84766282
Iteration 404, loss = 1.84513197
Iteration 405, loss = 1.84238545
Iteration 406, loss = 1.83759824
Iteration 407, loss = 1.83516466
Iteration 408, loss = 1.83193202
Iteration 409, loss = 1.82997880
Iteration 410, loss = 1.82655257
Iteration 411, loss = 1.82399492
Iteration 412, loss = 1.82077153
Iteration 413, loss = 1.81680804
Iteration 414, loss = 1.81387672
Iteration 415, loss = 1.81071123
Iteration 416, loss = 1.80956402
Iteration 417, loss = 1.80648014
Iteration 418, loss = 1.80214865
Iteration 419, loss = 1.79988304
Iteration 420, loss = 1.79679743
Iteration 421, loss = 1.79359605
Iteration 422, loss = 1.79118453
Iteration 423, loss = 1.78821823
Iteration 424, loss = 1.78547648
Iteration 425, loss = 1.78208999
Iteration 426, loss = 1.77933814
Iteration 427, loss = 1.77622930
Iteration 428, loss = 1.77313817
Iteration 429, loss = 1.77028942
Iteration 430, loss = 1.76826040
Iteration 431, loss = 1.76470587
Iteration 432, loss = 1.76232524
Iteration 433, loss = 1.75922488
Iteration 434, loss = 1.75695596
Iteration 435, loss = 1.75368171
Iteration 436, loss = 1.75230527
Iteration 437, loss = 1.74741093
Iteration 438, loss = 1.74550320
Iteration 439, loss = 1.74276883
Iteration 440, loss = 1.73931939
Iteration 441, loss = 1.73765523
Iteration 442, loss = 1.73385894
Iteration 443, loss = 1.73105474
Iteration 444, loss = 1.72959483
Iteration 445, loss = 1.72596483
Iteration 446, loss = 1.72263015
Iteration 447, loss = 1.72057711
Iteration 448, loss = 1.71874654
Iteration 449, loss = 1.71475415
Iteration 450, loss = 1.71294490
Iteration 451, loss = 1.70968174
Iteration 452, loss = 1.70711099
Iteration 453, loss = 1.70392784
Iteration 454, loss = 1.70198181
Iteration 455, loss = 1.69893019
Iteration 456, loss = 1.69680319
Iteration 457, loss = 1.69529627
Iteration 458, loss = 1.69242992
Iteration 459, loss = 1.68800468
Iteration 460, loss = 1.68576352
Iteration 461, loss = 1.68292012
Iteration 462, loss = 1.68076361
Iteration 463, loss = 1.67677994
Iteration 464, loss = 1.67476320
Iteration 465, loss = 1.67375796
Iteration 466, loss = 1.67099028
Iteration 467, loss = 1.66979898
Iteration 468, loss = 1.66516346
Iteration 469, loss = 1.66278713
Iteration 470, loss = 1.66037714
Iteration 471, loss = 1.65859074
Iteration 472, loss = 1.65536256
Iteration 473, loss = 1.65228561
Iteration 474, loss = 1.64992511
Iteration 475, loss = 1.64773853
Iteration 476, loss = 1.64413996
Iteration 477, loss = 1.64196773
Iteration 478, loss = 1.64087982
Iteration 479, loss = 1.63756890
Iteration 480, loss = 1.63599424
Iteration 481, loss = 1.63210094
Iteration 482, loss = 1.62963212
Iteration 483, loss = 1.62779679
Iteration 484, loss = 1.62568240
Iteration 485, loss = 1.62345114
Iteration 486, loss = 1.61969527
Iteration 487, loss = 1.61792943
Iteration 488, loss = 1.61616981
Iteration 489, loss = 1.61232692
Iteration 490, loss = 1.61008416
Iteration 491, loss = 1.60858910
Iteration 492, loss = 1.60593581
Iteration 493, loss = 1.60330317
Iteration 494, loss = 1.60058971
Iteration 495, loss = 1.59762447
Iteration 496, loss = 1.59582779
Iteration 497, loss = 1.59333828
Iteration 498, loss = 1.59107925
Iteration 499, loss = 1.58965336
Iteration 500, loss = 1.58814423
Iteration 501, loss = 1.58425232
Iteration 502, loss = 1.58236632
Iteration 503, loss = 1.57995552
Iteration 504, loss = 1.57647062
Iteration 505, loss = 1.57547144
Iteration 506, loss = 1.57104695
Iteration 507, loss = 1.57033012
Iteration 508, loss = 1.56718132
Iteration 509, loss = 1.56584809
Iteration 510, loss = 1.56340277
Iteration 511, loss = 1.56104601
Iteration 512, loss = 1.55867615
Iteration 513, loss = 1.55705699
Iteration 514, loss = 1.55544179
Iteration 515, loss = 1.55128096
Iteration 516, loss = 1.54867466
Iteration 517, loss = 1.54699417
Iteration 518, loss = 1.54533672
Iteration 519, loss = 1.54270246
Iteration 520, loss = 1.54052168
Iteration 521, loss = 1.53873461
Iteration 522, loss = 1.53673836
Iteration 523, loss = 1.53301801
Iteration 524, loss = 1.53234738
Iteration 525, loss = 1.52860095
Iteration 526, loss = 1.52697002
Iteration 527, loss = 1.52439040
Iteration 528, loss = 1.52170705
Iteration 529, loss = 1.52127908
Iteration 530, loss = 1.51868344
Iteration 531, loss = 1.51585084
Iteration 532, loss = 1.51263800
Iteration 533, loss = 1.51042509
Iteration 534, loss = 1.50873400
Iteration 535, loss = 1.50764599
Iteration 536, loss = 1.50490533
Iteration 537, loss = 1.50402794
Iteration 538, loss = 1.50093239
Iteration 539, loss = 1.49777013
Iteration 540, loss = 1.49723381
Iteration 541, loss = 1.49505491
Iteration 542, loss = 1.49260052
Iteration 543, loss = 1.48992288
Iteration 544, loss = 1.48737961
Iteration 545, loss = 1.48574668
Iteration 546, loss = 1.48465163
Iteration 547, loss = 1.48214489
Iteration 548, loss = 1.47973143
Iteration 549, loss = 1.47832651
Iteration 550, loss = 1.47542976
Iteration 551, loss = 1.47299299
Iteration 552, loss = 1.47152056
Iteration 553, loss = 1.46940682
Iteration 554, loss = 1.46629027
Iteration 555, loss = 1.46452060
Iteration 556, loss = 1.46327810
Iteration 557, loss = 1.46160385
Iteration 558, loss = 1.45911555
Iteration 559, loss = 1.45659961
Iteration 560, loss = 1.45457869
Iteration 561, loss = 1.45323048
Iteration 562, loss = 1.44989759
Iteration 563, loss = 1.44808723
Iteration 564, loss = 1.44619318
Iteration 565, loss = 1.44433366
Iteration 566, loss = 1.44255216
Iteration 567, loss = 1.44116490
Iteration 568, loss = 1.43731576
Iteration 569, loss = 1.43700586
Iteration 570, loss = 1.43448994
Iteration 571, loss = 1.43185024
Iteration 572, loss = 1.43038728
Iteration 573, loss = 1.42759225
Iteration 574, loss = 1.42731851
Iteration 575, loss = 1.42365975
Iteration 576, loss = 1.42102946
Iteration 577, loss = 1.42014869
Iteration 578, loss = 1.41867016
Iteration 579, loss = 1.41557580
Iteration 580, loss = 1.41447192
Iteration 581, loss = 1.41263096
Iteration 582, loss = 1.41019427
Iteration 583, loss = 1.40885861
Iteration 584, loss = 1.40622022
Iteration 585, loss = 1.40463181
Iteration 586, loss = 1.40152823
Iteration 587, loss = 1.39957607
Iteration 588, loss = 1.39836108
Iteration 589, loss = 1.39681759
Iteration 590, loss = 1.39493478
Iteration 591, loss = 1.39412776
Iteration 592, loss = 1.39157559
Iteration 593, loss = 1.38767809
Iteration 594, loss = 1.38638373
Iteration 595, loss = 1.38636404
Iteration 596, loss = 1.38495183
Iteration 597, loss = 1.38088983
Iteration 598, loss = 1.37968780
Iteration 599, loss = 1.37809163
Iteration 600, loss = 1.37617465
Iteration 601, loss = 1.37304297
Iteration 602, loss = 1.37154565
Iteration 603, loss = 1.37019399
Iteration 604, loss = 1.36897979
Iteration 605, loss = 1.36557665
Iteration 606, loss = 1.36458006
Iteration 607, loss = 1.36303090
Iteration 608, loss = 1.36161603
Iteration 609, loss = 1.35836908
Iteration 610, loss = 1.35814503
Iteration 611, loss = 1.35443106
Iteration 612, loss = 1.35318403
Iteration 613, loss = 1.35131972
Iteration 614, loss = 1.35122845
Iteration 615, loss = 1.34724105
Iteration 616, loss = 1.34710302
Iteration 617, loss = 1.34412676
Iteration 618, loss = 1.34194280
Iteration 619, loss = 1.34063810
Iteration 620, loss = 1.33848453
Iteration 621, loss = 1.33632113
Iteration 622, loss = 1.33559207
Iteration 623, loss = 1.33312471
Iteration 624, loss = 1.33232338
Iteration 625, loss = 1.33040510
Iteration 626, loss = 1.32869433
Iteration 627, loss = 1.32606395
Iteration 628, loss = 1.32349207
Iteration 629, loss = 1.32281779
Iteration 630, loss = 1.32125609
Iteration 631, loss = 1.31885989
Iteration 632, loss = 1.31734782
Iteration 633, loss = 1.31686384
Iteration 634, loss = 1.31396351
Iteration 635, loss = 1.31230967
Iteration 636, loss = 1.31018127
Iteration 637, loss = 1.30706751
Iteration 638, loss = 1.30610394
Iteration 639, loss = 1.30482175
Iteration 640, loss = 1.30368965
Iteration 641, loss = 1.30150778
Iteration 642, loss = 1.29957309
Iteration 643, loss = 1.29778329
Iteration 644, loss = 1.29594691
Iteration 645, loss = 1.29518361
Iteration 646, loss = 1.29181595
Iteration 647, loss = 1.29146116
Iteration 648, loss = 1.28938425
Iteration 649, loss = 1.28813651
Iteration 650, loss = 1.28656684
Iteration 651, loss = 1.28466701
Iteration 652, loss = 1.28265568
Iteration 653, loss = 1.28120513
Iteration 654, loss = 1.27892076
Iteration 655, loss = 1.27682124
Iteration 656, loss = 1.27550569
Iteration 657, loss = 1.27336978
Iteration 658, loss = 1.27325277
Iteration 659, loss = 1.27059855
Iteration 660, loss = 1.26761555
Iteration 661, loss = 1.26738147
Iteration 662, loss = 1.26465246
Iteration 663, loss = 1.26451187
Iteration 664, loss = 1.26290427
Iteration 665, loss = 1.26011350
Iteration 666, loss = 1.25835007
Iteration 667, loss = 1.25605970
Iteration 668, loss = 1.25487646
Iteration 669, loss = 1.25289588
Iteration 670, loss = 1.25283507
Iteration 671, loss = 1.25119906
Iteration 672, loss = 1.24886584
Iteration 673, loss = 1.24760931
Iteration 674, loss = 1.24513217
Iteration 675, loss = 1.24353296
Iteration 676, loss = 1.24301690
Iteration 677, loss = 1.24057193
Iteration 678, loss = 1.23903852
Iteration 679, loss = 1.23758505
Iteration 680, loss = 1.23657496
Iteration 681, loss = 1.23382438
Iteration 682, loss = 1.23261944
Iteration 683, loss = 1.23241580
Iteration 684, loss = 1.22896526
Iteration 685, loss = 1.22849984
Iteration 686, loss = 1.22620216
Iteration 687, loss = 1.22470432
Iteration 688, loss = 1.22328023
Iteration 689, loss = 1.22084067
Iteration 690, loss = 1.21911510
Iteration 691, loss = 1.21855269
Iteration 692, loss = 1.21652698
Iteration 693, loss = 1.21331482
Iteration 694, loss = 1.21294307
Iteration 695, loss = 1.21122601
Iteration 696, loss = 1.21033247
Iteration 697, loss = 1.21148534
Iteration 698, loss = 1.20726853
Iteration 699, loss = 1.20584093
Iteration 700, loss = 1.20486025
Iteration 701, loss = 1.20239219
Iteration 702, loss = 1.20158045
Iteration 703, loss = 1.19960279
Iteration 704, loss = 1.19814559
Iteration 705, loss = 1.19636588
Iteration 706, loss = 1.19304204
Iteration 707, loss = 1.19333057
Iteration 708, loss = 1.19150131
Iteration 709, loss = 1.19142213
Iteration 710, loss = 1.18944147
Iteration 711, loss = 1.18808957
Iteration 712, loss = 1.18545839
Iteration 713, loss = 1.18313113
Iteration 714, loss = 1.18238244
Iteration 715, loss = 1.18164234
Iteration 716, loss = 1.17943018
Iteration 717, loss = 1.17756930
Iteration 718, loss = 1.17608956
Iteration 719, loss = 1.17454440
Iteration 720, loss = 1.17363653
Iteration 721, loss = 1.17164068
Iteration 722, loss = 1.17043617
Iteration 723, loss = 1.16887011
Iteration 724, loss = 1.16801944
Iteration 725, loss = 1.16539603
Iteration 726, loss = 1.16457694
Iteration 727, loss = 1.16316521
Iteration 728, loss = 1.16156764
Iteration 729, loss = 1.15937745
Iteration 730, loss = 1.15887871
Iteration 731, loss = 1.15569939
Iteration 732, loss = 1.15604396
Iteration 733, loss = 1.15298529
Iteration 734, loss = 1.15228999
Iteration 735, loss = 1.15127586
Iteration 736, loss = 1.14896661
Iteration 737, loss = 1.14847720
Iteration 738, loss = 1.14705036
Iteration 739, loss = 1.14471691
Iteration 740, loss = 1.14417024
Iteration 741, loss = 1.14155434
Iteration 742, loss = 1.14176164
Iteration 743, loss = 1.14069870
Iteration 744, loss = 1.13805880
Iteration 745, loss = 1.13629853
Iteration 746, loss = 1.13664495
Iteration 747, loss = 1.13353718
Iteration 748, loss = 1.13271870
Iteration 749, loss = 1.13161979
Iteration 750, loss = 1.12929392
Iteration 751, loss = 1.12699338
Iteration 752, loss = 1.12594152
Iteration 753, loss = 1.12551112
Iteration 754, loss = 1.12338993
Iteration 755, loss = 1.12159267
Iteration 756, loss = 1.12065568
Iteration 757, loss = 1.11971237
Iteration 758, loss = 1.11891662
Iteration 759, loss = 1.11633445
Iteration 760, loss = 1.11674387
Iteration 761, loss = 1.11474992
Iteration 762, loss = 1.11231701
Iteration 763, loss = 1.11057281
Iteration 764, loss = 1.11028903
Iteration 765, loss = 1.10884140
Iteration 766, loss = 1.10820324
Iteration 767, loss = 1.10630129
Iteration 768, loss = 1.10417669
Iteration 769, loss = 1.10255653
Iteration 770, loss = 1.10056167
Iteration 771, loss = 1.09971145
Iteration 772, loss = 1.09865497
Iteration 773, loss = 1.09711442
Iteration 774, loss = 1.09579626
Iteration 775, loss = 1.09416695
Iteration 776, loss = 1.09346197
Iteration 777, loss = 1.09291073
Iteration 778, loss = 1.09091304
Iteration 779, loss = 1.08823500
Iteration 780, loss = 1.08833844
Iteration 781, loss = 1.08590085
Iteration 782, loss = 1.08440122
Iteration 783, loss = 1.08355228
Iteration 784, loss = 1.08300469
Iteration 785, loss = 1.08097220
Iteration 786, loss = 1.08000263
Iteration 787, loss = 1.07842796
Iteration 788, loss = 1.07686288
Iteration 789, loss = 1.07628797
Iteration 790, loss = 1.07401900
Iteration 791, loss = 1.07414280
Iteration 792, loss = 1.07227875
Iteration 793, loss = 1.06970887
Iteration 794, loss = 1.06878525
Iteration 795, loss = 1.06808678
Iteration 796, loss = 1.06683883
Iteration 797, loss = 1.06534330
Iteration 798, loss = 1.06432266
Iteration 799, loss = 1.06252246
Iteration 800, loss = 1.06044733
Iteration 801, loss = 1.05938925
Iteration 802, loss = 1.05777280
Iteration 803, loss = 1.05808394
Iteration 804, loss = 1.05635509
Iteration 805, loss = 1.05485767
Iteration 806, loss = 1.05239144
Iteration 807, loss = 1.05214718
Iteration 808, loss = 1.05124744
Iteration 809, loss = 1.04922508
Iteration 810, loss = 1.04675136
Iteration 811, loss = 1.04634899
Iteration 812, loss = 1.04613011
Iteration 813, loss = 1.04302255
Iteration 814, loss = 1.04262637
Iteration 815, loss = 1.04166727
Iteration 816, loss = 1.04033217
Iteration 817, loss = 1.04053503
Iteration 818, loss = 1.03819577
Iteration 819, loss = 1.03554467
Iteration 820, loss = 1.03618823
Iteration 821, loss = 1.03433692
Iteration 822, loss = 1.03284264
Iteration 823, loss = 1.03204786
Iteration 824, loss = 1.03020242
Iteration 825, loss = 1.02928347
Iteration 826, loss = 1.02796484
Iteration 827, loss = 1.02708005
Iteration 828, loss = 1.02471248
Iteration 829, loss = 1.02503593
Iteration 830, loss = 1.02279593
Iteration 831, loss = 1.02203068
Iteration 832, loss = 1.02013876
Iteration 833, loss = 1.01911452
Iteration 834, loss = 1.01775953
Iteration 835, loss = 1.01523737
Iteration 836, loss = 1.01448320
Iteration 837, loss = 1.01387142
Iteration 838, loss = 1.01172432
Iteration 839, loss = 1.01182233
Iteration 840, loss = 1.01020185
Iteration 841, loss = 1.00988135
Iteration 842, loss = 1.00798090
Iteration 843, loss = 1.00715075
Iteration 844, loss = 1.00618745
Iteration 845, loss = 1.00470712
Iteration 846, loss = 1.00372272
Iteration 847, loss = 1.00145749
Iteration 848, loss = 1.00044474
Iteration 849, loss = 0.99982269
Iteration 850, loss = 0.99947007
Iteration 851, loss = 0.99737034
Iteration 852, loss = 0.99637767
Iteration 853, loss = 0.99388906
Iteration 854, loss = 0.99365791
Iteration 855, loss = 0.99193530
Iteration 856, loss = 0.99120060
Iteration 857, loss = 0.98981975
Iteration 858, loss = 0.98828874
Iteration 859, loss = 0.98891663
Iteration 860, loss = 0.98622733
Iteration 861, loss = 0.98478592
Iteration 862, loss = 0.98435028
Iteration 863, loss = 0.98251423
Iteration 864, loss = 0.98182253
Iteration 865, loss = 0.98084100
Iteration 866, loss = 0.97863522
Iteration 867, loss = 0.97889315
Iteration 868, loss = 0.97724753
Iteration 869, loss = 0.97547588
Iteration 870, loss = 0.97377910
Iteration 871, loss = 0.97276218
Iteration 872, loss = 0.97225555
Iteration 873, loss = 0.97125938
Iteration 874, loss = 0.96955949
Iteration 875, loss = 0.96916086
Iteration 876, loss = 0.96753191
Iteration 877, loss = 0.96758914
Iteration 878, loss = 0.96626111
Iteration 879, loss = 0.96395762
Iteration 880, loss = 0.96206480
Iteration 881, loss = 0.96369601
Iteration 882, loss = 0.95993646
Iteration 883, loss = 0.95968900
Iteration 884, loss = 0.95792196
Iteration 885, loss = 0.95684736
Iteration 886, loss = 0.95565413
Iteration 887, loss = 0.95429283
Iteration 888, loss = 0.95371046
Iteration 889, loss = 0.95255868
Iteration 890, loss = 0.95201216
Iteration 891, loss = 0.95093768
Iteration 892, loss = 0.94904181
Iteration 893, loss = 0.94811658
Iteration 894, loss = 0.94596010
Iteration 895, loss = 0.94661220
Iteration 896, loss = 0.94448228
Iteration 897, loss = 0.94418829
Iteration 898, loss = 0.94234068
Iteration 899, loss = 0.94234886
Iteration 900, loss = 0.94096990
Iteration 901, loss = 0.94001318
Iteration 902, loss = 0.93819892
Iteration 903, loss = 0.93810530
Iteration 904, loss = 0.93633024
Iteration 905, loss = 0.93551123
Iteration 906, loss = 0.93348328
Iteration 907, loss = 0.93309308
Iteration 908, loss = 0.93026278
Iteration 909, loss = 0.93088531
Iteration 910, loss = 0.92897520
Iteration 911, loss = 0.92838588
Iteration 912, loss = 0.92706889
Iteration 913, loss = 0.92584856
Iteration 914, loss = 0.92559902
Iteration 915, loss = 0.92442688
Iteration 916, loss = 0.92442344
Iteration 917, loss = 0.92261399
Iteration 918, loss = 0.92108576
Iteration 919, loss = 0.91893975
Iteration 920, loss = 0.91854123
Iteration 921, loss = 0.91816278
Iteration 922, loss = 0.91705004
Iteration 923, loss = 0.91554744
Iteration 924, loss = 0.91480399
Iteration 925, loss = 0.91260136
Iteration 926, loss = 0.91169262
Iteration 927, loss = 0.91012616
Iteration 928, loss = 0.90945707
Iteration 929, loss = 0.90870896
Iteration 930, loss = 0.90805427
Iteration 931, loss = 0.90613516
Iteration 932, loss = 0.90658163
Iteration 933, loss = 0.90571062
Iteration 934, loss = 0.90191311
Iteration 935, loss = 0.90275715
Iteration 936, loss = 0.90113216
Iteration 937, loss = 0.90065351
Iteration 938, loss = 0.90017319
Iteration 939, loss = 0.89812378
Iteration 940, loss = 0.89641966
Iteration 941, loss = 0.89630054
Iteration 942, loss = 0.89555693
Iteration 943, loss = 0.89420616
Iteration 944, loss = 0.89416437
Iteration 945, loss = 0.89208997
Iteration 946, loss = 0.89141577
Iteration 947, loss = 0.89100388
Iteration 948, loss = 0.89003772
Iteration 949, loss = 0.88845242
Iteration 950, loss = 0.88836131
Iteration 951, loss = 0.88623683
Iteration 952, loss = 0.88499871
Iteration 953, loss = 0.88416582
Iteration 954, loss = 0.88253431
Iteration 955, loss = 0.88113398
Iteration 956, loss = 0.88169555
Iteration 957, loss = 0.88039761
Iteration 958, loss = 0.87861100
Iteration 959, loss = 0.87751959
Iteration 960, loss = 0.87740970
Iteration 961, loss = 0.87573092
Iteration 962, loss = 0.87525596
Iteration 963, loss = 0.87424953
Iteration 964, loss = 0.87263135
Iteration 965, loss = 0.87214846
Iteration 966, loss = 0.87071365
Iteration 967, loss = 0.87090312
Iteration 968, loss = 0.86978572
Iteration 969, loss = 0.86760979
Iteration 970, loss = 0.86650243
Iteration 971, loss = 0.86644585
Iteration 972, loss = 0.86575859
Iteration 973, loss = 0.86472997
Iteration 974, loss = 0.86311195
Iteration 975, loss = 0.86132259
Iteration 976, loss = 0.86178460
Iteration 977, loss = 0.86000027
Iteration 978, loss = 0.85918421
Iteration 979, loss = 0.85805968
Iteration 980, loss = 0.85808168
Iteration 981, loss = 0.85596682
Iteration 982, loss = 0.85533208
Iteration 983, loss = 0.85469097
Iteration 984, loss = 0.85316126
Iteration 985, loss = 0.85255596
Iteration 986, loss = 0.85182531
Iteration 987, loss = 0.85041213
Iteration 988, loss = 0.84894606
Iteration 989, loss = 0.84787015
Iteration 990, loss = 0.84773126
Iteration 991, loss = 0.84623744
Iteration 992, loss = 0.84486371
Iteration 993, loss = 0.84412774
Iteration 994, loss = 0.84328659
Iteration 995, loss = 0.84108885
Iteration 996, loss = 0.84117566
Iteration 997, loss = 0.84150503
Iteration 998, loss = 0.84063634
Iteration 999, loss = 0.83818681
Iteration 1000, loss = 0.83788311
Iteration 1001, loss = 0.83708564
Iteration 1002, loss = 0.83550558
Iteration 1003, loss = 0.83437113
Iteration 1004, loss = 0.83383467
Iteration 1005, loss = 0.83279316
Iteration 1006, loss = 0.83317310
Iteration 1007, loss = 0.82997390
Iteration 1008, loss = 0.83024390
Iteration 1009, loss = 0.82966121
Iteration 1010, loss = 0.82824765
Iteration 1011, loss = 0.82685633
Iteration 1012, loss = 0.82659883
Iteration 1013, loss = 0.82570641
Iteration 1014, loss = 0.82511157
Iteration 1015, loss = 0.82399891
Iteration 1016, loss = 0.82262440
Iteration 1017, loss = 0.82243279
Iteration 1018, loss = 0.82176498
Iteration 1019, loss = 0.81941475
Iteration 1020, loss = 0.81880338
Iteration 1021, loss = 0.81824843
Iteration 1022, loss = 0.81873418
Iteration 1023, loss = 0.81587615
Iteration 1024, loss = 0.81551764
Iteration 1025, loss = 0.81403009
Iteration 1026, loss = 0.81343417
Iteration 1027, loss = 0.81292117
Iteration 1028, loss = 0.81209258
Iteration 1029, loss = 0.81084300
Iteration 1030, loss = 0.81025233
Iteration 1031, loss = 0.80945352
Iteration 1032, loss = 0.80763774
Iteration 1033, loss = 0.80750704
Iteration 1034, loss = 0.80731509
Iteration 1035, loss = 0.80618943
Iteration 1036, loss = 0.80429320
Iteration 1037, loss = 0.80454691
Iteration 1038, loss = 0.80389194
Iteration 1039, loss = 0.80150234
Iteration 1040, loss = 0.80094582
Iteration 1041, loss = 0.80061145
Iteration 1042, loss = 0.79927732
Iteration 1043, loss = 0.79781597
Iteration 1044, loss = 0.79740185
Iteration 1045, loss = 0.79657907
Iteration 1046, loss = 0.79552699
Iteration 1047, loss = 0.79366599
Iteration 1048, loss = 0.79508742
Iteration 1049, loss = 0.79330968
Iteration 1050, loss = 0.79204461
Iteration 1051, loss = 0.79197395
Iteration 1052, loss = 0.78999296
Iteration 1053, loss = 0.78873626
Iteration 1054, loss = 0.78842014
Iteration 1055, loss = 0.78758862
Iteration 1056, loss = 0.78728258
Iteration 1057, loss = 0.78624267
Iteration 1058, loss = 0.78427370
Iteration 1059, loss = 0.78340507
Iteration 1060, loss = 0.78335052
Iteration 1061, loss = 0.78263965
Iteration 1062, loss = 0.78154146
Iteration 1063, loss = 0.78181788
Iteration 1064, loss = 0.78039721
Iteration 1065, loss = 0.77843339
Iteration 1066, loss = 0.77802110
Iteration 1067, loss = 0.77801132
Iteration 1068, loss = 0.77716822
Iteration 1069, loss = 0.77563810
Iteration 1070, loss = 0.77460657
Iteration 1071, loss = 0.77379717
Iteration 1072, loss = 0.77380452
Iteration 1073, loss = 0.77300745
Iteration 1074, loss = 0.77163538
Iteration 1075, loss = 0.76969807
Iteration 1076, loss = 0.77044592
Iteration 1077, loss = 0.76845512
Iteration 1078, loss = 0.76730325
Iteration 1079, loss = 0.76648391
Iteration 1080, loss = 0.76581988
Iteration 1081, loss = 0.76611630
Iteration 1082, loss = 0.76444934
Iteration 1083, loss = 0.76312844
Iteration 1084, loss = 0.76274532
Iteration 1085, loss = 0.76155208
Iteration 1086, loss = 0.76107611
Iteration 1087, loss = 0.76033586
Iteration 1088, loss = 0.75998691
Iteration 1089, loss = 0.75878098
Iteration 1090, loss = 0.75762578
Iteration 1091, loss = 0.75671603
Iteration 1092, loss = 0.75656448
Iteration 1093, loss = 0.75543512
Iteration 1094, loss = 0.75498468
Iteration 1095, loss = 0.75371536
Iteration 1096, loss = 0.75282300
Iteration 1097, loss = 0.75263022
Iteration 1098, loss = 0.75159719
Iteration 1099, loss = 0.75058982
Iteration 1100, loss = 0.74969411
Iteration 1101, loss = 0.74940570
Iteration 1102, loss = 0.74747361
Iteration 1103, loss = 0.74808304
Iteration 1104, loss = 0.74827718
Iteration 1105, loss = 0.74622566
Iteration 1106, loss = 0.74409756
Iteration 1107, loss = 0.74290285
Iteration 1108, loss = 0.74381512
Iteration 1109, loss = 0.74238910
Iteration 1110, loss = 0.74198084
Iteration 1111, loss = 0.74103407
Iteration 1112, loss = 0.73933837
Iteration 1113, loss = 0.73961185
Iteration 1114, loss = 0.73781004
Iteration 1115, loss = 0.73706352
Iteration 1116, loss = 0.73534490
Iteration 1117, loss = 0.73616007
Iteration 1118, loss = 0.73524200
Iteration 1119, loss = 0.73497346
Iteration 1120, loss = 0.73329165
Iteration 1121, loss = 0.73241005
Iteration 1122, loss = 0.73198497
Iteration 1123, loss = 0.73052270
Iteration 1124, loss = 0.72954611
Iteration 1125, loss = 0.72900868
Iteration 1126, loss = 0.72886804
Iteration 1127, loss = 0.72752691
Iteration 1128, loss = 0.72591732
Iteration 1129, loss = 0.72575490
Iteration 1130, loss = 0.72649500
Iteration 1131, loss = 0.72366544
Iteration 1132, loss = 0.72425538
Iteration 1133, loss = 0.72233934
Iteration 1134, loss = 0.72213496
Iteration 1135, loss = 0.72177710
Iteration 1136, loss = 0.72007472
Iteration 1137, loss = 0.72006845
Iteration 1138, loss = 0.71808339
Iteration 1139, loss = 0.71879977
Iteration 1140, loss = 0.71710913
Iteration 1141, loss = 0.71732130
Iteration 1142, loss = 0.71674317
Iteration 1143, loss = 0.71522932
Iteration 1144, loss = 0.71648311
Iteration 1145, loss = 0.71372330
Iteration 1146, loss = 0.71401271
Iteration 1147, loss = 0.71227104
Iteration 1148, loss = 0.71083374
Iteration 1149, loss = 0.71038993
Iteration 1150, loss = 0.70965309
Iteration 1151, loss = 0.70864166
Iteration 1152, loss = 0.70857232
Iteration 1153, loss = 0.70711455
Iteration 1154, loss = 0.70670624
Iteration 1155, loss = 0.70542332
Iteration 1156, loss = 0.70581066
Iteration 1157, loss = 0.70492749
Iteration 1158, loss = 0.70433718
Iteration 1159, loss = 0.70392535
Iteration 1160, loss = 0.70249287
Iteration 1161, loss = 0.70125636
Iteration 1162, loss = 0.69996342
Iteration 1163, loss = 0.69988499
Iteration 1164, loss = 0.69878305
Iteration 1165, loss = 0.69827209
Iteration 1166, loss = 0.69661221
Iteration 1167, loss = 0.69690662
Iteration 1168, loss = 0.69601281
Iteration 1169, loss = 0.69637388
Iteration 1170, loss = 0.69474094
Iteration 1171, loss = 0.69373777
Iteration 1172, loss = 0.69308963
Iteration 1173, loss = 0.69302463
Iteration 1174, loss = 0.69153386
Iteration 1175, loss = 0.69168004
Iteration 1176, loss = 0.69023832
Iteration 1177, loss = 0.68928599
Iteration 1178, loss = 0.68853359
Iteration 1179, loss = 0.68822328
Iteration 1180, loss = 0.68772697
Iteration 1181, loss = 0.68691604
Iteration 1182, loss = 0.68640444
Iteration 1183, loss = 0.68471791
Iteration 1184, loss = 0.68460071
Iteration 1185, loss = 0.68444483
Iteration 1186, loss = 0.68183076
Iteration 1187, loss = 0.68377954
Iteration 1188, loss = 0.68234455
Iteration 1189, loss = 0.68048485
Iteration 1190, loss = 0.67901674
Iteration 1191, loss = 0.67928291
Iteration 1192, loss = 0.67814935
Iteration 1193, loss = 0.67731587
Iteration 1194, loss = 0.67682505
Iteration 1195, loss = 0.67733953
Iteration 1196, loss = 0.67535462
Iteration 1197, loss = 0.67435524
Iteration 1198, loss = 0.67360533
Iteration 1199, loss = 0.67248651
Iteration 1200, loss = 0.67263934
Iteration 1201, loss = 0.67199106
Iteration 1202, loss = 0.67235078
Iteration 1203, loss = 0.67141757
Iteration 1204, loss = 0.67033397
Iteration 1205, loss = 0.66963664
Iteration 1206, loss = 0.66906230
Iteration 1207, loss = 0.66890070
Iteration 1208, loss = 0.66729948
Iteration 1209, loss = 0.66621927
Iteration 1210, loss = 0.66595641
Iteration 1211, loss = 0.66466963
Iteration 1212, loss = 0.66372107
Iteration 1213, loss = 0.66332991
Iteration 1214, loss = 0.66314420
Iteration 1215, loss = 0.66207373
Iteration 1216, loss = 0.66179757
Iteration 1217, loss = 0.66056640
Iteration 1218, loss = 0.65993591
Iteration 1219, loss = 0.65902296
Iteration 1220, loss = 0.65889848
Iteration 1221, loss = 0.65881310
Iteration 1222, loss = 0.65808235
Iteration 1223, loss = 0.65663032
Iteration 1224, loss = 0.65555920
Iteration 1225, loss = 0.65490414
Iteration 1226, loss = 0.65430596
Iteration 1227, loss = 0.65272936
Iteration 1228, loss = 0.65381063
Iteration 1229, loss = 0.65276420
Iteration 1230, loss = 0.65185555
Iteration 1231, loss = 0.65093223
Iteration 1232, loss = 0.64977291
Iteration 1233, loss = 0.65052212
Iteration 1234, loss = 0.64882797
Iteration 1235, loss = 0.64923115
Iteration 1236, loss = 0.64695783
Iteration 1237, loss = 0.64647490
Iteration 1238, loss = 0.64661773
Iteration 1239, loss = 0.64632674
Iteration 1240, loss = 0.64589445
Iteration 1241, loss = 0.64449401
Iteration 1242, loss = 0.64385577
Iteration 1243, loss = 0.64273172
Iteration 1244, loss = 0.64338384
Iteration 1245, loss = 0.64180019
Iteration 1246, loss = 0.64091383
Iteration 1247, loss = 0.64003333
Iteration 1248, loss = 0.63942686
Iteration 1249, loss = 0.63897730
Iteration 1250, loss = 0.63761235
Iteration 1251, loss = 0.63698887
Iteration 1252, loss = 0.63643936
Iteration 1253, loss = 0.63668572
Iteration 1254, loss = 0.63534817
Iteration 1255, loss = 0.63534563
Iteration 1256, loss = 0.63361619
Iteration 1257, loss = 0.63412609
Iteration 1258, loss = 0.63379332
Iteration 1259, loss = 0.63310291
Iteration 1260, loss = 0.63183066
Iteration 1261, loss = 0.63247401
Iteration 1262, loss = 0.63022232
Iteration 1263, loss = 0.62910069
Iteration 1264, loss = 0.62880843
Iteration 1265, loss = 0.62917563
Iteration 1266, loss = 0.62753752
Iteration 1267, loss = 0.62753852
Iteration 1268, loss = 0.62709646
Iteration 1269, loss = 0.62636399
Iteration 1270, loss = 0.62577782
Iteration 1271, loss = 0.62476205
Iteration 1272, loss = 0.62383268
Iteration 1273, loss = 0.62281173
Iteration 1274, loss = 0.62193353
Iteration 1275, loss = 0.62169089
Iteration 1276, loss = 0.62154326
Iteration 1277, loss = 0.62087615
Iteration 1278, loss = 0.62030599
Iteration 1279, loss = 0.61932088
Iteration 1280, loss = 0.61885767
Iteration 1281, loss = 0.61885094
Iteration 1282, loss = 0.61798256
Iteration 1283, loss = 0.61713766
Iteration 1284, loss = 0.61559611
Iteration 1285, loss = 0.61573940
Iteration 1286, loss = 0.61524534
Iteration 1287, loss = 0.61480559
Iteration 1288, loss = 0.61382601
Iteration 1289, loss = 0.61337646
Iteration 1290, loss = 0.61234876
Iteration 1291, loss = 0.61077698
Iteration 1292, loss = 0.61084927
Iteration 1293, loss = 0.61044119
Iteration 1294, loss = 0.60993449
Iteration 1295, loss = 0.60830847
Iteration 1296, loss = 0.60890816
Iteration 1297, loss = 0.60809322
Iteration 1298, loss = 0.60742930
Iteration 1299, loss = 0.60774852
Iteration 1300, loss = 0.60590097
Iteration 1301, loss = 0.60574622
Iteration 1302, loss = 0.60543352
Iteration 1303, loss = 0.60476498
Iteration 1304, loss = 0.60385505
Iteration 1305, loss = 0.60221434
Iteration 1306, loss = 0.60223887
Iteration 1307, loss = 0.60179606
Iteration 1308, loss = 0.60046157
Iteration 1309, loss = 0.60114826
Iteration 1310, loss = 0.60020904
Iteration 1311, loss = 0.59962013
Iteration 1312, loss = 0.59942235
Iteration 1313, loss = 0.59809310
Iteration 1314, loss = 0.59840399
Iteration 1315, loss = 0.59763048
Iteration 1316, loss = 0.59626467
Iteration 1317, loss = 0.59509065
Iteration 1318, loss = 0.59428727
Iteration 1319, loss = 0.59403391
Iteration 1320, loss = 0.59435330
Iteration 1321, loss = 0.59237405
Iteration 1322, loss = 0.59203011
Iteration 1323, loss = 0.59260897
Iteration 1324, loss = 0.59116323
Iteration 1325, loss = 0.59107023
Iteration 1326, loss = 0.58992926
Iteration 1327, loss = 0.58932566
Iteration 1328, loss = 0.58781775
Iteration 1329, loss = 0.58869679
Iteration 1330, loss = 0.58700117
Iteration 1331, loss = 0.58733133
Iteration 1332, loss = 0.58567715
Iteration 1333, loss = 0.58551646
Iteration 1334, loss = 0.58588375
Iteration 1335, loss = 0.58484574
Iteration 1336, loss = 0.58472842
Iteration 1337, loss = 0.58323914
Iteration 1338, loss = 0.58286697
Iteration 1339, loss = 0.58145358
Iteration 1340, loss = 0.58135610
Iteration 1341, loss = 0.58094992
Iteration 1342, loss = 0.58078439
Iteration 1343, loss = 0.58019033
Iteration 1344, loss = 0.57759110
Iteration 1345, loss = 0.57821149
Iteration 1346, loss = 0.57796217
Iteration 1347, loss = 0.57740759
Iteration 1348, loss = 0.57689557
Iteration 1349, loss = 0.57683945
Iteration 1350, loss = 0.57566865
Iteration 1351, loss = 0.57533284
Iteration 1352, loss = 0.57400667
Iteration 1353, loss = 0.57279305
Iteration 1354, loss = 0.57378623
Iteration 1355, loss = 0.57411448
Iteration 1356, loss = 0.57220288
Iteration 1357, loss = 0.57145689
Iteration 1358, loss = 0.56993598
Iteration 1359, loss = 0.57021625
Iteration 1360, loss = 0.57000315
Iteration 1361, loss = 0.56896007
Iteration 1362, loss = 0.56809515
Iteration 1363, loss = 0.56777277
Iteration 1364, loss = 0.56824193
Iteration 1365, loss = 0.56742292
Iteration 1366, loss = 0.56710393
Iteration 1367, loss = 0.56699705
Iteration 1368, loss = 0.56543108
Iteration 1369, loss = 0.56528699
Iteration 1370, loss = 0.56453487
Iteration 1371, loss = 0.56472095
Iteration 1372, loss = 0.56400304
Iteration 1373, loss = 0.56191303
Iteration 1374, loss = 0.56301230
Iteration 1375, loss = 0.56117326
Iteration 1376, loss = 0.56162386
Iteration 1377, loss = 0.56047233
Iteration 1378, loss = 0.55905680
Iteration 1379, loss = 0.55929539
Iteration 1380, loss = 0.55979902
Iteration 1381, loss = 0.55751300
Iteration 1382, loss = 0.55802869
Iteration 1383, loss = 0.55692432
Iteration 1384, loss = 0.55617351
Iteration 1385, loss = 0.55696149
Iteration 1386, loss = 0.55471598
Iteration 1387, loss = 0.55539452
Iteration 1388, loss = 0.55377280
Iteration 1389, loss = 0.55381593
Iteration 1390, loss = 0.55374779
Iteration 1391, loss = 0.55233251
Iteration 1392, loss = 0.55196799
Iteration 1393, loss = 0.55001299
Iteration 1394, loss = 0.55023820
Iteration 1395, loss = 0.55001530
Iteration 1396, loss = 0.55026281
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100), {'error': 1.0207701894156831, 'fit': 0.91574999999999995, 'time': 2466.4500000000007})
Iteration 1, loss = 5.98843606
Iteration 2, loss = 5.66193561
Iteration 3, loss = 5.59915697
Iteration 4, loss = 5.56243509
Iteration 5, loss = 5.53869234
Iteration 6, loss = 5.51852923
Iteration 7, loss = 5.50622014
Iteration 8, loss = 5.47846589
Iteration 9, loss = 5.45904856
Iteration 10, loss = 5.43918739
Iteration 11, loss = 5.41563161
Iteration 12, loss = 5.39378704
Iteration 13, loss = 5.36340005
Iteration 14, loss = 5.34133890
Iteration 15, loss = 5.31799534
Iteration 16, loss = 5.28295176
Iteration 17, loss = 5.26164678
Iteration 18, loss = 5.23375772
Iteration 19, loss = 5.20294059
Iteration 20, loss = 5.16843183
Iteration 21, loss = 5.13532293
Iteration 22, loss = 5.10489828
Iteration 23, loss = 5.06440433
Iteration 24, loss = 5.03084805
Iteration 25, loss = 4.98781605
Iteration 26, loss = 4.94841275
Iteration 27, loss = 4.89949227
Iteration 28, loss = 4.84738368
Iteration 29, loss = 4.79830729
Iteration 30, loss = 4.74630944
Iteration 31, loss = 4.70569613
Iteration 32, loss = 4.64564324
Iteration 33, loss = 4.59162291
Iteration 34, loss = 4.54520024
Iteration 35, loss = 4.48966221
Iteration 36, loss = 4.43859485
Iteration 37, loss = 4.38011533
Iteration 38, loss = 4.33375995
Iteration 39, loss = 4.29381581
Iteration 40, loss = 4.22953025
Iteration 41, loss = 4.18830761
Iteration 42, loss = 4.13894504
Iteration 43, loss = 4.08534161
Iteration 44, loss = 4.03229996
Iteration 45, loss = 3.98931673
Iteration 46, loss = 3.93558941
Iteration 47, loss = 3.89343380
Iteration 48, loss = 3.84184806
Iteration 49, loss = 3.79432307
Iteration 50, loss = 3.74919797
Iteration 51, loss = 3.70386302
Iteration 52, loss = 3.66093991
Iteration 53, loss = 3.61195589
Iteration 54, loss = 3.57140027
Iteration 55, loss = 3.53108828
Iteration 56, loss = 3.49410634
Iteration 57, loss = 3.44854579
Iteration 58, loss = 3.41571409
Iteration 59, loss = 3.36613402
Iteration 60, loss = 3.32499188
Iteration 61, loss = 3.29454789
Iteration 62, loss = 3.25596674
Iteration 63, loss = 3.21334318
Iteration 64, loss = 3.18113656
Iteration 65, loss = 3.14133099
Iteration 66, loss = 3.11310514
Iteration 67, loss = 3.07246011
Iteration 68, loss = 3.02673494
Iteration 69, loss = 2.99527577
Iteration 70, loss = 2.95764376
Iteration 71, loss = 2.92852130
Iteration 72, loss = 2.89756335
Iteration 73, loss = 2.85400397
Iteration 74, loss = 2.82840760
Iteration 75, loss = 2.79625413
Iteration 76, loss = 2.76462052
Iteration 77, loss = 2.72036993
Iteration 78, loss = 2.69866265
Iteration 79, loss = 2.67215210
Iteration 80, loss = 2.64145335
Iteration 81, loss = 2.60595223
Iteration 82, loss = 2.57279026
Iteration 83, loss = 2.54194271
Iteration 84, loss = 2.52715323
Iteration 85, loss = 2.49207167
Iteration 86, loss = 2.45440919
Iteration 87, loss = 2.43120787
Iteration 88, loss = 2.42005420
Iteration 89, loss = 2.38810380
Iteration 90, loss = 2.36218065
Iteration 91, loss = 2.33633933
Iteration 92, loss = 2.31186790
Iteration 93, loss = 2.27862539
Iteration 94, loss = 2.25259829
Iteration 95, loss = 2.23800659
Iteration 96, loss = 2.20888336
Iteration 97, loss = 2.18411938
Iteration 98, loss = 2.17469320
Iteration 99, loss = 2.14179605
Iteration 100, loss = 2.11596461
Iteration 101, loss = 2.09394959
Iteration 102, loss = 2.07100380
Iteration 103, loss = 2.05302829
Iteration 104, loss = 2.03365913
Iteration 105, loss = 2.01021692
Iteration 106, loss = 1.99228955
Iteration 107, loss = 1.97460906
Iteration 108, loss = 1.95253659
Iteration 109, loss = 1.92413969
Iteration 110, loss = 1.91259974
Iteration 111, loss = 1.88452340
Iteration 112, loss = 1.87060785
Iteration 113, loss = 1.85311033
Iteration 114, loss = 1.82948179
Iteration 115, loss = 1.81690769
Iteration 116, loss = 1.79137333
Iteration 117, loss = 1.76919635
Iteration 118, loss = 1.74852684
Iteration 119, loss = 1.73325507
Iteration 120, loss = 1.71805878
Iteration 121, loss = 1.70344162
Iteration 122, loss = 1.67838985
Iteration 123, loss = 1.66340278
Iteration 124, loss = 1.63860917
Iteration 125, loss = 1.63737637
Iteration 126, loss = 1.62051979
Iteration 127, loss = 1.59638685
Iteration 128, loss = 1.58439015
Iteration 129, loss = 1.55805256
Iteration 130, loss = 1.54656079
Iteration 131, loss = 1.52737758
Iteration 132, loss = 1.51587567
Iteration 133, loss = 1.51536796
Iteration 134, loss = 1.48578220
Iteration 135, loss = 1.46836642
Iteration 136, loss = 1.44932802
Iteration 137, loss = 1.42991583
Iteration 138, loss = 1.41729851
Iteration 139, loss = 1.40954402
Iteration 140, loss = 1.39151759
Iteration 141, loss = 1.37843951
Iteration 142, loss = 1.36347022
Iteration 143, loss = 1.34779049
Iteration 144, loss = 1.33692056
Iteration 145, loss = 1.32420019
Iteration 146, loss = 1.31079632
Iteration 147, loss = 1.30566311
Iteration 148, loss = 1.28449611
Iteration 149, loss = 1.26860527
Iteration 150, loss = 1.25466302
Iteration 151, loss = 1.23335111
Iteration 152, loss = 1.22765667
Iteration 153, loss = 1.22465944
Iteration 154, loss = 1.20180452
Iteration 155, loss = 1.19509754
Iteration 156, loss = 1.18112175
Iteration 157, loss = 1.17510034
Iteration 158, loss = 1.15383228
Iteration 159, loss = 1.13943720
Iteration 160, loss = 1.12630185
Iteration 161, loss = 1.11926722
Iteration 162, loss = 1.10554523
Iteration 163, loss = 1.09393605
Iteration 164, loss = 1.08923309
Iteration 165, loss = 1.07574002
Iteration 166, loss = 1.06729580
Iteration 167, loss = 1.05206561
Iteration 168, loss = 1.03458521
Iteration 169, loss = 1.02705305
Iteration 170, loss = 1.01823183
Iteration 171, loss = 1.00896493
Iteration 172, loss = 0.99538219
Iteration 173, loss = 1.00998116
Iteration 174, loss = 0.98882753
Iteration 175, loss = 0.97052245
Iteration 176, loss = 0.95552629
Iteration 177, loss = 0.95044664
Iteration 178, loss = 0.93950262
Iteration 179, loss = 0.94124576
Iteration 180, loss = 0.91883092
Iteration 181, loss = 0.90794858
Iteration 182, loss = 0.89454439
Iteration 183, loss = 0.89260083
Iteration 184, loss = 0.88167453
Iteration 185, loss = 0.86688714
Iteration 186, loss = 0.85527319
Iteration 187, loss = 0.85255001
Iteration 188, loss = 0.85537065
Iteration 189, loss = 0.83344077
Iteration 190, loss = 0.82445282
Iteration 191, loss = 0.81988443
Iteration 192, loss = 0.81511061
Iteration 193, loss = 0.80526246
Iteration 194, loss = 0.78910118
Iteration 195, loss = 0.78364561
Iteration 196, loss = 0.77683608
Iteration 197, loss = 0.77104780
Iteration 198, loss = 0.75721752
Iteration 199, loss = 0.74770795
Iteration 200, loss = 0.74633469
Iteration 201, loss = 0.73756833
Iteration 202, loss = 0.73632963
Iteration 203, loss = 0.72049386
Iteration 204, loss = 0.71269089
Iteration 205, loss = 0.71271694
Iteration 206, loss = 0.69902491
Iteration 207, loss = 0.69228710
Iteration 208, loss = 0.68957126
Iteration 209, loss = 0.68164526
Iteration 210, loss = 0.67953315
Iteration 211, loss = 0.66544395
Iteration 212, loss = 0.65716842
Iteration 213, loss = 0.64982745
Iteration 214, loss = 0.64707937
Iteration 215, loss = 0.63728115
Iteration 216, loss = 0.62875702
Iteration 217, loss = 0.62009851
Iteration 218, loss = 0.62411060
Iteration 219, loss = 0.60743850
Iteration 220, loss = 0.60341577
Iteration 221, loss = 0.59527954
Iteration 222, loss = 0.59048621
Iteration 223, loss = 0.59367385
Iteration 224, loss = 0.58171760
Iteration 225, loss = 0.57578042
Iteration 226, loss = 0.56536641
Iteration 227, loss = 0.55874219
Iteration 228, loss = 0.55552606
Iteration 229, loss = 0.55475095
Iteration 230, loss = 0.54369821
Iteration 231, loss = 0.54071144
Iteration 232, loss = 0.53725670
Iteration 233, loss = 0.53850435
Iteration 234, loss = 0.52885938
Iteration 235, loss = 0.52237395
Iteration 236, loss = 0.51540056
Iteration 237, loss = 0.50942585
Iteration 238, loss = 0.50208494
Iteration 239, loss = 0.49903376
Iteration 240, loss = 0.49620375
Iteration 241, loss = 0.48968733
Iteration 242, loss = 0.48327986
Iteration 243, loss = 0.47717343
Iteration 244, loss = 0.47481959
Iteration 245, loss = 0.47001159
Iteration 246, loss = 0.46477577
Iteration 247, loss = 0.45941545
Iteration 248, loss = 0.45519381
Iteration 249, loss = 0.44973362
Iteration 250, loss = 0.44840219
Iteration 251, loss = 0.44043234
Iteration 252, loss = 0.43315130
Iteration 253, loss = 0.43112774
Iteration 254, loss = 0.42710861
Iteration 255, loss = 0.42456191
Iteration 256, loss = 0.41939289
Iteration 257, loss = 0.42150643
Iteration 258, loss = 0.41011772
Iteration 259, loss = 0.40957451
Iteration 260, loss = 0.40211587
Iteration 261, loss = 0.40338208
Iteration 262, loss = 0.38991765
Iteration 263, loss = 0.38745186
Iteration 264, loss = 0.38094163
Iteration 265, loss = 0.37468229
Iteration 266, loss = 0.37641633
Iteration 267, loss = 0.37615853
Iteration 268, loss = 0.37249501
Iteration 269, loss = 0.36484151
Iteration 270, loss = 0.36395384
Iteration 271, loss = 0.35252178
Iteration 272, loss = 0.35862646
Iteration 273, loss = 0.34848892
Iteration 274, loss = 0.34481644
Iteration 275, loss = 0.34623827
Iteration 276, loss = 0.34155060
Iteration 277, loss = 0.33746779
Iteration 278, loss = 0.32908355
Iteration 279, loss = 0.32967229
Iteration 280, loss = 0.32885228
Iteration 281, loss = 0.32662749
Iteration 282, loss = 0.31929227
Iteration 283, loss = 0.32010220
Iteration 284, loss = 0.32290271
Iteration 285, loss = 0.31081711
Iteration 286, loss = 0.30500871
Iteration 287, loss = 0.30332523
Iteration 288, loss = 0.30327717
Iteration 289, loss = 0.30284246
Iteration 290, loss = 0.29457319
Iteration 291, loss = 0.29879229
Iteration 292, loss = 0.29372594
Iteration 293, loss = 0.28615661
Iteration 294, loss = 0.27716940
Iteration 295, loss = 0.27204676
Iteration 296, loss = 0.27610354
Iteration 297, loss = 0.27008897
Iteration 298, loss = 0.26613151
Iteration 299, loss = 0.26294556
Iteration 300, loss = 0.26446473
Iteration 301, loss = 0.26089126
Iteration 302, loss = 0.25768145
Iteration 303, loss = 0.26236098
Iteration 304, loss = 0.26010995
Iteration 305, loss = 0.25548039
Iteration 306, loss = 0.26185385
Iteration 307, loss = 0.25920214
Iteration 308, loss = 0.25254427
Iteration 309, loss = 0.25215199
Iteration 310, loss = 0.25001542
Iteration 311, loss = 0.24673694
Iteration 312, loss = 0.24329095
Iteration 313, loss = 0.23661242
Iteration 314, loss = 0.22975564
Iteration 315, loss = 0.23286976
Iteration 316, loss = 0.22852281
Iteration 317, loss = 0.22768797
Iteration 318, loss = 0.22348209
Iteration 319, loss = 0.22073041
Iteration 320, loss = 0.22100652
Iteration 321, loss = 0.21770580
Iteration 322, loss = 0.21278787
Iteration 323, loss = 0.20235041
Iteration 324, loss = 0.20916953
Iteration 325, loss = 0.20037686
Iteration 326, loss = 0.19542897
Iteration 327, loss = 0.19409541
Iteration 328, loss = 0.19606319
Iteration 329, loss = 0.19808276
Iteration 330, loss = 0.19905927
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((1000, 1000), {'error': 0.91503756247536805, 'fit': 0.98312500000000003, 'time': 4011.329999999998})
Iteration 1, loss = 6.54060690
Iteration 2, loss = 6.50381530
Iteration 3, loss = 6.46953190
Iteration 4, loss = 6.43714801
Iteration 5, loss = 6.40655304
Iteration 6, loss = 6.37789054
Iteration 7, loss = 6.35084998
Iteration 8, loss = 6.32543958
Iteration 9, loss = 6.30168514
Iteration 10, loss = 6.27934095
Iteration 11, loss = 6.25855524
Iteration 12, loss = 6.23919211
Iteration 13, loss = 6.22108709
Iteration 14, loss = 6.20419858
Iteration 15, loss = 6.18856294
Iteration 16, loss = 6.17401232
Iteration 17, loss = 6.16029323
Iteration 18, loss = 6.14766847
Iteration 19, loss = 6.13594302
Iteration 20, loss = 6.12504603
Iteration 21, loss = 6.11492982
Iteration 22, loss = 6.10547822
Iteration 23, loss = 6.09675259
Iteration 24, loss = 6.08869372
Iteration 25, loss = 6.08122443
Iteration 26, loss = 6.07418847
Iteration 27, loss = 6.06774290
Iteration 28, loss = 6.06176276
Iteration 29, loss = 6.05595511
Iteration 30, loss = 6.05026605
Iteration 31, loss = 6.04408730
Iteration 32, loss = 6.03653248
Iteration 33, loss = 6.02628934
Iteration 34, loss = 6.01422550
Iteration 35, loss = 6.00305994
Iteration 36, loss = 5.99334967
Iteration 37, loss = 5.98471440
Iteration 38, loss = 5.97677599
Iteration 39, loss = 5.96940869
Iteration 40, loss = 5.96233624
Iteration 41, loss = 5.95571952
Iteration 42, loss = 5.94928039
Iteration 43, loss = 5.94318526
Iteration 44, loss = 5.93723083
Iteration 45, loss = 5.93163491
Iteration 46, loss = 5.92612728
Iteration 47, loss = 5.92079890
Iteration 48, loss = 5.91565396
Iteration 49, loss = 5.91075273
Iteration 50, loss = 5.90589470
Iteration 51, loss = 5.90120613
Iteration 52, loss = 5.89674081
Iteration 53, loss = 5.89239138
Iteration 54, loss = 5.88809386
Iteration 55, loss = 5.88391797
Iteration 56, loss = 5.87993076
Iteration 57, loss = 5.87601482
Iteration 58, loss = 5.87222896
Iteration 59, loss = 5.86853000
Iteration 60, loss = 5.86491580
Iteration 61, loss = 5.86138452
Iteration 62, loss = 5.85793341
Iteration 63, loss = 5.85459944
Iteration 64, loss = 5.85140723
Iteration 65, loss = 5.84816453
Iteration 66, loss = 5.84500020
Iteration 67, loss = 5.84198149
Iteration 68, loss = 5.83906050
Iteration 69, loss = 5.83606297
Iteration 70, loss = 5.83321303
Iteration 71, loss = 5.83043800
Iteration 72, loss = 5.82777164
Iteration 73, loss = 5.82511334
Iteration 74, loss = 5.82244553
Iteration 75, loss = 5.81993286
Iteration 76, loss = 5.81743577
Iteration 77, loss = 5.81498615
Iteration 78, loss = 5.81259992
Iteration 79, loss = 5.81026685
Iteration 80, loss = 5.80802382
Iteration 81, loss = 5.80570724
Iteration 82, loss = 5.80361225
Iteration 83, loss = 5.80146154
Iteration 84, loss = 5.79937575
Iteration 85, loss = 5.79733798
Iteration 86, loss = 5.79536744
Iteration 87, loss = 5.79338546
Iteration 88, loss = 5.79151356
Iteration 89, loss = 5.78969695
Iteration 90, loss = 5.78782993
Iteration 91, loss = 5.78608156
Iteration 92, loss = 5.78443305
Iteration 93, loss = 5.78265810
Iteration 94, loss = 5.78100735
Iteration 95, loss = 5.77952298
Iteration 96, loss = 5.77793802
Iteration 97, loss = 5.77636758
Iteration 98, loss = 5.77475105
Iteration 99, loss = 5.77339457
Iteration 100, loss = 5.77180728
Iteration 101, loss = 5.77049560
Iteration 102, loss = 5.76900920
Iteration 103, loss = 5.76765874
Iteration 104, loss = 5.76633228
Iteration 105, loss = 5.76502270
Iteration 106, loss = 5.76386223
Iteration 107, loss = 5.76245861
Iteration 108, loss = 5.76127014
Iteration 109, loss = 5.76016644
Iteration 110, loss = 5.75894557
Iteration 111, loss = 5.75778633
Iteration 112, loss = 5.75670287
Iteration 113, loss = 5.75551089
Iteration 114, loss = 5.75451261
Iteration 115, loss = 5.75339330
Iteration 116, loss = 5.75238691
Iteration 117, loss = 5.75139283
Iteration 118, loss = 5.75033868
Iteration 119, loss = 5.74937870
Iteration 120, loss = 5.74847009
Iteration 121, loss = 5.74747066
Iteration 122, loss = 5.74673036
Iteration 123, loss = 5.74573612
Iteration 124, loss = 5.74482908
Iteration 125, loss = 5.74395680
Iteration 126, loss = 5.74311827
Iteration 127, loss = 5.74226074
Iteration 128, loss = 5.74138819
Iteration 129, loss = 5.74070521
Iteration 130, loss = 5.73985324
Iteration 131, loss = 5.73907488
Iteration 132, loss = 5.73837501
Iteration 133, loss = 5.73751947
Iteration 134, loss = 5.73679850
Iteration 135, loss = 5.73611509
Iteration 136, loss = 5.73540555
Iteration 137, loss = 5.73473459
Iteration 138, loss = 5.73397219
Iteration 139, loss = 5.73333145
Iteration 140, loss = 5.73271430
Iteration 141, loss = 5.73225068
Iteration 142, loss = 5.73141984
Iteration 143, loss = 5.73088674
Iteration 144, loss = 5.73007186
Iteration 145, loss = 5.72953024
Iteration 146, loss = 5.72893792
Iteration 147, loss = 5.72825095
Iteration 148, loss = 5.72775959
Iteration 149, loss = 5.72706778
Iteration 150, loss = 5.72643489
Iteration 151, loss = 5.72590079
Iteration 152, loss = 5.72533809
Iteration 153, loss = 5.72487156
Iteration 154, loss = 5.72421230
Iteration 155, loss = 5.72383902
Iteration 156, loss = 5.72319716
Iteration 157, loss = 5.72268225
Iteration 158, loss = 5.72215983
Iteration 159, loss = 5.72165028
Iteration 160, loss = 5.72134401
Iteration 161, loss = 5.72079505
Iteration 162, loss = 5.72031980
Iteration 163, loss = 5.71986271
Iteration 164, loss = 5.71931913
Iteration 165, loss = 5.71871634
Iteration 166, loss = 5.71834312
Iteration 167, loss = 5.71796288
Iteration 168, loss = 5.71750735
Iteration 169, loss = 5.71700491
Iteration 170, loss = 5.71657653
Iteration 171, loss = 5.71620862
Iteration 172, loss = 5.71565615
Iteration 173, loss = 5.71527952
Iteration 174, loss = 5.71483281
Iteration 175, loss = 5.71468736
Iteration 176, loss = 5.71401370
Iteration 177, loss = 5.71367380
Iteration 178, loss = 5.71335051
Iteration 179, loss = 5.71288715
Iteration 180, loss = 5.71246902
Iteration 181, loss = 5.71212054
Iteration 182, loss = 5.71173553
Iteration 183, loss = 5.71131253
Iteration 184, loss = 5.71098626
Iteration 185, loss = 5.71068533
Iteration 186, loss = 5.71043983
Iteration 187, loss = 5.70993602
Iteration 188, loss = 5.70961694
Iteration 189, loss = 5.70929979
Iteration 190, loss = 5.70892496
Iteration 191, loss = 5.70866009
Iteration 192, loss = 5.70831460
Iteration 193, loss = 5.70787771
Iteration 194, loss = 5.70764280
Iteration 195, loss = 5.70727169
Iteration 196, loss = 5.70702218
Iteration 197, loss = 5.70658330
Iteration 198, loss = 5.70637321
Iteration 199, loss = 5.70602152
Iteration 200, loss = 5.70567023
Iteration 201, loss = 5.70537833
Iteration 202, loss = 5.70510204
Iteration 203, loss = 5.70485704
Iteration 204, loss = 5.70446093
Iteration 205, loss = 5.70427232
Iteration 206, loss = 5.70390333
Iteration 207, loss = 5.70365322
Iteration 208, loss = 5.70332611
Iteration 209, loss = 5.70295573
Iteration 210, loss = 5.70282814
Iteration 211, loss = 5.70253082
Iteration 212, loss = 5.70210881
Iteration 213, loss = 5.70200724
Iteration 214, loss = 5.70185982
Iteration 215, loss = 5.70132700
Iteration 216, loss = 5.70104906
Iteration 217, loss = 5.70093388
Iteration 218, loss = 5.70067811
Iteration 219, loss = 5.70045776
Iteration 220, loss = 5.70010275
Iteration 221, loss = 5.69986310
Iteration 222, loss = 5.69956941
Iteration 223, loss = 5.69934478
Iteration 224, loss = 5.69917986
Iteration 225, loss = 5.69881595
Iteration 226, loss = 5.69866228
Iteration 227, loss = 5.69860685
Iteration 228, loss = 5.69817425
Iteration 229, loss = 5.69815059
Iteration 230, loss = 5.69768435
Iteration 231, loss = 5.69757896
Iteration 232, loss = 5.69713669
Iteration 233, loss = 5.69700735
Iteration 234, loss = 5.69676199
Iteration 235, loss = 5.69656962
Iteration 236, loss = 5.69644734
Iteration 237, loss = 5.69629120
Iteration 238, loss = 5.69596543
Iteration 239, loss = 5.69578184
Iteration 240, loss = 5.69539569
Iteration 241, loss = 5.69548982
Iteration 242, loss = 5.69503666
Iteration 243, loss = 5.69484257
Iteration 244, loss = 5.69460539
Iteration 245, loss = 5.69443117
Iteration 246, loss = 5.69418510
Iteration 247, loss = 5.69395396
Iteration 248, loss = 5.69377400
Iteration 249, loss = 5.69360432
Iteration 250, loss = 5.69369618
Iteration 251, loss = 5.69317567
Iteration 252, loss = 5.69314099
Iteration 253, loss = 5.69275111
Iteration 254, loss = 5.69268674
Iteration 255, loss = 5.69242083
Iteration 256, loss = 5.69217069
Iteration 257, loss = 5.69201972
Iteration 258, loss = 5.69211557
Iteration 259, loss = 5.69156238
Iteration 260, loss = 5.69173294
Iteration 261, loss = 5.69118304
Iteration 262, loss = 5.69115627
Iteration 263, loss = 5.69094597
Iteration 264, loss = 5.69073226
Iteration 265, loss = 5.69071020
Iteration 266, loss = 5.69028753
Iteration 267, loss = 5.69015801
Iteration 268, loss = 5.69003890
Iteration 269, loss = 5.68990216
Iteration 270, loss = 5.68973235
Iteration 271, loss = 5.68964994
Iteration 272, loss = 5.68938941
Iteration 273, loss = 5.68922284
Iteration 274, loss = 5.68904712
Iteration 275, loss = 5.68886325
Iteration 276, loss = 5.68862224
Iteration 277, loss = 5.68840807
Iteration 278, loss = 5.68836856
Iteration 279, loss = 5.68830363
Iteration 280, loss = 5.68816395
Iteration 281, loss = 5.68792551
Iteration 282, loss = 5.68776293
Iteration 283, loss = 5.68767313
Iteration 284, loss = 5.68760613
Iteration 285, loss = 5.68727052
Iteration 286, loss = 5.68714857
Iteration 287, loss = 5.68693169
Iteration 288, loss = 5.68678309
Iteration 289, loss = 5.68649004
Iteration 290, loss = 5.68634461
Iteration 291, loss = 5.68616649
Iteration 292, loss = 5.68608191
Iteration 293, loss = 5.68592406
Iteration 294, loss = 5.68574041
Iteration 295, loss = 5.68612679
Iteration 296, loss = 5.68549382
Iteration 297, loss = 5.68544753
Iteration 298, loss = 5.68538207
Iteration 299, loss = 5.68509784
Iteration 300, loss = 5.68498944
Iteration 301, loss = 5.68484904
Iteration 302, loss = 5.68487546
Iteration 303, loss = 5.68460303
Iteration 304, loss = 5.68466972
Iteration 305, loss = 5.68441417
Iteration 306, loss = 5.68416177
Iteration 307, loss = 5.68392301
Iteration 308, loss = 5.68390078
Iteration 309, loss = 5.68388507
Iteration 310, loss = 5.68389456
Iteration 311, loss = 5.68340232
Iteration 312, loss = 5.68327201
Iteration 313, loss = 5.68309285
Iteration 314, loss = 5.68305346
Iteration 315, loss = 5.68307185
Iteration 316, loss = 5.68306126
Iteration 317, loss = 5.68281393
Iteration 318, loss = 5.68263324
Iteration 319, loss = 5.68268945
Iteration 320, loss = 5.68225511
Iteration 321, loss = 5.68215566
Iteration 322, loss = 5.68212053
Iteration 323, loss = 5.68182672
Iteration 324, loss = 5.68180808
Iteration 325, loss = 5.68152775
Iteration 326, loss = 5.68159141
Iteration 327, loss = 5.68141889
Iteration 328, loss = 5.68113951
Iteration 329, loss = 5.68119544
Iteration 330, loss = 5.68117451
Iteration 331, loss = 5.68079969
Iteration 332, loss = 5.68085048
Iteration 333, loss = 5.68069455
Iteration 334, loss = 5.68064268
Iteration 335, loss = 5.68031631
Iteration 336, loss = 5.68050515
Iteration 337, loss = 5.68015693
Iteration 338, loss = 5.68010531
Iteration 339, loss = 5.68004502
Iteration 340, loss = 5.68013868
Iteration 341, loss = 5.67971069
Iteration 342, loss = 5.67983107
Iteration 343, loss = 5.67956824
Iteration 344, loss = 5.67938487
Iteration 345, loss = 5.67929643
Iteration 346, loss = 5.67913976
Iteration 347, loss = 5.67910197
Iteration 348, loss = 5.67897351
Iteration 349, loss = 5.67889120
Iteration 350, loss = 5.67863824
Iteration 351, loss = 5.67868117
Iteration 352, loss = 5.67866961
Iteration 353, loss = 5.67838754
Iteration 354, loss = 5.67825914
Iteration 355, loss = 5.67823329
Iteration 356, loss = 5.67826018
Iteration 357, loss = 5.67786590
Iteration 358, loss = 5.67805228
Iteration 359, loss = 5.67785826
Iteration 360, loss = 5.67765988
Iteration 361, loss = 5.67750343
Iteration 362, loss = 5.67761422
Iteration 363, loss = 5.67746790
Iteration 364, loss = 5.67732064
Iteration 365, loss = 5.67735724
Iteration 366, loss = 5.67739016
Iteration 367, loss = 5.67704353
Iteration 368, loss = 5.67696255
Iteration 369, loss = 5.67674885
Iteration 370, loss = 5.67657744
Iteration 371, loss = 5.67650977
Iteration 372, loss = 5.67657345
Iteration 373, loss = 5.67629777
Iteration 374, loss = 5.67636681
Iteration 375, loss = 5.67624259
Iteration 376, loss = 5.67591653
Iteration 377, loss = 5.67600700
Iteration 378, loss = 5.67629791
Iteration 379, loss = 5.67586007
Iteration 380, loss = 5.67576978
Iteration 381, loss = 5.67597309
Iteration 382, loss = 5.67566555
Iteration 383, loss = 5.67581036
Iteration 384, loss = 5.67542778
Iteration 385, loss = 5.67551039
Iteration 386, loss = 5.67498872
Iteration 387, loss = 5.67522571
Iteration 388, loss = 5.67504568
Iteration 389, loss = 5.67486097
Iteration 390, loss = 5.67488227
Iteration 391, loss = 5.67472325
Iteration 392, loss = 5.67479333
Iteration 393, loss = 5.67451106
Iteration 394, loss = 5.67446158
Iteration 395, loss = 5.67446514
Iteration 396, loss = 5.67423772
Iteration 397, loss = 5.67401122
Iteration 398, loss = 5.67403932
Iteration 399, loss = 5.67403148
Iteration 400, loss = 5.67386969
Iteration 401, loss = 5.67385011
Iteration 402, loss = 5.67377315
Iteration 403, loss = 5.67369107
Iteration 404, loss = 5.67343809
Iteration 405, loss = 5.67370225
Iteration 406, loss = 5.67340893
Iteration 407, loss = 5.67346484
Iteration 408, loss = 5.67332627
Iteration 409, loss = 5.67331607
Iteration 410, loss = 5.67309697
Iteration 411, loss = 5.67310052
Iteration 412, loss = 5.67285115
Iteration 413, loss = 5.67289783
Iteration 414, loss = 5.67277432
Iteration 415, loss = 5.67261731
Iteration 416, loss = 5.67257149
Iteration 417, loss = 5.67235479
Iteration 418, loss = 5.67247485
Iteration 419, loss = 5.67238813
Iteration 420, loss = 5.67220303
Iteration 421, loss = 5.67254146
Iteration 422, loss = 5.67222131
Iteration 423, loss = 5.67200139
Iteration 424, loss = 5.67209237
Iteration 425, loss = 5.67183902
Iteration 426, loss = 5.67188958
Iteration 427, loss = 5.67179121
Iteration 428, loss = 5.67168091
Iteration 429, loss = 5.67155320
Iteration 430, loss = 5.67154750
Iteration 431, loss = 5.67144885
Iteration 432, loss = 5.67138638
Iteration 433, loss = 5.67129718
Iteration 434, loss = 5.67131491
Iteration 435, loss = 5.67106467
Iteration 436, loss = 5.67106566
Iteration 437, loss = 5.67105571
Iteration 438, loss = 5.67088342
Iteration 439, loss = 5.67085065
Iteration 440, loss = 5.67074290
Iteration 441, loss = 5.67089381
Iteration 442, loss = 5.67048272
Iteration 443, loss = 5.67077724
Iteration 444, loss = 5.67053124
Iteration 445, loss = 5.67076519
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((1, 1, 1), {'error': 2.1491427305647548, 'fit': 0.021624999999999998, 'time': 524.4600000000028})
Iteration 1, loss = 6.51999278
Iteration 2, loss = 6.40296580
Iteration 3, loss = 6.22664099
Iteration 4, loss = 6.08172600
Iteration 5, loss = 5.99610140
Iteration 6, loss = 5.94512240
Iteration 7, loss = 5.91105863
Iteration 8, loss = 5.88368053
Iteration 9, loss = 5.86023700
Iteration 10, loss = 5.83897959
Iteration 11, loss = 5.81919448
Iteration 12, loss = 5.80043334
Iteration 13, loss = 5.78269580
Iteration 14, loss = 5.76528538
Iteration 15, loss = 5.74798054
Iteration 16, loss = 5.72990085
Iteration 17, loss = 5.70959856
Iteration 18, loss = 5.68653559
Iteration 19, loss = 5.66015309
Iteration 20, loss = 5.63439999
Iteration 21, loss = 5.61089798
Iteration 22, loss = 5.58959469
Iteration 23, loss = 5.57065047
Iteration 24, loss = 5.55330638
Iteration 25, loss = 5.53797608
Iteration 26, loss = 5.52337000
Iteration 27, loss = 5.50936534
Iteration 28, loss = 5.49722292
Iteration 29, loss = 5.48532737
Iteration 30, loss = 5.47409769
Iteration 31, loss = 5.46350957
Iteration 32, loss = 5.45375080
Iteration 33, loss = 5.44394904
Iteration 34, loss = 5.43540707
Iteration 35, loss = 5.42696484
Iteration 36, loss = 5.41952440
Iteration 37, loss = 5.41197380
Iteration 38, loss = 5.40493875
Iteration 39, loss = 5.39837123
Iteration 40, loss = 5.39228605
Iteration 41, loss = 5.38654955
Iteration 42, loss = 5.38097648
Iteration 43, loss = 5.37572416
Iteration 44, loss = 5.37024969
Iteration 45, loss = 5.36562247
Iteration 46, loss = 5.36123313
Iteration 47, loss = 5.35694931
Iteration 48, loss = 5.35250432
Iteration 49, loss = 5.34838173
Iteration 50, loss = 5.34369138
Iteration 51, loss = 5.33995912
Iteration 52, loss = 5.33643593
Iteration 53, loss = 5.33238294
Iteration 54, loss = 5.32848665
Iteration 55, loss = 5.32429505
Iteration 56, loss = 5.32116609
Iteration 57, loss = 5.31794460
Iteration 58, loss = 5.31410045
Iteration 59, loss = 5.31104002
Iteration 60, loss = 5.30731504
Iteration 61, loss = 5.30401020
Iteration 62, loss = 5.30074063
Iteration 63, loss = 5.29820342
Iteration 64, loss = 5.29433835
Iteration 65, loss = 5.29067918
Iteration 66, loss = 5.28759894
Iteration 67, loss = 5.28387453
Iteration 68, loss = 5.28129994
Iteration 69, loss = 5.27746596
Iteration 70, loss = 5.27402668
Iteration 71, loss = 5.27125135
Iteration 72, loss = 5.26842732
Iteration 73, loss = 5.26491586
Iteration 74, loss = 5.26155519
Iteration 75, loss = 5.25884925
Iteration 76, loss = 5.25561609
Iteration 77, loss = 5.25244032
Iteration 78, loss = 5.24941003
Iteration 79, loss = 5.24621135
Iteration 80, loss = 5.24293882
Iteration 81, loss = 5.24012230
Iteration 82, loss = 5.23653189
Iteration 83, loss = 5.23278631
Iteration 84, loss = 5.22991116
Iteration 85, loss = 5.22632887
Iteration 86, loss = 5.22307355
Iteration 87, loss = 5.21981721
Iteration 88, loss = 5.21661712
Iteration 89, loss = 5.21318232
Iteration 90, loss = 5.21040981
Iteration 91, loss = 5.20783935
Iteration 92, loss = 5.20464222
Iteration 93, loss = 5.20130433
Iteration 94, loss = 5.19816309
Iteration 95, loss = 5.19546979
Iteration 96, loss = 5.19224292
Iteration 97, loss = 5.18978118
Iteration 98, loss = 5.18721355
Iteration 99, loss = 5.18365812
Iteration 100, loss = 5.18121580
Iteration 101, loss = 5.17834350
Iteration 102, loss = 5.17582580
Iteration 103, loss = 5.17257057
Iteration 104, loss = 5.17093187
Iteration 105, loss = 5.16834398
Iteration 106, loss = 5.16448313
Iteration 107, loss = 5.16202859
Iteration 108, loss = 5.15919892
Iteration 109, loss = 5.15697993
Iteration 110, loss = 5.15477337
Iteration 111, loss = 5.15175250
Iteration 112, loss = 5.14892974
Iteration 113, loss = 5.14697909
Iteration 114, loss = 5.14404292
Iteration 115, loss = 5.14098741
Iteration 116, loss = 5.13898705
Iteration 117, loss = 5.13670757
Iteration 118, loss = 5.13362652
Iteration 119, loss = 5.13089738
Iteration 120, loss = 5.12883057
Iteration 121, loss = 5.12605365
Iteration 122, loss = 5.12386545
Iteration 123, loss = 5.12062729
Iteration 124, loss = 5.11881889
Iteration 125, loss = 5.11613839
Iteration 126, loss = 5.11337660
Iteration 127, loss = 5.11145697
Iteration 128, loss = 5.10833580
Iteration 129, loss = 5.10620231
Iteration 130, loss = 5.10521721
Iteration 131, loss = 5.10185520
Iteration 132, loss = 5.09898280
Iteration 133, loss = 5.09684330
Iteration 134, loss = 5.09461897
Iteration 135, loss = 5.09146503
Iteration 136, loss = 5.08971431
Iteration 137, loss = 5.08688328
Iteration 138, loss = 5.08467356
Iteration 139, loss = 5.08172359
Iteration 140, loss = 5.08027768
Iteration 141, loss = 5.07709418
Iteration 142, loss = 5.07468251
Iteration 143, loss = 5.07240136
Iteration 144, loss = 5.07042035
Iteration 145, loss = 5.06827950
Iteration 146, loss = 5.06620123
Iteration 147, loss = 5.06307564
Iteration 148, loss = 5.06165681
Iteration 149, loss = 5.05830654
Iteration 150, loss = 5.05625688
Iteration 151, loss = 5.05389636
Iteration 152, loss = 5.05179608
Iteration 153, loss = 5.04960080
Iteration 154, loss = 5.04703280
Iteration 155, loss = 5.04512297
Iteration 156, loss = 5.04337541
Iteration 157, loss = 5.04073661
Iteration 158, loss = 5.03797224
Iteration 159, loss = 5.03708882
Iteration 160, loss = 5.03481218
Iteration 161, loss = 5.03176466
Iteration 162, loss = 5.03007768
Iteration 163, loss = 5.02774514
Iteration 164, loss = 5.02459214
Iteration 165, loss = 5.02424740
Iteration 166, loss = 5.02165826
Iteration 167, loss = 5.01881349
Iteration 168, loss = 5.01747676
Iteration 169, loss = 5.01527762
Iteration 170, loss = 5.01304857
Iteration 171, loss = 5.01109072
Iteration 172, loss = 5.00886851
Iteration 173, loss = 5.00691899
Iteration 174, loss = 5.00493405
Iteration 175, loss = 5.00286461
Iteration 176, loss = 5.00132278
Iteration 177, loss = 4.99941203
Iteration 178, loss = 4.99740493
Iteration 179, loss = 4.99498376
Iteration 180, loss = 4.99279350
Iteration 181, loss = 4.99095641
Iteration 182, loss = 4.98866870
Iteration 183, loss = 4.98776557
Iteration 184, loss = 4.98508903
Iteration 185, loss = 4.98298059
Iteration 186, loss = 4.98108679
Iteration 187, loss = 4.97979341
Iteration 188, loss = 4.97747886
Iteration 189, loss = 4.97588266
Iteration 190, loss = 4.97361198
Iteration 191, loss = 4.97215485
Iteration 192, loss = 4.97010161
Iteration 193, loss = 4.96821332
Iteration 194, loss = 4.96668361
Iteration 195, loss = 4.96438042
Iteration 196, loss = 4.96296104
Iteration 197, loss = 4.96089931
Iteration 198, loss = 4.96005422
Iteration 199, loss = 4.95736937
Iteration 200, loss = 4.95609186
Iteration 201, loss = 4.95417430
Iteration 202, loss = 4.95207906
Iteration 203, loss = 4.95111923
Iteration 204, loss = 4.94974199
Iteration 205, loss = 4.94732955
Iteration 206, loss = 4.94569945
Iteration 207, loss = 4.94392645
Iteration 208, loss = 4.94281516
Iteration 209, loss = 4.94031527
Iteration 210, loss = 4.93895707
Iteration 211, loss = 4.93772318
Iteration 212, loss = 4.93639822
Iteration 213, loss = 4.93425067
Iteration 214, loss = 4.93337139
Iteration 215, loss = 4.93211624
Iteration 216, loss = 4.92986106
Iteration 217, loss = 4.92765109
Iteration 218, loss = 4.92624671
Iteration 219, loss = 4.92443080
Iteration 220, loss = 4.92341810
Iteration 221, loss = 4.92184105
Iteration 222, loss = 4.92013507
Iteration 223, loss = 4.92022161
Iteration 224, loss = 4.91783831
Iteration 225, loss = 4.91733041
Iteration 226, loss = 4.91465198
Iteration 227, loss = 4.91298762
Iteration 228, loss = 4.91171795
Iteration 229, loss = 4.91039985
Iteration 230, loss = 4.90905204
Iteration 231, loss = 4.90832500
Iteration 232, loss = 4.90658532
Iteration 233, loss = 4.90401542
Iteration 234, loss = 4.90327326
Iteration 235, loss = 4.90119506
Iteration 236, loss = 4.90113222
Iteration 237, loss = 4.89865335
Iteration 238, loss = 4.89718812
Iteration 239, loss = 4.89541436
Iteration 240, loss = 4.89478594
Iteration 241, loss = 4.89306497
Iteration 242, loss = 4.89278763
Iteration 243, loss = 4.89066247
Iteration 244, loss = 4.88902230
Iteration 245, loss = 4.88779346
Iteration 246, loss = 4.88646233
Iteration 247, loss = 4.88497628
Iteration 248, loss = 4.88420162
Iteration 249, loss = 4.88349927
Iteration 250, loss = 4.88245883
Iteration 251, loss = 4.88105656
Iteration 252, loss = 4.87977970
Iteration 253, loss = 4.87875683
Iteration 254, loss = 4.87719270
Iteration 255, loss = 4.87607506
Iteration 256, loss = 4.87404954
Iteration 257, loss = 4.87235707
Iteration 258, loss = 4.87079929
Iteration 259, loss = 4.86947139
Iteration 260, loss = 4.86885161
Iteration 261, loss = 4.86741166
Iteration 262, loss = 4.86565816
Iteration 263, loss = 4.86425308
Iteration 264, loss = 4.86360660
Iteration 265, loss = 4.86233066
Iteration 266, loss = 4.86120118
Iteration 267, loss = 4.86017139
Iteration 268, loss = 4.85873809
Iteration 269, loss = 4.85819816
Iteration 270, loss = 4.85671904
Iteration 271, loss = 4.85560925
Iteration 272, loss = 4.85418535
Iteration 273, loss = 4.85350490
Iteration 274, loss = 4.85203625
Iteration 275, loss = 4.85099576
Iteration 276, loss = 4.85105740
Iteration 277, loss = 4.84875884
Iteration 278, loss = 4.84765217
Iteration 279, loss = 4.84702290
Iteration 280, loss = 4.84466843
Iteration 281, loss = 4.84367460
Iteration 282, loss = 4.84364089
Iteration 283, loss = 4.84181174
Iteration 284, loss = 4.84037447
Iteration 285, loss = 4.84042342
Iteration 286, loss = 4.83999307
Iteration 287, loss = 4.83684538
Iteration 288, loss = 4.83594717
Iteration 289, loss = 4.83418991
Iteration 290, loss = 4.83386388
Iteration 291, loss = 4.83284494
Iteration 292, loss = 4.83196007
Iteration 293, loss = 4.83090559
Iteration 294, loss = 4.83113269
Iteration 295, loss = 4.82802530
Iteration 296, loss = 4.82731976
Iteration 297, loss = 4.82634574
Iteration 298, loss = 4.82500126
Iteration 299, loss = 4.82404655
Iteration 300, loss = 4.82323734
Iteration 301, loss = 4.82211268
Iteration 302, loss = 4.82074966
Iteration 303, loss = 4.82147944
Iteration 304, loss = 4.81839020
Iteration 305, loss = 4.81883135
Iteration 306, loss = 4.81701091
Iteration 307, loss = 4.81568966
Iteration 308, loss = 4.81457433
Iteration 309, loss = 4.81363871
Iteration 310, loss = 4.81339829
Iteration 311, loss = 4.81166108
Iteration 312, loss = 4.81034067
Iteration 313, loss = 4.81012943
Iteration 314, loss = 4.80845997
Iteration 315, loss = 4.80791424
Iteration 316, loss = 4.80790067
Iteration 317, loss = 4.80682559
Iteration 318, loss = 4.80533359
Iteration 319, loss = 4.80454191
Iteration 320, loss = 4.80209128
Iteration 321, loss = 4.80159399
Iteration 322, loss = 4.80147389
Iteration 323, loss = 4.80025829
Iteration 324, loss = 4.79839343
Iteration 325, loss = 4.79884418
Iteration 326, loss = 4.79708733
Iteration 327, loss = 4.79516463
Iteration 328, loss = 4.79498407
Iteration 329, loss = 4.79461576
Iteration 330, loss = 4.79324136
Iteration 331, loss = 4.79183689
Iteration 332, loss = 4.79141416
Iteration 333, loss = 4.79120570
Iteration 334, loss = 4.78948741
Iteration 335, loss = 4.78839583
Iteration 336, loss = 4.78813541
Iteration 337, loss = 4.78823241
Iteration 338, loss = 4.78618720
Iteration 339, loss = 4.78435978
Iteration 340, loss = 4.78454970
Iteration 341, loss = 4.78249644
Iteration 342, loss = 4.78167843
Iteration 343, loss = 4.78143065
Iteration 344, loss = 4.78085590
Iteration 345, loss = 4.77920886
Iteration 346, loss = 4.77801122
Iteration 347, loss = 4.77735669
Iteration 348, loss = 4.77631970
Iteration 349, loss = 4.77586204
Iteration 350, loss = 4.77485228
Iteration 351, loss = 4.77398979
Iteration 352, loss = 4.77324618
Iteration 353, loss = 4.77285645
Iteration 354, loss = 4.77244499
Iteration 355, loss = 4.77082923
Iteration 356, loss = 4.77037044
Iteration 357, loss = 4.76954082
Iteration 358, loss = 4.76929120
Iteration 359, loss = 4.76678033
Iteration 360, loss = 4.76668760
Iteration 361, loss = 4.76529580
Iteration 362, loss = 4.76463468
Iteration 363, loss = 4.76357613
Iteration 364, loss = 4.76310793
Iteration 365, loss = 4.76199525
Iteration 366, loss = 4.76075457
Iteration 367, loss = 4.76009827
Iteration 368, loss = 4.75950322
Iteration 369, loss = 4.76050031
Iteration 370, loss = 4.75829122
Iteration 371, loss = 4.75705242
Iteration 372, loss = 4.75786407
Iteration 373, loss = 4.75564231
Iteration 374, loss = 4.75508856
Iteration 375, loss = 4.75431087
Iteration 376, loss = 4.75301715
Iteration 377, loss = 4.75294986
Iteration 378, loss = 4.75118364
Iteration 379, loss = 4.75146812
Iteration 380, loss = 4.75074055
Iteration 381, loss = 4.74947191
Iteration 382, loss = 4.74928856
Iteration 383, loss = 4.74835325
Iteration 384, loss = 4.74795628
Iteration 385, loss = 4.74644153
Iteration 386, loss = 4.74578978
Iteration 387, loss = 4.74446001
Iteration 388, loss = 4.74517556
Iteration 389, loss = 4.74331464
Iteration 390, loss = 4.74238138
Iteration 391, loss = 4.74124030
Iteration 392, loss = 4.74105749
Iteration 393, loss = 4.74137738
Iteration 394, loss = 4.74115879
Iteration 395, loss = 4.73975035
Iteration 396, loss = 4.73742662
Iteration 397, loss = 4.73749828
Iteration 398, loss = 4.73758531
Iteration 399, loss = 4.73644402
Iteration 400, loss = 4.73576472
Iteration 401, loss = 4.73513824
Iteration 402, loss = 4.73443520
Iteration 403, loss = 4.73377366
Iteration 404, loss = 4.73269479
Iteration 405, loss = 4.73224652
Iteration 406, loss = 4.73097571
Iteration 407, loss = 4.73148197
Iteration 408, loss = 4.73002654
Iteration 409, loss = 4.72893405
Iteration 410, loss = 4.72853887
Iteration 411, loss = 4.72849740
Iteration 412, loss = 4.72857607
Iteration 413, loss = 4.72716167
Iteration 414, loss = 4.72596153
Iteration 415, loss = 4.72460228
Iteration 416, loss = 4.72502394
Iteration 417, loss = 4.72485046
Iteration 418, loss = 4.72416877
Iteration 419, loss = 4.72249358
Iteration 420, loss = 4.72280693
Iteration 421, loss = 4.72239740
Iteration 422, loss = 4.72071177
Iteration 423, loss = 4.72014092
Iteration 424, loss = 4.71919227
Iteration 425, loss = 4.71888970
Iteration 426, loss = 4.71875677
Iteration 427, loss = 4.71773387
Iteration 428, loss = 4.71749595
Iteration 429, loss = 4.71706376
Iteration 430, loss = 4.71641865
Iteration 431, loss = 4.71513658
Iteration 432, loss = 4.71573932
Iteration 433, loss = 4.71395376
Iteration 434, loss = 4.71359147
Iteration 435, loss = 4.71333032
Iteration 436, loss = 4.71199364
Iteration 437, loss = 4.71194287
Iteration 438, loss = 4.71155976
Iteration 439, loss = 4.71005188
Iteration 440, loss = 4.70998584
Iteration 441, loss = 4.70907158
Iteration 442, loss = 4.71019978
Iteration 443, loss = 4.70880425
Iteration 444, loss = 4.70739846
Iteration 445, loss = 4.70632850
Iteration 446, loss = 4.70679449
Iteration 447, loss = 4.70462150
Iteration 448, loss = 4.70483208
Iteration 449, loss = 4.70497674
Iteration 450, loss = 4.70355463
Iteration 451, loss = 4.70348821
Iteration 452, loss = 4.70249702
Iteration 453, loss = 4.70328838
Iteration 454, loss = 4.70197589
Iteration 455, loss = 4.70184104
Iteration 456, loss = 4.70066352
Iteration 457, loss = 4.69966847
Iteration 458, loss = 4.69998352
Iteration 459, loss = 4.70014318
Iteration 460, loss = 4.69758603
Iteration 461, loss = 4.69831205
Iteration 462, loss = 4.69697714
Iteration 463, loss = 4.69595255
Iteration 464, loss = 4.69492628
Iteration 465, loss = 4.69535473
Iteration 466, loss = 4.69477933
Iteration 467, loss = 4.69396168
Iteration 468, loss = 4.69412121
Iteration 469, loss = 4.69305003
Iteration 470, loss = 4.69349072
Iteration 471, loss = 4.69327815
Iteration 472, loss = 4.69135834
Iteration 473, loss = 4.69038983
Iteration 474, loss = 4.69003978
Iteration 475, loss = 4.68986970
Iteration 476, loss = 4.69053645
Iteration 477, loss = 4.69006093
Iteration 478, loss = 4.68892257
Iteration 479, loss = 4.68817234
Iteration 480, loss = 4.68743339
Iteration 481, loss = 4.68739752
Iteration 482, loss = 4.68633838
Iteration 483, loss = 4.68590701
Iteration 484, loss = 4.68606014
Iteration 485, loss = 4.68463135
Iteration 486, loss = 4.68398585
Iteration 487, loss = 4.68442433
Iteration 488, loss = 4.68317003
Iteration 489, loss = 4.68352132
Iteration 490, loss = 4.68253895
Iteration 491, loss = 4.68183614
Iteration 492, loss = 4.68165457
Iteration 493, loss = 4.68100218
Iteration 494, loss = 4.68180649
Iteration 495, loss = 4.68025463
Iteration 496, loss = 4.68003763
Iteration 497, loss = 4.67904593
Iteration 498, loss = 4.67804418
Iteration 499, loss = 4.67724096
Iteration 500, loss = 4.67704165
Iteration 501, loss = 4.67707164
Iteration 502, loss = 4.67605386
Iteration 503, loss = 4.67537781
Iteration 504, loss = 4.67689571
Iteration 505, loss = 4.67460778
Iteration 506, loss = 4.67409265
Iteration 507, loss = 4.67366982
Iteration 508, loss = 4.67403234
Iteration 509, loss = 4.67291626
Iteration 510, loss = 4.67293730
Iteration 511, loss = 4.67385464
Iteration 512, loss = 4.67125441
Iteration 513, loss = 4.67081947
Iteration 514, loss = 4.66981980
Iteration 515, loss = 4.67092597
Iteration 516, loss = 4.66948036
Iteration 517, loss = 4.66935047
Iteration 518, loss = 4.66919893
Iteration 519, loss = 4.66886814
Iteration 520, loss = 4.66913714
Iteration 521, loss = 4.66777543
Iteration 522, loss = 4.66647121
Iteration 523, loss = 4.66623361
Iteration 524, loss = 4.66645213
Iteration 525, loss = 4.66654955
Iteration 526, loss = 4.66684095
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((10, 10, 10), {'error': 0.79624906066751744, 'fit': 0.049625000000000002, 'time': 673.25})
Iteration 1, loss = 6.24262589
Iteration 2, loss = 5.75626398
Iteration 3, loss = 5.62314867
Iteration 4, loss = 5.54349103
Iteration 5, loss = 5.48239307
Iteration 6, loss = 5.42909250
Iteration 7, loss = 5.38130633
Iteration 8, loss = 5.33701612
Iteration 9, loss = 5.29680782
Iteration 10, loss = 5.25889363
Iteration 11, loss = 5.22395524
Iteration 12, loss = 5.19115417
Iteration 13, loss = 5.15918163
Iteration 14, loss = 5.12700309
Iteration 15, loss = 5.09845109
Iteration 16, loss = 5.06703874
Iteration 17, loss = 5.04215866
Iteration 18, loss = 5.01552739
Iteration 19, loss = 4.98773471
Iteration 20, loss = 4.95874939
Iteration 21, loss = 4.93075074
Iteration 22, loss = 4.90477309
Iteration 23, loss = 4.87769221
Iteration 24, loss = 4.85231330
Iteration 25, loss = 4.82298527
Iteration 26, loss = 4.79821348
Iteration 27, loss = 4.77189553
Iteration 28, loss = 4.74341753
Iteration 29, loss = 4.71774711
Iteration 30, loss = 4.69089747
Iteration 31, loss = 4.66485160
Iteration 32, loss = 4.63998233
Iteration 33, loss = 4.61587517
Iteration 34, loss = 4.58730083
Iteration 35, loss = 4.56410402
Iteration 36, loss = 4.53641114
Iteration 37, loss = 4.51122432
Iteration 38, loss = 4.48779552
Iteration 39, loss = 4.46484842
Iteration 40, loss = 4.44015789
Iteration 41, loss = 4.41541020
Iteration 42, loss = 4.39289093
Iteration 43, loss = 4.36867687
Iteration 44, loss = 4.34702799
Iteration 45, loss = 4.32241820
Iteration 46, loss = 4.29959505
Iteration 47, loss = 4.27831973
Iteration 48, loss = 4.25662073
Iteration 49, loss = 4.23290504
Iteration 50, loss = 4.21173508
Iteration 51, loss = 4.19140282
Iteration 52, loss = 4.16960794
Iteration 53, loss = 4.14928421
Iteration 54, loss = 4.12819159
Iteration 55, loss = 4.10548769
Iteration 56, loss = 4.08610947
Iteration 57, loss = 4.06578471
Iteration 58, loss = 4.04716867
Iteration 59, loss = 4.02524338
Iteration 60, loss = 4.00599877
Iteration 61, loss = 3.98868363
Iteration 62, loss = 3.96793281
Iteration 63, loss = 3.94709154
Iteration 64, loss = 3.92849967
Iteration 65, loss = 3.91061868
Iteration 66, loss = 3.89314907
Iteration 67, loss = 3.87462752
Iteration 68, loss = 3.85540135
Iteration 69, loss = 3.83671282
Iteration 70, loss = 3.81791529
Iteration 71, loss = 3.80059097
Iteration 72, loss = 3.78233569
Iteration 73, loss = 3.76760583
Iteration 74, loss = 3.75003513
Iteration 75, loss = 3.73305998
Iteration 76, loss = 3.71424463
Iteration 77, loss = 3.69831612
Iteration 78, loss = 3.68191822
Iteration 79, loss = 3.66353596
Iteration 80, loss = 3.64702598
Iteration 81, loss = 3.63099704
Iteration 82, loss = 3.61370229
Iteration 83, loss = 3.59899572
Iteration 84, loss = 3.58209863
Iteration 85, loss = 3.56823323
Iteration 86, loss = 3.55103008
Iteration 87, loss = 3.53491194
Iteration 88, loss = 3.51811458
Iteration 89, loss = 3.50371790
Iteration 90, loss = 3.48779627
Iteration 91, loss = 3.47132846
Iteration 92, loss = 3.45556463
Iteration 93, loss = 3.43955105
Iteration 94, loss = 3.42599617
Iteration 95, loss = 3.41273236
Iteration 96, loss = 3.39655008
Iteration 97, loss = 3.38296417
Iteration 98, loss = 3.36866118
Iteration 99, loss = 3.35157141
Iteration 100, loss = 3.33929549
Iteration 101, loss = 3.32427447
Iteration 102, loss = 3.30997731
Iteration 103, loss = 3.29734365
Iteration 104, loss = 3.28257074
Iteration 105, loss = 3.26823261
Iteration 106, loss = 3.25587817
Iteration 107, loss = 3.23920151
Iteration 108, loss = 3.22707319
Iteration 109, loss = 3.21309049
Iteration 110, loss = 3.19952556
Iteration 111, loss = 3.18724841
Iteration 112, loss = 3.17342802
Iteration 113, loss = 3.16191282
Iteration 114, loss = 3.14589380
Iteration 115, loss = 3.13262227
Iteration 116, loss = 3.12116800
Iteration 117, loss = 3.10677187
Iteration 118, loss = 3.09450555
Iteration 119, loss = 3.08198698
Iteration 120, loss = 3.07040988
Iteration 121, loss = 3.05563815
Iteration 122, loss = 3.04352781
Iteration 123, loss = 3.03193799
Iteration 124, loss = 3.02087032
Iteration 125, loss = 3.01011959
Iteration 126, loss = 2.99599401
Iteration 127, loss = 2.98380046
Iteration 128, loss = 2.97162415
Iteration 129, loss = 2.96073679
Iteration 130, loss = 2.94931748
Iteration 131, loss = 2.93481169
Iteration 132, loss = 2.92529905
Iteration 133, loss = 2.91207177
Iteration 134, loss = 2.89861651
Iteration 135, loss = 2.88954874
Iteration 136, loss = 2.88057425
Iteration 137, loss = 2.86809946
Iteration 138, loss = 2.85584073
Iteration 139, loss = 2.84578108
Iteration 140, loss = 2.83491283
Iteration 141, loss = 2.82185409
Iteration 142, loss = 2.81081101
Iteration 143, loss = 2.79773367
Iteration 144, loss = 2.78883291
Iteration 145, loss = 2.77699117
Iteration 146, loss = 2.76699812
Iteration 147, loss = 2.75578578
Iteration 148, loss = 2.74500131
Iteration 149, loss = 2.73676451
Iteration 150, loss = 2.72355951
Iteration 151, loss = 2.71466229
Iteration 152, loss = 2.70272971
Iteration 153, loss = 2.69271548
Iteration 154, loss = 2.68228542
Iteration 155, loss = 2.67319696
Iteration 156, loss = 2.66257619
Iteration 157, loss = 2.65376017
Iteration 158, loss = 2.64043405
Iteration 159, loss = 2.63060409
Iteration 160, loss = 2.62190286
Iteration 161, loss = 2.61195097
Iteration 162, loss = 2.60349703
Iteration 163, loss = 2.59254186
Iteration 164, loss = 2.58381290
Iteration 165, loss = 2.57654708
Iteration 166, loss = 2.56241365
Iteration 167, loss = 2.55261757
Iteration 168, loss = 2.54543253
Iteration 169, loss = 2.53531292
Iteration 170, loss = 2.52531451
Iteration 171, loss = 2.51709528
Iteration 172, loss = 2.50483979
Iteration 173, loss = 2.49795066
Iteration 174, loss = 2.48661024
Iteration 175, loss = 2.47793867
Iteration 176, loss = 2.46949613
Iteration 177, loss = 2.45996631
Iteration 178, loss = 2.45230675
Iteration 179, loss = 2.44251858
Iteration 180, loss = 2.43490039
Iteration 181, loss = 2.42503748
Iteration 182, loss = 2.41676431
Iteration 183, loss = 2.40630954
Iteration 184, loss = 2.39648877
Iteration 185, loss = 2.38833833
Iteration 186, loss = 2.38084959
Iteration 187, loss = 2.37220800
Iteration 188, loss = 2.36574707
Iteration 189, loss = 2.35521323
Iteration 190, loss = 2.34881555
Iteration 191, loss = 2.33753412
Iteration 192, loss = 2.32938860
Iteration 193, loss = 2.32017500
Iteration 194, loss = 2.31261870
Iteration 195, loss = 2.30343471
Iteration 196, loss = 2.29699034
Iteration 197, loss = 2.28650922
Iteration 198, loss = 2.27979299
Iteration 199, loss = 2.27222740
Iteration 200, loss = 2.26201323
Iteration 201, loss = 2.25630332
Iteration 202, loss = 2.24826096
Iteration 203, loss = 2.24248650
Iteration 204, loss = 2.23156098
Iteration 205, loss = 2.22377623
Iteration 206, loss = 2.21603859
Iteration 207, loss = 2.20986898
Iteration 208, loss = 2.20094530
Iteration 209, loss = 2.19227792
Iteration 210, loss = 2.18519342
Iteration 211, loss = 2.17687327
Iteration 212, loss = 2.17048359
Iteration 213, loss = 2.16248991
Iteration 214, loss = 2.15329617
Iteration 215, loss = 2.14441439
Iteration 216, loss = 2.13806072
Iteration 217, loss = 2.13445004
Iteration 218, loss = 2.12320009
Iteration 219, loss = 2.11696985
Iteration 220, loss = 2.10990568
Iteration 221, loss = 2.10070384
Iteration 222, loss = 2.09429476
Iteration 223, loss = 2.08901662
Iteration 224, loss = 2.07925609
Iteration 225, loss = 2.07154872
Iteration 226, loss = 2.06569697
Iteration 227, loss = 2.05997012
Iteration 228, loss = 2.05396134
Iteration 229, loss = 2.04649853
Iteration 230, loss = 2.03859737
Iteration 231, loss = 2.03121388
Iteration 232, loss = 2.02359656
Iteration 233, loss = 2.01533501
Iteration 234, loss = 2.00969157
Iteration 235, loss = 2.00467786
Iteration 236, loss = 1.99769819
Iteration 237, loss = 1.98777076
Iteration 238, loss = 1.98163459
Iteration 239, loss = 1.97477099
Iteration 240, loss = 1.96734520
Iteration 241, loss = 1.96504460
Iteration 242, loss = 1.95498926
Iteration 243, loss = 1.94808986
Iteration 244, loss = 1.94164631
Iteration 245, loss = 1.93341363
Iteration 246, loss = 1.93012458
Iteration 247, loss = 1.92238511
Iteration 248, loss = 1.91689235
Iteration 249, loss = 1.91085251
Iteration 250, loss = 1.90652426
Iteration 251, loss = 1.89585861
Iteration 252, loss = 1.89041332
Iteration 253, loss = 1.88557920
Iteration 254, loss = 1.87775245
Iteration 255, loss = 1.87189659
Iteration 256, loss = 1.86246216
Iteration 257, loss = 1.85741778
Iteration 258, loss = 1.85037857
Iteration 259, loss = 1.84534369
Iteration 260, loss = 1.84100508
Iteration 261, loss = 1.83255387
Iteration 262, loss = 1.82712765
Iteration 263, loss = 1.82082564
Iteration 264, loss = 1.81420866
Iteration 265, loss = 1.80692972
Iteration 266, loss = 1.80369317
Iteration 267, loss = 1.79765174
Iteration 268, loss = 1.78989999
Iteration 269, loss = 1.78588481
Iteration 270, loss = 1.77799341
Iteration 271, loss = 1.77400499
Iteration 272, loss = 1.76588667
Iteration 273, loss = 1.75795599
Iteration 274, loss = 1.75346348
Iteration 275, loss = 1.75064853
Iteration 276, loss = 1.74141015
Iteration 277, loss = 1.73723591
Iteration 278, loss = 1.73079012
Iteration 279, loss = 1.72412816
Iteration 280, loss = 1.71934195
Iteration 281, loss = 1.71534825
Iteration 282, loss = 1.70854663
Iteration 283, loss = 1.70293525
Iteration 284, loss = 1.69748133
Iteration 285, loss = 1.69160002
Iteration 286, loss = 1.68452387
Iteration 287, loss = 1.68030881
Iteration 288, loss = 1.67479442
Iteration 289, loss = 1.67038823
Iteration 290, loss = 1.66320837
Iteration 291, loss = 1.65757773
Iteration 292, loss = 1.65154202
Iteration 293, loss = 1.64542088
Iteration 294, loss = 1.63937421
Iteration 295, loss = 1.63671086
Iteration 296, loss = 1.62924732
Iteration 297, loss = 1.62449921
Iteration 298, loss = 1.61975138
Iteration 299, loss = 1.61506709
Iteration 300, loss = 1.61041892
Iteration 301, loss = 1.60320459
Iteration 302, loss = 1.59810276
Iteration 303, loss = 1.59211317
Iteration 304, loss = 1.58683977
Iteration 305, loss = 1.58340475
Iteration 306, loss = 1.57582764
Iteration 307, loss = 1.57330143
Iteration 308, loss = 1.56626063
Iteration 309, loss = 1.56104123
Iteration 310, loss = 1.55757369
Iteration 311, loss = 1.55008477
Iteration 312, loss = 1.54521107
Iteration 313, loss = 1.54088839
Iteration 314, loss = 1.53657674
Iteration 315, loss = 1.53250843
Iteration 316, loss = 1.52685548
Iteration 317, loss = 1.52248719
Iteration 318, loss = 1.51678901
Iteration 319, loss = 1.51156435
Iteration 320, loss = 1.50597056
Iteration 321, loss = 1.50129700
Iteration 322, loss = 1.49552765
Iteration 323, loss = 1.49127207
Iteration 324, loss = 1.48484296
Iteration 325, loss = 1.48210124
Iteration 326, loss = 1.47744463
Iteration 327, loss = 1.47175946
Iteration 328, loss = 1.46936601
Iteration 329, loss = 1.46383831
Iteration 330, loss = 1.45839540
Iteration 331, loss = 1.45264644
Iteration 332, loss = 1.44773248
Iteration 333, loss = 1.44529308
Iteration 334, loss = 1.43836266
Iteration 335, loss = 1.43566836
Iteration 336, loss = 1.43095257
Iteration 337, loss = 1.42517596
Iteration 338, loss = 1.42001751
Iteration 339, loss = 1.41704720
Iteration 340, loss = 1.41236410
Iteration 341, loss = 1.40883252
Iteration 342, loss = 1.40400798
Iteration 343, loss = 1.39863315
Iteration 344, loss = 1.39129297
Iteration 345, loss = 1.39079457
Iteration 346, loss = 1.38369929
Iteration 347, loss = 1.37969288
Iteration 348, loss = 1.37629602
Iteration 349, loss = 1.37232577
Iteration 350, loss = 1.36629292
Iteration 351, loss = 1.36216320
Iteration 352, loss = 1.35849996
Iteration 353, loss = 1.35224163
Iteration 354, loss = 1.35001630
Iteration 355, loss = 1.34569061
Iteration 356, loss = 1.34091896
Iteration 357, loss = 1.33708207
Iteration 358, loss = 1.33186212
Iteration 359, loss = 1.32849421
Iteration 360, loss = 1.32201584
Iteration 361, loss = 1.31952704
Iteration 362, loss = 1.31489469
Iteration 363, loss = 1.31091839
Iteration 364, loss = 1.30508623
Iteration 365, loss = 1.30279911
Iteration 366, loss = 1.29777583
Iteration 367, loss = 1.29450619
Iteration 368, loss = 1.28905269
Iteration 369, loss = 1.28492343
Iteration 370, loss = 1.28150253
Iteration 371, loss = 1.27458636
Iteration 372, loss = 1.27025140
Iteration 373, loss = 1.26899498
Iteration 374, loss = 1.26530304
Iteration 375, loss = 1.26057750
Iteration 376, loss = 1.25772594
Iteration 377, loss = 1.25189445
Iteration 378, loss = 1.24791869
Iteration 379, loss = 1.24526891
Iteration 380, loss = 1.24121733
Iteration 381, loss = 1.23840557
Iteration 382, loss = 1.23073820
Iteration 383, loss = 1.22763151
Iteration 384, loss = 1.22522541
Iteration 385, loss = 1.22240914
Iteration 386, loss = 1.21920609
Iteration 387, loss = 1.21467087
Iteration 388, loss = 1.20725894
Iteration 389, loss = 1.20443404
Iteration 390, loss = 1.20223669
Iteration 391, loss = 1.19940382
Iteration 392, loss = 1.19473131
Iteration 393, loss = 1.19191338
Iteration 394, loss = 1.18514458
Iteration 395, loss = 1.18334572
Iteration 396, loss = 1.17911530
Iteration 397, loss = 1.17405285
Iteration 398, loss = 1.17118363
Iteration 399, loss = 1.16685958
Iteration 400, loss = 1.16457982
Iteration 401, loss = 1.15915197
Iteration 402, loss = 1.15576826
Iteration 403, loss = 1.15430890
Iteration 404, loss = 1.14958089
Iteration 405, loss = 1.14609973
Iteration 406, loss = 1.14052113
Iteration 407, loss = 1.13726685
Iteration 408, loss = 1.13572357
Iteration 409, loss = 1.13110965
Iteration 410, loss = 1.12781441
Iteration 411, loss = 1.12612109
Iteration 412, loss = 1.12008774
Iteration 413, loss = 1.11532080
Iteration 414, loss = 1.11241051
Iteration 415, loss = 1.10950361
Iteration 416, loss = 1.10596254
Iteration 417, loss = 1.10157379
Iteration 418, loss = 1.09916081
Iteration 419, loss = 1.09475168
Iteration 420, loss = 1.09082142
Iteration 421, loss = 1.08763050
Iteration 422, loss = 1.08517422
Iteration 423, loss = 1.08056566
Iteration 424, loss = 1.07673599
Iteration 425, loss = 1.07553556
Iteration 426, loss = 1.07013977
Iteration 427, loss = 1.06807304
Iteration 428, loss = 1.06391833
Iteration 429, loss = 1.06240959
Iteration 430, loss = 1.05824526
Iteration 431, loss = 1.05472002
Iteration 432, loss = 1.05129771
Iteration 433, loss = 1.04775808
Iteration 434, loss = 1.04197477
Iteration 435, loss = 1.04096863
Iteration 436, loss = 1.03641187
Iteration 437, loss = 1.03506016
Iteration 438, loss = 1.02978911
Iteration 439, loss = 1.02633072
Iteration 440, loss = 1.02360834
Iteration 441, loss = 1.02261734
Iteration 442, loss = 1.01955822
Iteration 443, loss = 1.01315385
Iteration 444, loss = 1.00956347
Iteration 445, loss = 1.00681873
Iteration 446, loss = 1.00466076
Iteration 447, loss = 0.99984463
Iteration 448, loss = 0.99795403
Iteration 449, loss = 0.99390793
Iteration 450, loss = 0.99223134
Iteration 451, loss = 0.98566273
Iteration 452, loss = 0.98538316
Iteration 453, loss = 0.98438454
Iteration 454, loss = 0.98135569
Iteration 455, loss = 0.97586143
Iteration 456, loss = 0.97260437
Iteration 457, loss = 0.96982421
Iteration 458, loss = 0.96677685
Iteration 459, loss = 0.96410985
Iteration 460, loss = 0.96005458
Iteration 461, loss = 0.95733736
Iteration 462, loss = 0.95550435
Iteration 463, loss = 0.95120104
Iteration 464, loss = 0.94873068
Iteration 465, loss = 0.94592815
Iteration 466, loss = 0.94327493
Iteration 467, loss = 0.94040570
Iteration 468, loss = 0.93676990
Iteration 469, loss = 0.93171021
Iteration 470, loss = 0.93015639
Iteration 471, loss = 0.92627624
Iteration 472, loss = 0.92316804
Iteration 473, loss = 0.92082380
Iteration 474, loss = 0.91835357
Iteration 475, loss = 0.91611096
Iteration 476, loss = 0.91413306
Iteration 477, loss = 0.91013090
Iteration 478, loss = 0.90696257
Iteration 479, loss = 0.90341604
Iteration 480, loss = 0.90120741
Iteration 481, loss = 0.89940534
Iteration 482, loss = 0.89491644
Iteration 483, loss = 0.89340955
Iteration 484, loss = 0.88995708
Iteration 485, loss = 0.88782589
Iteration 486, loss = 0.88425486
Iteration 487, loss = 0.87917687
Iteration 488, loss = 0.87761970
Iteration 489, loss = 0.87407847
Iteration 490, loss = 0.87516272
Iteration 491, loss = 0.86974950
Iteration 492, loss = 0.86724415
Iteration 493, loss = 0.86432609
Iteration 494, loss = 0.86273405
Iteration 495, loss = 0.86071475
Iteration 496, loss = 0.85493186
Iteration 497, loss = 0.85325231
Iteration 498, loss = 0.85096337
Iteration 499, loss = 0.84761869
Iteration 500, loss = 0.84293159
Iteration 501, loss = 0.84081927
Iteration 502, loss = 0.84009743
Iteration 503, loss = 0.83539027
Iteration 504, loss = 0.83359291
Iteration 505, loss = 0.83221305
Iteration 506, loss = 0.82879971
Iteration 507, loss = 0.82572536
Iteration 508, loss = 0.82474127
Iteration 509, loss = 0.82134148
Iteration 510, loss = 0.81677088
Iteration 511, loss = 0.81417295
Iteration 512, loss = 0.81222966
Iteration 513, loss = 0.81084642
Iteration 514, loss = 0.80818018
Iteration 515, loss = 0.80582945
Iteration 516, loss = 0.80223744
Iteration 517, loss = 0.79939860
Iteration 518, loss = 0.79646292
Iteration 519, loss = 0.79374505
Iteration 520, loss = 0.79352061
Iteration 521, loss = 0.79163608
Iteration 522, loss = 0.78561118
Iteration 523, loss = 0.78479547
Iteration 524, loss = 0.78230252
Iteration 525, loss = 0.77808051
Iteration 526, loss = 0.77680400
Iteration 527, loss = 0.77516398
Iteration 528, loss = 0.77273032
Iteration 529, loss = 0.76865289
Iteration 530, loss = 0.76677690
Iteration 531, loss = 0.76459188
Iteration 532, loss = 0.76269214
Iteration 533, loss = 0.76133739
Iteration 534, loss = 0.75836613
Iteration 535, loss = 0.75371185
Iteration 536, loss = 0.75289084
Iteration 537, loss = 0.75073694
Iteration 538, loss = 0.74907838
Iteration 539, loss = 0.74516191
Iteration 540, loss = 0.74197131
Iteration 541, loss = 0.74073803
Iteration 542, loss = 0.73853443
Iteration 543, loss = 0.73533225
Iteration 544, loss = 0.73351593
Iteration 545, loss = 0.72925560
Iteration 546, loss = 0.72908776
Iteration 547, loss = 0.72779777
Iteration 548, loss = 0.72470039
Iteration 549, loss = 0.72119804
Iteration 550, loss = 0.71916142
Iteration 551, loss = 0.71695608
Iteration 552, loss = 0.71474249
Iteration 553, loss = 0.71132422
Iteration 554, loss = 0.70936145
Iteration 555, loss = 0.70862718
Iteration 556, loss = 0.70580685
Iteration 557, loss = 0.70376946
Iteration 558, loss = 0.70160401
Iteration 559, loss = 0.69818053
Iteration 560, loss = 0.69649126
Iteration 561, loss = 0.69305101
Iteration 562, loss = 0.69214918
Iteration 563, loss = 0.68872059
Iteration 564, loss = 0.68701442
Iteration 565, loss = 0.68566083
Iteration 566, loss = 0.68271399
Iteration 567, loss = 0.68179496
Iteration 568, loss = 0.67946151
Iteration 569, loss = 0.67611213
Iteration 570, loss = 0.67305138
Iteration 571, loss = 0.67265971
Iteration 572, loss = 0.67001597
Iteration 573, loss = 0.66581675
Iteration 574, loss = 0.66349566
Iteration 575, loss = 0.66415431
Iteration 576, loss = 0.66175021
Iteration 577, loss = 0.65928875
Iteration 578, loss = 0.65657855
Iteration 579, loss = 0.65371062
Iteration 580, loss = 0.65361799
Iteration 581, loss = 0.65101742
Iteration 582, loss = 0.64949740
Iteration 583, loss = 0.64576035
Iteration 584, loss = 0.64436994
Iteration 585, loss = 0.64194259
Iteration 586, loss = 0.64149107
Iteration 587, loss = 0.63807403
Iteration 588, loss = 0.63489792
Iteration 589, loss = 0.63407587
Iteration 590, loss = 0.63017349
Iteration 591, loss = 0.62842169
Iteration 592, loss = 0.62787895
Iteration 593, loss = 0.62699274
Iteration 594, loss = 0.62158437
Iteration 595, loss = 0.62115742
Iteration 596, loss = 0.62164465
Iteration 597, loss = 0.61743945
Iteration 598, loss = 0.61547519
Iteration 599, loss = 0.61393834
Iteration 600, loss = 0.61171616
Iteration 601, loss = 0.61078469
Iteration 602, loss = 0.60777125
Iteration 603, loss = 0.60594854
Iteration 604, loss = 0.60435719
Iteration 605, loss = 0.60156282
Iteration 606, loss = 0.59913641
Iteration 607, loss = 0.59950959
Iteration 608, loss = 0.59658490
Iteration 609, loss = 0.59429302
Iteration 610, loss = 0.59227701
Iteration 611, loss = 0.59152313
Iteration 612, loss = 0.58717747
Iteration 613, loss = 0.58553185
Iteration 614, loss = 0.58435311
Iteration 615, loss = 0.58175199
Iteration 616, loss = 0.58023447
Iteration 617, loss = 0.57752348
Iteration 618, loss = 0.57658211
Iteration 619, loss = 0.57751863
Iteration 620, loss = 0.57275843
Iteration 621, loss = 0.57146575
Iteration 622, loss = 0.56993796
Iteration 623, loss = 0.56906049
Iteration 624, loss = 0.56702591
Iteration 625, loss = 0.56308119
Iteration 626, loss = 0.56075984
Iteration 627, loss = 0.55976856
Iteration 628, loss = 0.55769203
Iteration 629, loss = 0.55835449
Iteration 630, loss = 0.55474945
Iteration 631, loss = 0.55372920
Iteration 632, loss = 0.55178119
Iteration 633, loss = 0.55015361
Iteration 634, loss = 0.54735116
Iteration 635, loss = 0.54560626
Iteration 636, loss = 0.54386631
Iteration 637, loss = 0.54280348
Iteration 638, loss = 0.53931632
Iteration 639, loss = 0.54067761
Iteration 640, loss = 0.53536331
Iteration 641, loss = 0.53407647
Iteration 642, loss = 0.53251524
Iteration 643, loss = 0.53192793
Iteration 644, loss = 0.52959307
Iteration 645, loss = 0.52840962
Iteration 646, loss = 0.52491520
Iteration 647, loss = 0.52579359
Iteration 648, loss = 0.52291624
Iteration 649, loss = 0.52179463
Iteration 650, loss = 0.51927796
Iteration 651, loss = 0.51763021
Iteration 652, loss = 0.51633827
Iteration 653, loss = 0.51408921
Iteration 654, loss = 0.51400315
Iteration 655, loss = 0.51274162
Iteration 656, loss = 0.50998873
Iteration 657, loss = 0.50608077
Iteration 658, loss = 0.50633586
Iteration 659, loss = 0.50594939
Iteration 660, loss = 0.50365228
Iteration 661, loss = 0.50090796
Iteration 662, loss = 0.49970714
Iteration 663, loss = 0.49735367
Iteration 664, loss = 0.49633341
Iteration 665, loss = 0.49485543
Iteration 666, loss = 0.49187622
Iteration 667, loss = 0.49179933
Iteration 668, loss = 0.48968978
Iteration 669, loss = 0.48959618
Iteration 670, loss = 0.48706949
Iteration 671, loss = 0.48532484
Iteration 672, loss = 0.48497327
Iteration 673, loss = 0.48170880
Iteration 674, loss = 0.48005293
Iteration 675, loss = 0.47928668
Iteration 676, loss = 0.47721902
Iteration 677, loss = 0.47568003
Iteration 678, loss = 0.47378965
Iteration 679, loss = 0.47537473
Iteration 680, loss = 0.47126253
Iteration 681, loss = 0.47037218
Iteration 682, loss = 0.46878549
Iteration 683, loss = 0.46809276
Iteration 684, loss = 0.46389856
Iteration 685, loss = 0.46302425
Iteration 686, loss = 0.46288667
Iteration 687, loss = 0.46006463
Iteration 688, loss = 0.45909522
Iteration 689, loss = 0.45772712
Iteration 690, loss = 0.45674970
Iteration 691, loss = 0.45521355
Iteration 692, loss = 0.45330261
Iteration 693, loss = 0.45198418
Iteration 694, loss = 0.45115505
Iteration 695, loss = 0.44683970
Iteration 696, loss = 0.44676490
Iteration 697, loss = 0.44521969
Iteration 698, loss = 0.44212212
Iteration 699, loss = 0.44299590
Iteration 700, loss = 0.44071013
Iteration 701, loss = 0.44014316
Iteration 702, loss = 0.43704779
Iteration 703, loss = 0.43600959
Iteration 704, loss = 0.43416108
Iteration 705, loss = 0.43384053
Iteration 706, loss = 0.43305475
Iteration 707, loss = 0.43260353
Iteration 708, loss = 0.42967572
Iteration 709, loss = 0.42880499
Iteration 710, loss = 0.42698057
Iteration 711, loss = 0.42511056
Iteration 712, loss = 0.42429369
Iteration 713, loss = 0.42174710
Iteration 714, loss = 0.42016979
Iteration 715, loss = 0.41925061
Iteration 716, loss = 0.41707602
Iteration 717, loss = 0.41680136
Iteration 718, loss = 0.41628150
Iteration 719, loss = 0.41502783
Iteration 720, loss = 0.41380777
Iteration 721, loss = 0.41359794
Iteration 722, loss = 0.41106114
Iteration 723, loss = 0.40959177
Iteration 724, loss = 0.40781666
Iteration 725, loss = 0.40814845
Iteration 726, loss = 0.40460943
Iteration 727, loss = 0.40375813
Iteration 728, loss = 0.40389914
Iteration 729, loss = 0.40146678
Iteration 730, loss = 0.40125441
Iteration 731, loss = 0.39783782
Iteration 732, loss = 0.39825074
Iteration 733, loss = 0.39445444
Iteration 734, loss = 0.39425540
Iteration 735, loss = 0.39295775
Iteration 736, loss = 0.39223067
Iteration 737, loss = 0.39065971
Iteration 738, loss = 0.38975093
Iteration 739, loss = 0.38874599
Iteration 740, loss = 0.38697662
Iteration 741, loss = 0.38450661
Iteration 742, loss = 0.38468545
Iteration 743, loss = 0.38282345
Iteration 744, loss = 0.38139271
Iteration 745, loss = 0.37935765
Iteration 746, loss = 0.37820585
Iteration 747, loss = 0.37782517
Iteration 748, loss = 0.37636988
Iteration 749, loss = 0.37569008
Iteration 750, loss = 0.37398787
Iteration 751, loss = 0.37381410
Iteration 752, loss = 0.37264627
Iteration 753, loss = 0.37189103
Iteration 754, loss = 0.37057236
Iteration 755, loss = 0.36739814
Iteration 756, loss = 0.36653262
Iteration 757, loss = 0.36627221
Iteration 758, loss = 0.36602273
Iteration 759, loss = 0.36467643
Iteration 760, loss = 0.36256640
Iteration 761, loss = 0.36086058
Iteration 762, loss = 0.35910001
Iteration 763, loss = 0.35878265
Iteration 764, loss = 0.35651280
Iteration 765, loss = 0.35565200
Iteration 766, loss = 0.35530200
Iteration 767, loss = 0.35350181
Iteration 768, loss = 0.35143100
Iteration 769, loss = 0.35156956
Iteration 770, loss = 0.35154673
Iteration 771, loss = 0.35035219
Iteration 772, loss = 0.34805881
Iteration 773, loss = 0.34688121
Iteration 774, loss = 0.34618540
Iteration 775, loss = 0.34494287
Iteration 776, loss = 0.34280432
Iteration 777, loss = 0.34475397
Iteration 778, loss = 0.34044228
Iteration 779, loss = 0.34089457
Iteration 780, loss = 0.34048224
Iteration 781, loss = 0.33677750
Iteration 782, loss = 0.33701504
Iteration 783, loss = 0.33636571
Iteration 784, loss = 0.33477877
Iteration 785, loss = 0.33285957
Iteration 786, loss = 0.33341685
Iteration 787, loss = 0.33244564
Iteration 788, loss = 0.33125401
Iteration 789, loss = 0.32994669
Iteration 790, loss = 0.32962032
Iteration 791, loss = 0.32742216
Iteration 792, loss = 0.32531653
Iteration 793, loss = 0.32599167
Iteration 794, loss = 0.32402928
Iteration 795, loss = 0.32274919
Iteration 796, loss = 0.32050185
Iteration 797, loss = 0.32104747
Iteration 798, loss = 0.32093501
Iteration 799, loss = 0.31866855
Iteration 800, loss = 0.31785252
Iteration 801, loss = 0.31615896
Iteration 802, loss = 0.31753754
Iteration 803, loss = 0.31415238
Iteration 804, loss = 0.31526442
Iteration 805, loss = 0.31259704
Iteration 806, loss = 0.31165360
Iteration 807, loss = 0.31199988
Iteration 808, loss = 0.30997192
Iteration 809, loss = 0.30774965
Iteration 810, loss = 0.30785918
Iteration 811, loss = 0.30598737
Iteration 812, loss = 0.30598586
Iteration 813, loss = 0.30605494
Iteration 814, loss = 0.30364020
Iteration 815, loss = 0.30189327
Iteration 816, loss = 0.30076091
Iteration 817, loss = 0.30165531
Iteration 818, loss = 0.29886244
Iteration 819, loss = 0.29819437
Iteration 820, loss = 0.29925641
Iteration 821, loss = 0.29642168
Iteration 822, loss = 0.29608139
Iteration 823, loss = 0.29467439
Iteration 824, loss = 0.29371431
Iteration 825, loss = 0.29313045
Iteration 826, loss = 0.29208183
Iteration 827, loss = 0.29065050
Iteration 828, loss = 0.28969623
Iteration 829, loss = 0.28922432
Iteration 830, loss = 0.28766397
Iteration 831, loss = 0.28745021
Iteration 832, loss = 0.28661535
Iteration 833, loss = 0.28620700
Iteration 834, loss = 0.28502634
Iteration 835, loss = 0.28510241
Iteration 836, loss = 0.28209351
Iteration 837, loss = 0.28144333
Iteration 838, loss = 0.27995708
Iteration 839, loss = 0.27918786
Iteration 840, loss = 0.27862431
Iteration 841, loss = 0.27733572
Iteration 842, loss = 0.27656207
Iteration 843, loss = 0.27622686
Iteration 844, loss = 0.27553615
Iteration 845, loss = 0.27542148
Iteration 846, loss = 0.27353258
Iteration 847, loss = 0.27397120
Iteration 848, loss = 0.27256198
Iteration 849, loss = 0.27095599
Iteration 850, loss = 0.26987881
Iteration 851, loss = 0.27017291
Iteration 852, loss = 0.26837801
Iteration 853, loss = 0.26733964
Iteration 854, loss = 0.26630544
Iteration 855, loss = 0.26642635
Iteration 856, loss = 0.26554373
Iteration 857, loss = 0.26375764
Iteration 858, loss = 0.26239254
Iteration 859, loss = 0.26274688
Iteration 860, loss = 0.26255655
Iteration 861, loss = 0.26094559
Iteration 862, loss = 0.25907361
Iteration 863, loss = 0.25975097
Iteration 864, loss = 0.25896009
Iteration 865, loss = 0.25728566
Iteration 866, loss = 0.25723917
Iteration 867, loss = 0.25730856
Iteration 868, loss = 0.25505347
Iteration 869, loss = 0.25289159
Iteration 870, loss = 0.25482800
Iteration 871, loss = 0.25267099
Iteration 872, loss = 0.25040524
Iteration 873, loss = 0.24904303
Iteration 874, loss = 0.25120921
Iteration 875, loss = 0.24943469
Iteration 876, loss = 0.24786012
Iteration 877, loss = 0.24708901
Iteration 878, loss = 0.24612509
Iteration 879, loss = 0.24699425
Iteration 880, loss = 0.24450317
Iteration 881, loss = 0.24540881
Iteration 882, loss = 0.24291965
Iteration 883, loss = 0.24203938
Iteration 884, loss = 0.24122507
Iteration 885, loss = 0.24078475
Iteration 886, loss = 0.24013475
Iteration 887, loss = 0.23891916
Iteration 888, loss = 0.23879948
Iteration 889, loss = 0.23979431
Iteration 890, loss = 0.23760888
Iteration 891, loss = 0.23624943
Iteration 892, loss = 0.23563691
Iteration 893, loss = 0.23445348
Iteration 894, loss = 0.23408983
Iteration 895, loss = 0.23281232
Iteration 896, loss = 0.23316842
Iteration 897, loss = 0.23098252
Iteration 898, loss = 0.22928266
Iteration 899, loss = 0.23085376
Iteration 900, loss = 0.22901527
Iteration 901, loss = 0.22916137
Iteration 902, loss = 0.22980654
Iteration 903, loss = 0.22701144
Iteration 904, loss = 0.22716886
Iteration 905, loss = 0.22685712
Iteration 906, loss = 0.22543553
Iteration 907, loss = 0.22487667
Iteration 908, loss = 0.22387764
Iteration 909, loss = 0.22457773
Iteration 910, loss = 0.22204764
Iteration 911, loss = 0.22099578
Iteration 912, loss = 0.22283886
Iteration 913, loss = 0.22203363
Iteration 914, loss = 0.22085264
Iteration 915, loss = 0.21980682
Iteration 916, loss = 0.22052288
Iteration 917, loss = 0.21858843
Iteration 918, loss = 0.21740681
Iteration 919, loss = 0.21551819
Iteration 920, loss = 0.21695574
Iteration 921, loss = 0.21388710
Iteration 922, loss = 0.21440995
Iteration 923, loss = 0.21327368
Iteration 924, loss = 0.21191505
Iteration 925, loss = 0.21269624
Iteration 926, loss = 0.21180251
Iteration 927, loss = 0.21117968
Iteration 928, loss = 0.21275393
Iteration 929, loss = 0.20939208
Iteration 930, loss = 0.20897152
Iteration 931, loss = 0.20723548
Iteration 932, loss = 0.20741279
Iteration 933, loss = 0.20835364
Iteration 934, loss = 0.20628261
Iteration 935, loss = 0.20458012
Iteration 936, loss = 0.20531168
Iteration 937, loss = 0.20533659
Iteration 938, loss = 0.20497047
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100), {'error': 0.84913004434664685, 'fit': 0.98875000000000002, 'time': 1788.630000000001})
Iteration 1, loss = 5.99847179
Iteration 2, loss = 5.70945378
Iteration 3, loss = 5.64042010
Iteration 4, loss = 5.59967138
Iteration 5, loss = 5.56120673
Iteration 6, loss = 5.51542067
Iteration 7, loss = 5.46635990
Iteration 8, loss = 5.41393693
Iteration 9, loss = 5.36960733
Iteration 10, loss = 5.33906150
Iteration 11, loss = 5.29257871
Iteration 12, loss = 5.25142421
Iteration 13, loss = 5.23376963
Iteration 14, loss = 5.18452991
Iteration 15, loss = 5.17527814
Iteration 16, loss = 5.15259942
Iteration 17, loss = 5.10239530
Iteration 18, loss = 5.06557221
Iteration 19, loss = 5.02610300
Iteration 20, loss = 4.97517657
Iteration 21, loss = 4.92611726
Iteration 22, loss = 4.87724787
Iteration 23, loss = 4.82609643
Iteration 24, loss = 4.75746102
Iteration 25, loss = 4.70788373
Iteration 26, loss = 4.63243481
Iteration 27, loss = 4.55608815
Iteration 28, loss = 4.47573041
Iteration 29, loss = 4.41607754
Iteration 30, loss = 4.34259949
Iteration 31, loss = 4.27460294
Iteration 32, loss = 4.19065434
Iteration 33, loss = 4.10677182
Iteration 34, loss = 4.03675385
Iteration 35, loss = 3.97342597
Iteration 36, loss = 3.86808331
Iteration 37, loss = 3.79464316
Iteration 38, loss = 3.71675067
Iteration 39, loss = 3.63043568
Iteration 40, loss = 3.56830525
Iteration 41, loss = 3.48891883
Iteration 42, loss = 3.41098845
Iteration 43, loss = 3.32077120
Iteration 44, loss = 3.27506710
Iteration 45, loss = 3.17185560
Iteration 46, loss = 3.12855633
Iteration 47, loss = 3.02944471
Iteration 48, loss = 2.98076847
Iteration 49, loss = 2.88472484
Iteration 50, loss = 2.81656697
Iteration 51, loss = 2.74796643
Iteration 52, loss = 2.66041867
Iteration 53, loss = 2.60361898
Iteration 54, loss = 2.54372875
Iteration 55, loss = 2.47843606
Iteration 56, loss = 2.39946637
Iteration 57, loss = 2.34880575
Iteration 58, loss = 2.26948546
Iteration 59, loss = 2.21984094
Iteration 60, loss = 2.14927554
Iteration 61, loss = 2.09079538
Iteration 62, loss = 2.01952166
Iteration 63, loss = 1.95918231
Iteration 64, loss = 1.90933141
Iteration 65, loss = 1.86055829
Iteration 66, loss = 1.79594651
Iteration 67, loss = 1.72208527
Iteration 68, loss = 1.68552446
Iteration 69, loss = 1.62573262
Iteration 70, loss = 1.57010864
Iteration 71, loss = 1.53137325
Iteration 72, loss = 1.47838523
Iteration 73, loss = 1.43857149
Iteration 74, loss = 1.37222947
Iteration 75, loss = 1.33478286
Iteration 76, loss = 1.28461822
Iteration 77, loss = 1.24491058
Iteration 78, loss = 1.20014965
Iteration 79, loss = 1.15439284
Iteration 80, loss = 1.12457163
Iteration 81, loss = 1.08439490
Iteration 82, loss = 1.05389857
Iteration 83, loss = 1.02106413
Iteration 84, loss = 0.98700691
Iteration 85, loss = 0.93409824
Iteration 86, loss = 0.89961217
Iteration 87, loss = 0.87494137
Iteration 88, loss = 0.84928574
Iteration 89, loss = 0.80340119
Iteration 90, loss = 0.78156411
Iteration 91, loss = 0.75470474
Iteration 92, loss = 0.73377193
Iteration 93, loss = 0.71953771
Iteration 94, loss = 0.68701651
Iteration 95, loss = 0.65964191
Iteration 96, loss = 0.63712161
Iteration 97, loss = 0.61474189
Iteration 98, loss = 0.59745413
Iteration 99, loss = 0.57530290
Iteration 100, loss = 0.55131070
Iteration 101, loss = 0.53468625
Iteration 102, loss = 0.51525727
Iteration 103, loss = 0.51002180
Iteration 104, loss = 0.48604377
Iteration 105, loss = 0.46655352
Iteration 106, loss = 0.44776769
Iteration 107, loss = 0.43655339
Iteration 108, loss = 0.41276325
Iteration 109, loss = 0.41228894
Iteration 110, loss = 0.39929689
Iteration 111, loss = 0.39066335
Iteration 112, loss = 0.37638074
Iteration 113, loss = 0.37298041
Iteration 114, loss = 0.37522401
Iteration 115, loss = 0.34502569
Iteration 116, loss = 0.33413394
Iteration 117, loss = 0.33209619
Iteration 118, loss = 0.31859721
Iteration 119, loss = 0.30377403
Iteration 120, loss = 0.30856500
Iteration 121, loss = 0.29596473
Iteration 122, loss = 0.27737857
Iteration 123, loss = 0.27637760
Iteration 124, loss = 0.27535911
Iteration 125, loss = 0.26741340
Iteration 126, loss = 0.25911437
Iteration 127, loss = 0.24437411
Iteration 128, loss = 0.24494611
Iteration 129, loss = 0.23760957
Iteration 130, loss = 0.23173251
Iteration 131, loss = 0.22382951
Iteration 132, loss = 0.21875292
Iteration 133, loss = 0.22151474
Iteration 134, loss = 0.22710466
Iteration 135, loss = 0.22216517
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((1000, 1000, 1000), {'error': 0.82210336640175596, 'fit': 0.97562499999999996, 'time': 2481.790000000001})
Iteration 1, loss = 6.54215434
Iteration 2, loss = 6.51442164
Iteration 3, loss = 6.48293073
Iteration 4, loss = 6.44844264
Iteration 5, loss = 6.41311270
Iteration 6, loss = 6.37854961
Iteration 7, loss = 6.34588023
Iteration 8, loss = 6.31557884
Iteration 9, loss = 6.28783970
Iteration 10, loss = 6.26230075
Iteration 11, loss = 6.23901234
Iteration 12, loss = 6.21787655
Iteration 13, loss = 6.19851820
Iteration 14, loss = 6.18069461
Iteration 15, loss = 6.16464938
Iteration 16, loss = 6.14983736
Iteration 17, loss = 6.13637648
Iteration 18, loss = 6.12404942
Iteration 19, loss = 6.11271117
Iteration 20, loss = 6.10240878
Iteration 21, loss = 6.09299825
Iteration 22, loss = 6.08431030
Iteration 23, loss = 6.07640853
Iteration 24, loss = 6.06915839
Iteration 25, loss = 6.06253322
Iteration 26, loss = 6.05652536
Iteration 27, loss = 6.05083491
Iteration 28, loss = 6.04552273
Iteration 29, loss = 6.04013469
Iteration 30, loss = 6.03364021
Iteration 31, loss = 6.02096047
Iteration 32, loss = 6.00311755
Iteration 33, loss = 5.99308162
Iteration 34, loss = 5.98434935
Iteration 35, loss = 5.97639952
Iteration 36, loss = 5.96899119
Iteration 37, loss = 5.96198389
Iteration 38, loss = 5.95540613
Iteration 39, loss = 5.94907328
Iteration 40, loss = 5.94307455
Iteration 41, loss = 5.93735709
Iteration 42, loss = 5.93181108
Iteration 43, loss = 5.92647165
Iteration 44, loss = 5.92149926
Iteration 45, loss = 5.91650941
Iteration 46, loss = 5.91177152
Iteration 47, loss = 5.90729381
Iteration 48, loss = 5.90282104
Iteration 49, loss = 5.89849919
Iteration 50, loss = 5.89435105
Iteration 51, loss = 5.89031776
Iteration 52, loss = 5.88643814
Iteration 53, loss = 5.88271222
Iteration 54, loss = 5.87905291
Iteration 55, loss = 5.87542791
Iteration 56, loss = 5.87197906
Iteration 57, loss = 5.86866246
Iteration 58, loss = 5.86531151
Iteration 59, loss = 5.86214250
Iteration 60, loss = 5.85899463
Iteration 61, loss = 5.85602947
Iteration 62, loss = 5.85307502
Iteration 63, loss = 5.85020001
Iteration 64, loss = 5.84736175
Iteration 65, loss = 5.84457990
Iteration 66, loss = 5.84186341
Iteration 67, loss = 5.83924587
Iteration 68, loss = 5.83667119
Iteration 69, loss = 5.83418993
Iteration 70, loss = 5.83165819
Iteration 71, loss = 5.82931044
Iteration 72, loss = 5.82695802
Iteration 73, loss = 5.82464430
Iteration 74, loss = 5.82247195
Iteration 75, loss = 5.82019539
Iteration 76, loss = 5.81807755
Iteration 77, loss = 5.81589481
Iteration 78, loss = 5.81381664
Iteration 79, loss = 5.81186440
Iteration 80, loss = 5.80981103
Iteration 81, loss = 5.80788708
Iteration 82, loss = 5.80602001
Iteration 83, loss = 5.80408824
Iteration 84, loss = 5.80233955
Iteration 85, loss = 5.80052154
Iteration 86, loss = 5.79873503
Iteration 87, loss = 5.79705395
Iteration 88, loss = 5.79537203
Iteration 89, loss = 5.79366190
Iteration 90, loss = 5.79210166
Iteration 91, loss = 5.79045333
Iteration 92, loss = 5.78902699
Iteration 93, loss = 5.78741290
Iteration 94, loss = 5.78587763
Iteration 95, loss = 5.78451583
Iteration 96, loss = 5.78305513
Iteration 97, loss = 5.78168974
Iteration 98, loss = 5.78032084
Iteration 99, loss = 5.77903157
Iteration 100, loss = 5.77774606
Iteration 101, loss = 5.77657632
Iteration 102, loss = 5.77529636
Iteration 103, loss = 5.77404850
Iteration 104, loss = 5.77296082
Iteration 105, loss = 5.77188234
Iteration 106, loss = 5.77068194
Iteration 107, loss = 5.76971030
Iteration 108, loss = 5.76856005
Iteration 109, loss = 5.76759001
Iteration 110, loss = 5.76651990
Iteration 111, loss = 5.76550333
Iteration 112, loss = 5.76458636
Iteration 113, loss = 5.76363060
Iteration 114, loss = 5.76270921
Iteration 115, loss = 5.76175881
Iteration 116, loss = 5.76093893
Iteration 117, loss = 5.76005809
Iteration 118, loss = 5.75929925
Iteration 119, loss = 5.75834321
Iteration 120, loss = 5.75752246
Iteration 121, loss = 5.75687984
Iteration 122, loss = 5.75611119
Iteration 123, loss = 5.75523353
Iteration 124, loss = 5.75444124
Iteration 125, loss = 5.75386413
Iteration 126, loss = 5.75300933
Iteration 127, loss = 5.75224344
Iteration 128, loss = 5.75157168
Iteration 129, loss = 5.75091450
Iteration 130, loss = 5.75020833
Iteration 131, loss = 5.74960964
Iteration 132, loss = 5.74895211
Iteration 133, loss = 5.74835543
Iteration 134, loss = 5.74771481
Iteration 135, loss = 5.74710726
Iteration 136, loss = 5.74666772
Iteration 137, loss = 5.74575582
Iteration 138, loss = 5.74516561
Iteration 139, loss = 5.74456720
Iteration 140, loss = 5.74421023
Iteration 141, loss = 5.74333122
Iteration 142, loss = 5.74279280
Iteration 143, loss = 5.74220027
Iteration 144, loss = 5.74171114
Iteration 145, loss = 5.74108875
Iteration 146, loss = 5.74057345
Iteration 147, loss = 5.73997311
Iteration 148, loss = 5.73946695
Iteration 149, loss = 5.73892907
Iteration 150, loss = 5.73836824
Iteration 151, loss = 5.73781994
Iteration 152, loss = 5.73737391
Iteration 153, loss = 5.73691596
Iteration 154, loss = 5.73622621
Iteration 155, loss = 5.73560906
Iteration 156, loss = 5.73522014
Iteration 157, loss = 5.73462879
Iteration 158, loss = 5.73409628
Iteration 159, loss = 5.73358483
Iteration 160, loss = 5.73301465
Iteration 161, loss = 5.73248323
Iteration 162, loss = 5.73189160
Iteration 163, loss = 5.73142922
Iteration 164, loss = 5.73087694
Iteration 165, loss = 5.73030120
Iteration 166, loss = 5.72975993
Iteration 167, loss = 5.72957209
Iteration 168, loss = 5.72884951
Iteration 169, loss = 5.72843002
Iteration 170, loss = 5.72778857
Iteration 171, loss = 5.72719618
Iteration 172, loss = 5.72663740
Iteration 173, loss = 5.72614030
Iteration 174, loss = 5.72559930
Iteration 175, loss = 5.72512113
Iteration 176, loss = 5.72462567
Iteration 177, loss = 5.72404673
Iteration 178, loss = 5.72353535
Iteration 179, loss = 5.72318509
Iteration 180, loss = 5.72270131
Iteration 181, loss = 5.72191313
Iteration 182, loss = 5.72156044
Iteration 183, loss = 5.72100846
Iteration 184, loss = 5.72037220
Iteration 185, loss = 5.71995014
Iteration 186, loss = 5.71950664
Iteration 187, loss = 5.71885863
Iteration 188, loss = 5.71839627
Iteration 189, loss = 5.71795274
Iteration 190, loss = 5.71734855
Iteration 191, loss = 5.71702638
Iteration 192, loss = 5.71645349
Iteration 193, loss = 5.71593808
Iteration 194, loss = 5.71566847
Iteration 195, loss = 5.71513704
Iteration 196, loss = 5.71467431
Iteration 197, loss = 5.71419256
Iteration 198, loss = 5.71370030
Iteration 199, loss = 5.71326674
Iteration 200, loss = 5.71271379
Iteration 201, loss = 5.71228987
Iteration 202, loss = 5.71199737
Iteration 203, loss = 5.71144121
Iteration 204, loss = 5.71113858
Iteration 205, loss = 5.71087110
Iteration 206, loss = 5.71041143
Iteration 207, loss = 5.71002363
Iteration 208, loss = 5.70953399
Iteration 209, loss = 5.70898240
Iteration 210, loss = 5.70876041
Iteration 211, loss = 5.70838827
Iteration 212, loss = 5.70799651
Iteration 213, loss = 5.70768003
Iteration 214, loss = 5.70721702
Iteration 215, loss = 5.70712132
Iteration 216, loss = 5.70680494
Iteration 217, loss = 5.70634066
Iteration 218, loss = 5.70582099
Iteration 219, loss = 5.70560248
Iteration 220, loss = 5.70521370
Iteration 221, loss = 5.70490501
Iteration 222, loss = 5.70449087
Iteration 223, loss = 5.70412956
Iteration 224, loss = 5.70405653
Iteration 225, loss = 5.70349479
Iteration 226, loss = 5.70338908
Iteration 227, loss = 5.70298521
Iteration 228, loss = 5.70267602
Iteration 229, loss = 5.70233221
Iteration 230, loss = 5.70190104
Iteration 231, loss = 5.70182766
Iteration 232, loss = 5.70166018
Iteration 233, loss = 5.70102476
Iteration 234, loss = 5.70116540
Iteration 235, loss = 5.70063454
Iteration 236, loss = 5.70057816
Iteration 237, loss = 5.70020177
Iteration 238, loss = 5.69998969
Iteration 239, loss = 5.69943384
Iteration 240, loss = 5.69933150
Iteration 241, loss = 5.69912964
Iteration 242, loss = 5.69901092
Iteration 243, loss = 5.69858415
Iteration 244, loss = 5.69827495
Iteration 245, loss = 5.69836286
Iteration 246, loss = 5.69791982
Iteration 247, loss = 5.69745900
Iteration 248, loss = 5.69750954
Iteration 249, loss = 5.69723962
Iteration 250, loss = 5.69687213
Iteration 251, loss = 5.69659660
Iteration 252, loss = 5.69624643
Iteration 253, loss = 5.69613954
Iteration 254, loss = 5.69605021
Iteration 255, loss = 5.69555984
Iteration 256, loss = 5.69576162
Iteration 257, loss = 5.69551193
Iteration 258, loss = 5.69497955
Iteration 259, loss = 5.69471421
Iteration 260, loss = 5.69462512
Iteration 261, loss = 5.69450130
Iteration 262, loss = 5.69410769
Iteration 263, loss = 5.69403939
Iteration 264, loss = 5.69382384
Iteration 265, loss = 5.69351689
Iteration 266, loss = 5.69320614
Iteration 267, loss = 5.69323417
Iteration 268, loss = 5.69275276
Iteration 269, loss = 5.69271534
Iteration 270, loss = 5.69287252
Iteration 271, loss = 5.69241920
Iteration 272, loss = 5.69223933
Iteration 273, loss = 5.69188086
Iteration 274, loss = 5.69197675
Iteration 275, loss = 5.69191559
Iteration 276, loss = 5.69126357
Iteration 277, loss = 5.69111151
Iteration 278, loss = 5.69090839
Iteration 279, loss = 5.69087301
Iteration 280, loss = 5.69055839
Iteration 281, loss = 5.69053621
Iteration 282, loss = 5.69060966
Iteration 283, loss = 5.68993407
Iteration 284, loss = 5.69031206
Iteration 285, loss = 5.68951551
Iteration 286, loss = 5.68954572
Iteration 287, loss = 5.68922305
Iteration 288, loss = 5.68949360
Iteration 289, loss = 5.68900350
Iteration 290, loss = 5.68853170
Iteration 291, loss = 5.68846758
Iteration 292, loss = 5.68863258
Iteration 293, loss = 5.68820046
Iteration 294, loss = 5.68846744
Iteration 295, loss = 5.68797130
Iteration 296, loss = 5.68782879
Iteration 297, loss = 5.68761071
Iteration 298, loss = 5.68759959
Iteration 299, loss = 5.68724609
Iteration 300, loss = 5.68703312
Iteration 301, loss = 5.68679589
Iteration 302, loss = 5.68682595
Iteration 303, loss = 5.68660113
Iteration 304, loss = 5.68649079
Iteration 305, loss = 5.68637556
Iteration 306, loss = 5.68612053
Iteration 307, loss = 5.68624803
Iteration 308, loss = 5.68616859
Iteration 309, loss = 5.68562379
Iteration 310, loss = 5.68553611
Iteration 311, loss = 5.68561723
Iteration 312, loss = 5.68522822
Iteration 313, loss = 5.68508562
Iteration 314, loss = 5.68486570
Iteration 315, loss = 5.68524665
Iteration 316, loss = 5.68496788
Iteration 317, loss = 5.68517291
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((1, 1, 1, 1), {'error': 2.3075837500788983, 'fit': 0.021250000000000002, 'time': 378.0600000000013})
Iteration 1, loss = 6.52627546
Iteration 2, loss = 6.42807764
Iteration 3, loss = 6.26206435
Iteration 4, loss = 6.10020874
Iteration 5, loss = 5.98198078
Iteration 6, loss = 5.90279511
Iteration 7, loss = 5.84590312
Iteration 8, loss = 5.80052154
Iteration 9, loss = 5.75976733
Iteration 10, loss = 5.72170959
Iteration 11, loss = 5.68660329
Iteration 12, loss = 5.65517444
Iteration 13, loss = 5.62702887
Iteration 14, loss = 5.60236024
Iteration 15, loss = 5.58021796
Iteration 16, loss = 5.56094676
Iteration 17, loss = 5.54352991
Iteration 18, loss = 5.52810631
Iteration 19, loss = 5.51428444
Iteration 20, loss = 5.50126511
Iteration 21, loss = 5.48928689
Iteration 22, loss = 5.47820034
Iteration 23, loss = 5.46729131
Iteration 24, loss = 5.45769578
Iteration 25, loss = 5.44796359
Iteration 26, loss = 5.43913125
Iteration 27, loss = 5.43037029
Iteration 28, loss = 5.42251471
Iteration 29, loss = 5.41385944
Iteration 30, loss = 5.40659786
Iteration 31, loss = 5.39848806
Iteration 32, loss = 5.39095258
Iteration 33, loss = 5.38378432
Iteration 34, loss = 5.37693711
Iteration 35, loss = 5.36963811
Iteration 36, loss = 5.36310186
Iteration 37, loss = 5.35639082
Iteration 38, loss = 5.35003979
Iteration 39, loss = 5.34393925
Iteration 40, loss = 5.33871175
Iteration 41, loss = 5.33254108
Iteration 42, loss = 5.32811000
Iteration 43, loss = 5.32251047
Iteration 44, loss = 5.31710245
Iteration 45, loss = 5.31235145
Iteration 46, loss = 5.30840669
Iteration 47, loss = 5.30383510
Iteration 48, loss = 5.29869749
Iteration 49, loss = 5.29451919
Iteration 50, loss = 5.29048117
Iteration 51, loss = 5.28648614
Iteration 52, loss = 5.28314196
Iteration 53, loss = 5.27933688
Iteration 54, loss = 5.27473384
Iteration 55, loss = 5.26993602
Iteration 56, loss = 5.26657908
Iteration 57, loss = 5.26255590
Iteration 58, loss = 5.25819087
Iteration 59, loss = 5.25330614
Iteration 60, loss = 5.24948191
Iteration 61, loss = 5.24475092
Iteration 62, loss = 5.24093358
Iteration 63, loss = 5.23647703
Iteration 64, loss = 5.23167004
Iteration 65, loss = 5.22647011
Iteration 66, loss = 5.22306700
Iteration 67, loss = 5.21811507
Iteration 68, loss = 5.21542334
Iteration 69, loss = 5.21061455
Iteration 70, loss = 5.20614656
Iteration 71, loss = 5.20256641
Iteration 72, loss = 5.19916166
Iteration 73, loss = 5.19495135
Iteration 74, loss = 5.19231684
Iteration 75, loss = 5.18856295
Iteration 76, loss = 5.18452558
Iteration 77, loss = 5.18122587
Iteration 78, loss = 5.17845325
Iteration 79, loss = 5.17586229
Iteration 80, loss = 5.17253889
Iteration 81, loss = 5.16909120
Iteration 82, loss = 5.16635107
Iteration 83, loss = 5.16368826
Iteration 84, loss = 5.15988244
Iteration 85, loss = 5.15772190
Iteration 86, loss = 5.15501940
Iteration 87, loss = 5.15227726
Iteration 88, loss = 5.14907534
Iteration 89, loss = 5.14656756
Iteration 90, loss = 5.14598113
Iteration 91, loss = 5.14215407
Iteration 92, loss = 5.13978613
Iteration 93, loss = 5.13663083
Iteration 94, loss = 5.13421183
Iteration 95, loss = 5.13185882
Iteration 96, loss = 5.13050570
Iteration 97, loss = 5.12687464
Iteration 98, loss = 5.12475771
Iteration 99, loss = 5.12187464
Iteration 100, loss = 5.12007117
Iteration 101, loss = 5.11716270
Iteration 102, loss = 5.11540973
Iteration 103, loss = 5.11337482
Iteration 104, loss = 5.11093513
Iteration 105, loss = 5.10887668
Iteration 106, loss = 5.10718566
Iteration 107, loss = 5.10499817
Iteration 108, loss = 5.10226387
Iteration 109, loss = 5.10050624
Iteration 110, loss = 5.09854194
Iteration 111, loss = 5.09662262
Iteration 112, loss = 5.09481767
Iteration 113, loss = 5.09303197
Iteration 114, loss = 5.09031137
Iteration 115, loss = 5.08828544
Iteration 116, loss = 5.08619269
Iteration 117, loss = 5.08403221
Iteration 118, loss = 5.08201278
Iteration 119, loss = 5.08012435
Iteration 120, loss = 5.07856554
Iteration 121, loss = 5.07735845
Iteration 122, loss = 5.07522871
Iteration 123, loss = 5.07294706
Iteration 124, loss = 5.07049145
Iteration 125, loss = 5.07075904
Iteration 126, loss = 5.06841387
Iteration 127, loss = 5.06568873
Iteration 128, loss = 5.06378719
Iteration 129, loss = 5.06184461
Iteration 130, loss = 5.06047424
Iteration 131, loss = 5.05885783
Iteration 132, loss = 5.05715333
Iteration 133, loss = 5.05519808
Iteration 134, loss = 5.05366357
Iteration 135, loss = 5.05277620
Iteration 136, loss = 5.05063675
Iteration 137, loss = 5.04889336
Iteration 138, loss = 5.04696306
Iteration 139, loss = 5.04493662
Iteration 140, loss = 5.04284118
Iteration 141, loss = 5.04190118
Iteration 142, loss = 5.04020022
Iteration 143, loss = 5.03905280
Iteration 144, loss = 5.03729751
Iteration 145, loss = 5.03509379
Iteration 146, loss = 5.03341059
Iteration 147, loss = 5.03228497
Iteration 148, loss = 5.03123438
Iteration 149, loss = 5.03133532
Iteration 150, loss = 5.02846293
Iteration 151, loss = 5.02744214
Iteration 152, loss = 5.02550659
Iteration 153, loss = 5.02305594
Iteration 154, loss = 5.02260180
Iteration 155, loss = 5.02066292
Iteration 156, loss = 5.01934278
Iteration 157, loss = 5.01811044
Iteration 158, loss = 5.01661634
Iteration 159, loss = 5.01385474
Iteration 160, loss = 5.01373754
Iteration 161, loss = 5.01200254
Iteration 162, loss = 5.01102367
Iteration 163, loss = 5.00905080
Iteration 164, loss = 5.00728339
Iteration 165, loss = 5.00635444
Iteration 166, loss = 5.00460113
Iteration 167, loss = 5.00332653
Iteration 168, loss = 5.00364102
Iteration 169, loss = 5.00140540
Iteration 170, loss = 5.00006364
Iteration 171, loss = 4.99980782
Iteration 172, loss = 4.99683018
Iteration 173, loss = 4.99522181
Iteration 174, loss = 4.99452646
Iteration 175, loss = 4.99299448
Iteration 176, loss = 4.99214552
Iteration 177, loss = 4.99284527
Iteration 178, loss = 4.98954371
Iteration 179, loss = 4.98791918
Iteration 180, loss = 4.98732265
Iteration 181, loss = 4.98657586
Iteration 182, loss = 4.98401673
Iteration 183, loss = 4.98286845
Iteration 184, loss = 4.98129756
Iteration 185, loss = 4.98158035
Iteration 186, loss = 4.97885983
Iteration 187, loss = 4.97874587
Iteration 188, loss = 4.97680934
Iteration 189, loss = 4.97471283
Iteration 190, loss = 4.97462425
Iteration 191, loss = 4.97429711
Iteration 192, loss = 4.97150269
Iteration 193, loss = 4.97134366
Iteration 194, loss = 4.97023626
Iteration 195, loss = 4.96746430
Iteration 196, loss = 4.96667702
Iteration 197, loss = 4.96716834
Iteration 198, loss = 4.96423746
Iteration 199, loss = 4.96313977
Iteration 200, loss = 4.96229004
Iteration 201, loss = 4.96149129
Iteration 202, loss = 4.96066655
Iteration 203, loss = 4.95866887
Iteration 204, loss = 4.95814629
Iteration 205, loss = 4.95814404
Iteration 206, loss = 4.95501302
Iteration 207, loss = 4.95464234
Iteration 208, loss = 4.95233180
Iteration 209, loss = 4.95096774
Iteration 210, loss = 4.95040856
Iteration 211, loss = 4.94917628
Iteration 212, loss = 4.94738895
Iteration 213, loss = 4.94672044
Iteration 214, loss = 4.94537522
Iteration 215, loss = 4.94360950
Iteration 216, loss = 4.94422930
Iteration 217, loss = 4.94151386
Iteration 218, loss = 4.94061032
Iteration 219, loss = 4.94056332
Iteration 220, loss = 4.93685442
Iteration 221, loss = 4.93672013
Iteration 222, loss = 4.93564175
Iteration 223, loss = 4.93425103
Iteration 224, loss = 4.93169097
Iteration 225, loss = 4.93062144
Iteration 226, loss = 4.93008573
Iteration 227, loss = 4.92871370
Iteration 228, loss = 4.92737494
Iteration 229, loss = 4.92625076
Iteration 230, loss = 4.92483797
Iteration 231, loss = 4.92363952
Iteration 232, loss = 4.92211282
Iteration 233, loss = 4.92112409
Iteration 234, loss = 4.92001792
Iteration 235, loss = 4.91844814
Iteration 236, loss = 4.91945042
Iteration 237, loss = 4.91863111
Iteration 238, loss = 4.91516747
Iteration 239, loss = 4.91502754
Iteration 240, loss = 4.91291363
Iteration 241, loss = 4.91310139
Iteration 242, loss = 4.91034779
Iteration 243, loss = 4.91047192
Iteration 244, loss = 4.90897741
Iteration 245, loss = 4.90741302
Iteration 246, loss = 4.90663292
Iteration 247, loss = 4.90499258
Iteration 248, loss = 4.90453506
Iteration 249, loss = 4.90367887
Iteration 250, loss = 4.90246760
Iteration 251, loss = 4.90125882
Iteration 252, loss = 4.89877846
Iteration 253, loss = 4.89793135
Iteration 254, loss = 4.89539358
Iteration 255, loss = 4.89481636
Iteration 256, loss = 4.89365112
Iteration 257, loss = 4.89192183
Iteration 258, loss = 4.88966405
Iteration 259, loss = 4.88812272
Iteration 260, loss = 4.88706076
Iteration 261, loss = 4.88656561
Iteration 262, loss = 4.88309443
Iteration 263, loss = 4.88134185
Iteration 264, loss = 4.87947951
Iteration 265, loss = 4.87945427
Iteration 266, loss = 4.87807630
Iteration 267, loss = 4.87525425
Iteration 268, loss = 4.87542076
Iteration 269, loss = 4.87387072
Iteration 270, loss = 4.87126101
Iteration 271, loss = 4.87015977
Iteration 272, loss = 4.86868525
Iteration 273, loss = 4.86733831
Iteration 274, loss = 4.86510622
Iteration 275, loss = 4.86482357
Iteration 276, loss = 4.86364501
Iteration 277, loss = 4.86224172
Iteration 278, loss = 4.86118195
Iteration 279, loss = 4.86145405
Iteration 280, loss = 4.85807021
Iteration 281, loss = 4.85701340
Iteration 282, loss = 4.85627062
Iteration 283, loss = 4.85324623
Iteration 284, loss = 4.85410054
Iteration 285, loss = 4.85138937
Iteration 286, loss = 4.85024666
Iteration 287, loss = 4.84879285
Iteration 288, loss = 4.84831975
Iteration 289, loss = 4.84659973
Iteration 290, loss = 4.84541712
Iteration 291, loss = 4.84530997
Iteration 292, loss = 4.84252673
Iteration 293, loss = 4.84385128
Iteration 294, loss = 4.84101808
Iteration 295, loss = 4.83928882
Iteration 296, loss = 4.83870193
Iteration 297, loss = 4.83708911
Iteration 298, loss = 4.83822019
Iteration 299, loss = 4.83516389
Iteration 300, loss = 4.83380055
Iteration 301, loss = 4.83271659
Iteration 302, loss = 4.83251118
Iteration 303, loss = 4.83121231
Iteration 304, loss = 4.83360712
Iteration 305, loss = 4.82818917
Iteration 306, loss = 4.82860415
Iteration 307, loss = 4.82596561
Iteration 308, loss = 4.82532221
Iteration 309, loss = 4.82412589
Iteration 310, loss = 4.82229919
Iteration 311, loss = 4.82149296
Iteration 312, loss = 4.82034971
Iteration 313, loss = 4.81969197
Iteration 314, loss = 4.81860505
Iteration 315, loss = 4.81793787
Iteration 316, loss = 4.81726108
Iteration 317, loss = 4.81641542
Iteration 318, loss = 4.81520922
Iteration 319, loss = 4.81245290
Iteration 320, loss = 4.81393623
Iteration 321, loss = 4.81154047
Iteration 322, loss = 4.81022735
Iteration 323, loss = 4.81064186
Iteration 324, loss = 4.80856253
Iteration 325, loss = 4.80773638
Iteration 326, loss = 4.80687804
Iteration 327, loss = 4.80735984
Iteration 328, loss = 4.80503884
Iteration 329, loss = 4.80690212
Iteration 330, loss = 4.80319372
Iteration 331, loss = 4.80229755
Iteration 332, loss = 4.80199667
Iteration 333, loss = 4.79926332
Iteration 334, loss = 4.79994493
Iteration 335, loss = 4.79796568
Iteration 336, loss = 4.79911072
Iteration 337, loss = 4.79656297
Iteration 338, loss = 4.79612713
Iteration 339, loss = 4.79462805
Iteration 340, loss = 4.79593137
Iteration 341, loss = 4.79528230
Iteration 342, loss = 4.79400679
Iteration 343, loss = 4.79195750
Iteration 344, loss = 4.79097530
Iteration 345, loss = 4.78940467
Iteration 346, loss = 4.78919964
Iteration 347, loss = 4.78845224
Iteration 348, loss = 4.78745914
Iteration 349, loss = 4.78934317
Iteration 350, loss = 4.78738113
Iteration 351, loss = 4.78529042
Iteration 352, loss = 4.78528157
Iteration 353, loss = 4.78454267
Iteration 354, loss = 4.78370790
Iteration 355, loss = 4.78262461
Iteration 356, loss = 4.78405937
Iteration 357, loss = 4.78191404
Iteration 358, loss = 4.78038060
Iteration 359, loss = 4.77999833
Iteration 360, loss = 4.77790179
Iteration 361, loss = 4.77780432
Iteration 362, loss = 4.77632240
Iteration 363, loss = 4.77525697
Iteration 364, loss = 4.77620342
Iteration 365, loss = 4.77392883
Iteration 366, loss = 4.77222060
Iteration 367, loss = 4.77365435
Iteration 368, loss = 4.77225709
Iteration 369, loss = 4.77150858
Iteration 370, loss = 4.77002807
Iteration 371, loss = 4.76978424
Iteration 372, loss = 4.77062576
Iteration 373, loss = 4.76956859
Iteration 374, loss = 4.76835652
Iteration 375, loss = 4.76817832
Iteration 376, loss = 4.76862124
Iteration 377, loss = 4.76547867
Iteration 378, loss = 4.76577415
Iteration 379, loss = 4.76623480
Iteration 380, loss = 4.76756688
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((10, 10, 10, 10), {'error': 0.87762844933170847, 'fit': 0.037374999999999999, 'time': 493.5})
Iteration 1, loss = 6.21591142
Iteration 2, loss = 5.74703881
Iteration 3, loss = 5.59682263
Iteration 4, loss = 5.50542234
Iteration 5, loss = 5.43857751
Iteration 6, loss = 5.38099195
Iteration 7, loss = 5.32727785
Iteration 8, loss = 5.27965428
Iteration 9, loss = 5.23283953
Iteration 10, loss = 5.19249693
Iteration 11, loss = 5.14957901
Iteration 12, loss = 5.11268105
Iteration 13, loss = 5.06879108
Iteration 14, loss = 5.02835807
Iteration 15, loss = 4.99022445
Iteration 16, loss = 4.95119511
Iteration 17, loss = 4.90963605
Iteration 18, loss = 4.87018554
Iteration 19, loss = 4.83528249
Iteration 20, loss = 4.79875306
Iteration 21, loss = 4.76504836
Iteration 22, loss = 4.73031303
Iteration 23, loss = 4.69369052
Iteration 24, loss = 4.66367342
Iteration 25, loss = 4.63705084
Iteration 26, loss = 4.60581708
Iteration 27, loss = 4.57514950
Iteration 28, loss = 4.54436948
Iteration 29, loss = 4.51993020
Iteration 30, loss = 4.49391694
Iteration 31, loss = 4.46444588
Iteration 32, loss = 4.44407655
Iteration 33, loss = 4.41418273
Iteration 34, loss = 4.39115968
Iteration 35, loss = 4.37185690
Iteration 36, loss = 4.34482029
Iteration 37, loss = 4.31813974
Iteration 38, loss = 4.29684649
Iteration 39, loss = 4.27680101
Iteration 40, loss = 4.25371675
Iteration 41, loss = 4.23179150
Iteration 42, loss = 4.20853713
Iteration 43, loss = 4.18647455
Iteration 44, loss = 4.16648461
Iteration 45, loss = 4.14475575
Iteration 46, loss = 4.12352881
Iteration 47, loss = 4.10095667
Iteration 48, loss = 4.08369737
Iteration 49, loss = 4.06313197
Iteration 50, loss = 4.03981632
Iteration 51, loss = 4.01925432
Iteration 52, loss = 4.00617949
Iteration 53, loss = 3.98023027
Iteration 54, loss = 3.96256051
Iteration 55, loss = 3.94088971
Iteration 56, loss = 3.92184774
Iteration 57, loss = 3.90119179
Iteration 58, loss = 3.88495283
Iteration 59, loss = 3.86442438
Iteration 60, loss = 3.84588483
Iteration 61, loss = 3.82701998
Iteration 62, loss = 3.80878253
Iteration 63, loss = 3.79023481
Iteration 64, loss = 3.77437101
Iteration 65, loss = 3.75639747
Iteration 66, loss = 3.73907929
Iteration 67, loss = 3.71889843
Iteration 68, loss = 3.70021455
Iteration 69, loss = 3.68016678
Iteration 70, loss = 3.66171256
Iteration 71, loss = 3.65119632
Iteration 72, loss = 3.63381907
Iteration 73, loss = 3.61148406
Iteration 74, loss = 3.59974105
Iteration 75, loss = 3.57912785
Iteration 76, loss = 3.56096835
Iteration 77, loss = 3.54188604
Iteration 78, loss = 3.52712816
Iteration 79, loss = 3.50781446
Iteration 80, loss = 3.49662900
Iteration 81, loss = 3.47741526
Iteration 82, loss = 3.45942516
Iteration 83, loss = 3.44774819
Iteration 84, loss = 3.43128976
Iteration 85, loss = 3.41503910
Iteration 86, loss = 3.39758446
Iteration 87, loss = 3.38086595
Iteration 88, loss = 3.36597636
Iteration 89, loss = 3.35280784
Iteration 90, loss = 3.33459487
Iteration 91, loss = 3.31653646
Iteration 92, loss = 3.29904513
Iteration 93, loss = 3.28389085
Iteration 94, loss = 3.27036267
Iteration 95, loss = 3.25443039
Iteration 96, loss = 3.23598027
Iteration 97, loss = 3.22499203
Iteration 98, loss = 3.20784555
Iteration 99, loss = 3.19449610
Iteration 100, loss = 3.17894995
Iteration 101, loss = 3.17065463
Iteration 102, loss = 3.14889016
Iteration 103, loss = 3.13480997
Iteration 104, loss = 3.11990162
Iteration 105, loss = 3.10586491
Iteration 106, loss = 3.09441812
Iteration 107, loss = 3.07607633
Iteration 108, loss = 3.06094043
Iteration 109, loss = 3.05101772
Iteration 110, loss = 3.03399767
Iteration 111, loss = 3.02124984
Iteration 112, loss = 3.00884166
Iteration 113, loss = 2.98882462
Iteration 114, loss = 2.97902216
Iteration 115, loss = 2.96112571
Iteration 116, loss = 2.95070077
Iteration 117, loss = 2.93608727
Iteration 118, loss = 2.92142658
Iteration 119, loss = 2.91003034
Iteration 120, loss = 2.89847408
Iteration 121, loss = 2.88192907
Iteration 122, loss = 2.86617140
Iteration 123, loss = 2.85727300
Iteration 124, loss = 2.84434773
Iteration 125, loss = 2.83128324
Iteration 126, loss = 2.81607947
Iteration 127, loss = 2.80597950
Iteration 128, loss = 2.78994759
Iteration 129, loss = 2.77709333
Iteration 130, loss = 2.76326311
Iteration 131, loss = 2.74919425
Iteration 132, loss = 2.74097585
Iteration 133, loss = 2.72819693
Iteration 134, loss = 2.71641888
Iteration 135, loss = 2.69876648
Iteration 136, loss = 2.68732470
Iteration 137, loss = 2.67908125
Iteration 138, loss = 2.66893019
Iteration 139, loss = 2.65599447
Iteration 140, loss = 2.64177728
Iteration 141, loss = 2.62557691
Iteration 142, loss = 2.61657471
Iteration 143, loss = 2.60436867
Iteration 144, loss = 2.59250461
Iteration 145, loss = 2.58530483
Iteration 146, loss = 2.57026646
Iteration 147, loss = 2.55807484
Iteration 148, loss = 2.54269961
Iteration 149, loss = 2.53594682
Iteration 150, loss = 2.52154192
Iteration 151, loss = 2.51356042
Iteration 152, loss = 2.49730522
Iteration 153, loss = 2.48624792
Iteration 154, loss = 2.47376148
Iteration 155, loss = 2.46535097
Iteration 156, loss = 2.45263722
Iteration 157, loss = 2.44514429
Iteration 158, loss = 2.43708219
Iteration 159, loss = 2.42433632
Iteration 160, loss = 2.41170028
Iteration 161, loss = 2.39799622
Iteration 162, loss = 2.38702339
Iteration 163, loss = 2.37487260
Iteration 164, loss = 2.36533651
Iteration 165, loss = 2.35040850
Iteration 166, loss = 2.34490483
Iteration 167, loss = 2.33127787
Iteration 168, loss = 2.32678712
Iteration 169, loss = 2.31522367
Iteration 170, loss = 2.30252298
Iteration 171, loss = 2.29306377
Iteration 172, loss = 2.28236817
Iteration 173, loss = 2.27141662
Iteration 174, loss = 2.26056511
Iteration 175, loss = 2.24911146
Iteration 176, loss = 2.24199438
Iteration 177, loss = 2.23031693
Iteration 178, loss = 2.22256205
Iteration 179, loss = 2.20898521
Iteration 180, loss = 2.20215611
Iteration 181, loss = 2.18959651
Iteration 182, loss = 2.18285111
Iteration 183, loss = 2.17215216
Iteration 184, loss = 2.16330339
Iteration 185, loss = 2.14802755
Iteration 186, loss = 2.13955534
Iteration 187, loss = 2.13250316
Iteration 188, loss = 2.12452613
Iteration 189, loss = 2.11215671
Iteration 190, loss = 2.10302917
Iteration 191, loss = 2.09425897
Iteration 192, loss = 2.08215542
Iteration 193, loss = 2.07479587
Iteration 194, loss = 2.06398894
Iteration 195, loss = 2.05482432
Iteration 196, loss = 2.04537219
Iteration 197, loss = 2.03740009
Iteration 198, loss = 2.03076013
Iteration 199, loss = 2.01492688
Iteration 200, loss = 2.00978943
Iteration 201, loss = 2.00410885
Iteration 202, loss = 1.99397111
Iteration 203, loss = 1.98507750
Iteration 204, loss = 1.97425432
Iteration 205, loss = 1.96176773
Iteration 206, loss = 1.94780123
Iteration 207, loss = 1.94693931
Iteration 208, loss = 1.93642171
Iteration 209, loss = 1.92897975
Iteration 210, loss = 1.91898938
Iteration 211, loss = 1.90982008
Iteration 212, loss = 1.90347500
Iteration 213, loss = 1.89601004
Iteration 214, loss = 1.88719926
Iteration 215, loss = 1.87923564
Iteration 216, loss = 1.86822093
Iteration 217, loss = 1.85604525
Iteration 218, loss = 1.85597876
Iteration 219, loss = 1.84823689
Iteration 220, loss = 1.83334525
Iteration 221, loss = 1.82970503
Iteration 222, loss = 1.81948364
Iteration 223, loss = 1.81312414
Iteration 224, loss = 1.80527306
Iteration 225, loss = 1.79307809
Iteration 226, loss = 1.78930963
Iteration 227, loss = 1.77656361
Iteration 228, loss = 1.77061530
Iteration 229, loss = 1.76358469
Iteration 230, loss = 1.75725150
Iteration 231, loss = 1.74921794
Iteration 232, loss = 1.74126914
Iteration 233, loss = 1.73199980
Iteration 234, loss = 1.72241729
Iteration 235, loss = 1.71365655
Iteration 236, loss = 1.70670646
Iteration 237, loss = 1.70135561
Iteration 238, loss = 1.69801718
Iteration 239, loss = 1.68273044
Iteration 240, loss = 1.67607286
Iteration 241, loss = 1.66807459
Iteration 242, loss = 1.66514048
Iteration 243, loss = 1.65378378
Iteration 244, loss = 1.65098843
Iteration 245, loss = 1.64069709
Iteration 246, loss = 1.63304367
Iteration 247, loss = 1.62993481
Iteration 248, loss = 1.61751984
Iteration 249, loss = 1.61389213
Iteration 250, loss = 1.60169158
Iteration 251, loss = 1.59667960
Iteration 252, loss = 1.59104406
Iteration 253, loss = 1.58306178
Iteration 254, loss = 1.57613891
Iteration 255, loss = 1.57103887
Iteration 256, loss = 1.56389675
Iteration 257, loss = 1.55213350
Iteration 258, loss = 1.54218587
Iteration 259, loss = 1.53832563
Iteration 260, loss = 1.53045158
Iteration 261, loss = 1.52429344
Iteration 262, loss = 1.51867229
Iteration 263, loss = 1.50972641
Iteration 264, loss = 1.50243134
Iteration 265, loss = 1.50261436
Iteration 266, loss = 1.49431936
Iteration 267, loss = 1.48498948
Iteration 268, loss = 1.47805304
Iteration 269, loss = 1.47044670
Iteration 270, loss = 1.46423907
Iteration 271, loss = 1.45607260
Iteration 272, loss = 1.45097249
Iteration 273, loss = 1.44199568
Iteration 274, loss = 1.43812336
Iteration 275, loss = 1.43705006
Iteration 276, loss = 1.42859563
Iteration 277, loss = 1.42261887
Iteration 278, loss = 1.41075184
Iteration 279, loss = 1.40679342
Iteration 280, loss = 1.40396738
Iteration 281, loss = 1.39561977
Iteration 282, loss = 1.39151727
Iteration 283, loss = 1.38161285
Iteration 284, loss = 1.37707347
Iteration 285, loss = 1.37158034
Iteration 286, loss = 1.36409274
Iteration 287, loss = 1.35719197
Iteration 288, loss = 1.35265264
Iteration 289, loss = 1.34355322
Iteration 290, loss = 1.33669114
Iteration 291, loss = 1.33688041
Iteration 292, loss = 1.32966518
Iteration 293, loss = 1.32304864
Iteration 294, loss = 1.31253309
Iteration 295, loss = 1.30783516
Iteration 296, loss = 1.30441389
Iteration 297, loss = 1.29733071
Iteration 298, loss = 1.29881553
Iteration 299, loss = 1.28729700
Iteration 300, loss = 1.27964472
Iteration 301, loss = 1.27269381
Iteration 302, loss = 1.26800718
Iteration 303, loss = 1.26479722
Iteration 304, loss = 1.25866395
Iteration 305, loss = 1.25302324
Iteration 306, loss = 1.24437040
Iteration 307, loss = 1.24106022
Iteration 308, loss = 1.23859450
Iteration 309, loss = 1.22888565
Iteration 310, loss = 1.22348697
Iteration 311, loss = 1.21835444
Iteration 312, loss = 1.20861198
Iteration 313, loss = 1.20151767
Iteration 314, loss = 1.19850965
Iteration 315, loss = 1.19579432
Iteration 316, loss = 1.19408182
Iteration 317, loss = 1.18778656
Iteration 318, loss = 1.18062478
Iteration 319, loss = 1.17453174
Iteration 320, loss = 1.16949672
Iteration 321, loss = 1.16765879
Iteration 322, loss = 1.15945012
Iteration 323, loss = 1.15445152
Iteration 324, loss = 1.14994409
Iteration 325, loss = 1.14417241
Iteration 326, loss = 1.13675393
Iteration 327, loss = 1.13392518
Iteration 328, loss = 1.12962498
Iteration 329, loss = 1.12224377
Iteration 330, loss = 1.11787271
Iteration 331, loss = 1.11342923
Iteration 332, loss = 1.10975816
Iteration 333, loss = 1.10172087
Iteration 334, loss = 1.09928444
Iteration 335, loss = 1.09473003
Iteration 336, loss = 1.08901017
Iteration 337, loss = 1.08264882
Iteration 338, loss = 1.07351089
Iteration 339, loss = 1.07020138
Iteration 340, loss = 1.06604217
Iteration 341, loss = 1.06034273
Iteration 342, loss = 1.05960294
Iteration 343, loss = 1.05404650
Iteration 344, loss = 1.04959839
Iteration 345, loss = 1.04514305
Iteration 346, loss = 1.04060820
Iteration 347, loss = 1.03439804
Iteration 348, loss = 1.03206061
Iteration 349, loss = 1.02916174
Iteration 350, loss = 1.02664731
Iteration 351, loss = 1.01751775
Iteration 352, loss = 1.02154144
Iteration 353, loss = 1.01018861
Iteration 354, loss = 1.00138709
Iteration 355, loss = 0.99428758
Iteration 356, loss = 0.99252547
Iteration 357, loss = 0.99144762
Iteration 358, loss = 0.98420210
Iteration 359, loss = 0.97921680
Iteration 360, loss = 0.97574520
Iteration 361, loss = 0.97061477
Iteration 362, loss = 0.96741182
Iteration 363, loss = 0.96544825
Iteration 364, loss = 0.95979109
Iteration 365, loss = 0.95788776
Iteration 366, loss = 0.95087424
Iteration 367, loss = 0.94657807
Iteration 368, loss = 0.94226878
Iteration 369, loss = 0.93475551
Iteration 370, loss = 0.93168191
Iteration 371, loss = 0.92538968
Iteration 372, loss = 0.92403606
Iteration 373, loss = 0.91801099
Iteration 374, loss = 0.92022296
Iteration 375, loss = 0.91395344
Iteration 376, loss = 0.90623018
Iteration 377, loss = 0.90078196
Iteration 378, loss = 0.89786092
Iteration 379, loss = 0.89770596
Iteration 380, loss = 0.89310258
Iteration 381, loss = 0.88791282
Iteration 382, loss = 0.88399933
Iteration 383, loss = 0.87994968
Iteration 384, loss = 0.87656803
Iteration 385, loss = 0.87128887
Iteration 386, loss = 0.86547230
Iteration 387, loss = 0.86122798
Iteration 388, loss = 0.85932963
Iteration 389, loss = 0.85851440
Iteration 390, loss = 0.85191163
Iteration 391, loss = 0.84499156
Iteration 392, loss = 0.84859941
Iteration 393, loss = 0.83903959
Iteration 394, loss = 0.83679436
Iteration 395, loss = 0.83018996
Iteration 396, loss = 0.82652171
Iteration 397, loss = 0.82474892
Iteration 398, loss = 0.82045783
Iteration 399, loss = 0.81845919
Iteration 400, loss = 0.81292090
Iteration 401, loss = 0.80916556
Iteration 402, loss = 0.80522483
Iteration 403, loss = 0.80147847
Iteration 404, loss = 0.79824332
Iteration 405, loss = 0.79452014
Iteration 406, loss = 0.79317244
Iteration 407, loss = 0.79427404
Iteration 408, loss = 0.78588441
Iteration 409, loss = 0.78052593
Iteration 410, loss = 0.77595400
Iteration 411, loss = 0.77222554
Iteration 412, loss = 0.76898196
Iteration 413, loss = 0.76639503
Iteration 414, loss = 0.76158201
Iteration 415, loss = 0.76073990
Iteration 416, loss = 0.75577992
Iteration 417, loss = 0.75341368
Iteration 418, loss = 0.74890725
Iteration 419, loss = 0.74709096
Iteration 420, loss = 0.74271377
Iteration 421, loss = 0.73959213
Iteration 422, loss = 0.73674628
Iteration 423, loss = 0.73612711
Iteration 424, loss = 0.72829443
Iteration 425, loss = 0.72639415
Iteration 426, loss = 0.72077436
Iteration 427, loss = 0.71616327
Iteration 428, loss = 0.71383736
Iteration 429, loss = 0.71293055
Iteration 430, loss = 0.71004065
Iteration 431, loss = 0.70695145
Iteration 432, loss = 0.70375978
Iteration 433, loss = 0.70261167
Iteration 434, loss = 0.69606082
Iteration 435, loss = 0.69865941
Iteration 436, loss = 0.69261654
Iteration 437, loss = 0.68768837
Iteration 438, loss = 0.68338444
Iteration 439, loss = 0.68341668
Iteration 440, loss = 0.67779979
Iteration 441, loss = 0.67234001
Iteration 442, loss = 0.67025867
Iteration 443, loss = 0.67036856
Iteration 444, loss = 0.66737972
Iteration 445, loss = 0.66677664
Iteration 446, loss = 0.66366102
Iteration 447, loss = 0.66201243
Iteration 448, loss = 0.65482637
Iteration 449, loss = 0.65290671
Iteration 450, loss = 0.64857768
Iteration 451, loss = 0.64704347
Iteration 452, loss = 0.64138017
Iteration 453, loss = 0.64145938
Iteration 454, loss = 0.63652241
Iteration 455, loss = 0.63256207
Iteration 456, loss = 0.63157660
Iteration 457, loss = 0.62804971
Iteration 458, loss = 0.62710367
Iteration 459, loss = 0.62301809
Iteration 460, loss = 0.61691371
Iteration 461, loss = 0.61568686
Iteration 462, loss = 0.61699730
Iteration 463, loss = 0.61382059
Iteration 464, loss = 0.60981479
Iteration 465, loss = 0.60377680
Iteration 466, loss = 0.60260059
Iteration 467, loss = 0.59862993
Iteration 468, loss = 0.60002164
Iteration 469, loss = 0.59505770
Iteration 470, loss = 0.59255005
Iteration 471, loss = 0.59093819
Iteration 472, loss = 0.58578928
Iteration 473, loss = 0.58590221
Iteration 474, loss = 0.57908768
Iteration 475, loss = 0.57460159
Iteration 476, loss = 0.57710514
Iteration 477, loss = 0.57682932
Iteration 478, loss = 0.57484349
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100), {'error': 0.76457026640732462, 'fit': 0.91749999999999998, 'time': 990.5})
Iteration 1, loss = 6.04867248
Iteration 2, loss = 5.74498494
Iteration 3, loss = 5.67612324
Iteration 4, loss = 5.58400772
Iteration 5, loss = 5.49076375
Iteration 6, loss = 5.43834549
Iteration 7, loss = 5.39073123
Iteration 8, loss = 5.36756488
Iteration 9, loss = 5.32824859
Iteration 10, loss = 5.28887372
Iteration 11, loss = 5.26240469
Iteration 12, loss = 5.23902647
Iteration 13, loss = 5.21951863
Iteration 14, loss = 5.16564849
Iteration 15, loss = 5.16039082
Iteration 16, loss = 5.10774956
Iteration 17, loss = 5.06790071
Iteration 18, loss = 5.01259341
Iteration 19, loss = 4.96720045
Iteration 20, loss = 4.91035391
Iteration 21, loss = 4.85135942
Iteration 22, loss = 4.80457108
Iteration 23, loss = 4.71347619
Iteration 24, loss = 4.61309634
Iteration 25, loss = 4.53259877
Iteration 26, loss = 4.45194007
Iteration 27, loss = 4.37678553
Iteration 28, loss = 4.26387074
Iteration 29, loss = 4.18160421
Iteration 30, loss = 4.07177272
Iteration 31, loss = 3.95324609
Iteration 32, loss = 3.87651257
Iteration 33, loss = 3.79136427
Iteration 34, loss = 3.69698551
Iteration 35, loss = 3.58826240
Iteration 36, loss = 3.49575355
Iteration 37, loss = 3.38963454
Iteration 38, loss = 3.26480041
Iteration 39, loss = 3.18021921
Iteration 40, loss = 3.08594417
Iteration 41, loss = 2.98476702
Iteration 42, loss = 2.89070533
Iteration 43, loss = 2.78921917
Iteration 44, loss = 2.68551543
Iteration 45, loss = 2.61186374
Iteration 46, loss = 2.50041617
Iteration 47, loss = 2.41513955
Iteration 48, loss = 2.31515043
Iteration 49, loss = 2.21907544
Iteration 50, loss = 2.15018358
Iteration 51, loss = 2.05656923
Iteration 52, loss = 1.96744366
Iteration 53, loss = 1.88386968
Iteration 54, loss = 1.83758682
Iteration 55, loss = 1.78709452
Iteration 56, loss = 1.67037738
Iteration 57, loss = 1.59669645
Iteration 58, loss = 1.51902554
Iteration 59, loss = 1.44949640
Iteration 60, loss = 1.37694658
Iteration 61, loss = 1.33156849
Iteration 62, loss = 1.24992237
Iteration 63, loss = 1.17983154
Iteration 64, loss = 1.13686458
Iteration 65, loss = 1.08284888
Iteration 66, loss = 1.03092872
Iteration 67, loss = 0.99064103
Iteration 68, loss = 0.93624027
Iteration 69, loss = 0.90399983
Iteration 70, loss = 0.85013167
Iteration 71, loss = 0.80033787
Iteration 72, loss = 0.75929609
Iteration 73, loss = 0.74311679
Iteration 74, loss = 0.70426516
Iteration 75, loss = 0.68347384
Iteration 76, loss = 0.65200424
Iteration 77, loss = 0.62712368
Iteration 78, loss = 0.59747955
Iteration 79, loss = 0.55859845
Iteration 80, loss = 0.53361424
Iteration 81, loss = 0.51826572
Iteration 82, loss = 0.48746259
Iteration 83, loss = 0.46071344
Iteration 84, loss = 0.44032656
Iteration 85, loss = 0.42101752
Iteration 86, loss = 0.39361405
Iteration 87, loss = 0.37864501
Iteration 88, loss = 0.37842716
Iteration 89, loss = 0.37794556
Iteration 90, loss = 0.36329136
Iteration 91, loss = 0.38185081
Iteration 92, loss = 0.35905757
Iteration 93, loss = 0.35814921
Iteration 94, loss = 0.33921954
Iteration 95, loss = 0.32892128
Iteration 96, loss = 0.30484697
Iteration 97, loss = 0.28224132
Iteration 98, loss = 0.26652897
Iteration 99, loss = 0.25825692
Iteration 100, loss = 0.24711823
Iteration 101, loss = 0.25129847
Iteration 102, loss = 0.29364959
Iteration 103, loss = 0.27688510
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((1000, 1000, 1000, 1000), {'error': 0.80128332882857511, 'fit': 0.95150000000000001, 'time': 2555.109999999997})
Iteration 1, loss = 6.53768216
Iteration 2, loss = 6.50825598
Iteration 3, loss = 6.47307709
Iteration 4, loss = 6.43476166
Iteration 5, loss = 6.39694851
Iteration 6, loss = 6.36166234
Iteration 7, loss = 6.32963002
Iteration 8, loss = 6.30041922
Iteration 9, loss = 6.27394277
Iteration 10, loss = 6.24991330
Iteration 11, loss = 6.22821616
Iteration 12, loss = 6.20820271
Iteration 13, loss = 6.19009723
Iteration 14, loss = 6.17342076
Iteration 15, loss = 6.15812625
Iteration 16, loss = 6.14404021
Iteration 17, loss = 6.13104744
Iteration 18, loss = 6.11898335
Iteration 19, loss = 6.10764909
Iteration 20, loss = 6.09671496
Iteration 21, loss = 6.08588295
Iteration 22, loss = 6.07444027
Iteration 23, loss = 6.06174187
Iteration 24, loss = 6.04884334
Iteration 25, loss = 6.03691923
Iteration 26, loss = 6.02607033
Iteration 27, loss = 6.01605607
Iteration 28, loss = 6.00684283
Iteration 29, loss = 5.99790909
Iteration 30, loss = 5.98957545
Iteration 31, loss = 5.98160687
Iteration 32, loss = 5.97399803
Iteration 33, loss = 5.96675662
Iteration 34, loss = 5.95977379
Iteration 35, loss = 5.95323107
Iteration 36, loss = 5.94683208
Iteration 37, loss = 5.94071749
Iteration 38, loss = 5.93488294
Iteration 39, loss = 5.92930428
Iteration 40, loss = 5.92407118
Iteration 41, loss = 5.91886318
Iteration 42, loss = 5.91390532
Iteration 43, loss = 5.90918977
Iteration 44, loss = 5.90449238
Iteration 45, loss = 5.90002124
Iteration 46, loss = 5.89565205
Iteration 47, loss = 5.89139058
Iteration 48, loss = 5.88729407
Iteration 49, loss = 5.88338047
Iteration 50, loss = 5.87946914
Iteration 51, loss = 5.87578272
Iteration 52, loss = 5.87213101
Iteration 53, loss = 5.86862680
Iteration 54, loss = 5.86514755
Iteration 55, loss = 5.86181915
Iteration 56, loss = 5.85856636
Iteration 57, loss = 5.85541299
Iteration 58, loss = 5.85227960
Iteration 59, loss = 5.84927598
Iteration 60, loss = 5.84635992
Iteration 61, loss = 5.84347321
Iteration 62, loss = 5.84063345
Iteration 63, loss = 5.83774965
Iteration 64, loss = 5.83504315
Iteration 65, loss = 5.83233821
Iteration 66, loss = 5.82973266
Iteration 67, loss = 5.82714688
Iteration 68, loss = 5.82469324
Iteration 69, loss = 5.82208596
Iteration 70, loss = 5.81969746
Iteration 71, loss = 5.81730352
Iteration 72, loss = 5.81488428
Iteration 73, loss = 5.81250104
Iteration 74, loss = 5.81026896
Iteration 75, loss = 5.80808366
Iteration 76, loss = 5.80583525
Iteration 77, loss = 5.80377175
Iteration 78, loss = 5.80172735
Iteration 79, loss = 5.79967008
Iteration 80, loss = 5.79756698
Iteration 81, loss = 5.79568155
Iteration 82, loss = 5.79372086
Iteration 83, loss = 5.79187925
Iteration 84, loss = 5.79001694
Iteration 85, loss = 5.78817549
Iteration 86, loss = 5.78661812
Iteration 87, loss = 5.78469957
Iteration 88, loss = 5.78298334
Iteration 89, loss = 5.78138701
Iteration 90, loss = 5.77973564
Iteration 91, loss = 5.77821025
Iteration 92, loss = 5.77670880
Iteration 93, loss = 5.77511836
Iteration 94, loss = 5.77367270
Iteration 95, loss = 5.77217643
Iteration 96, loss = 5.77088168
Iteration 97, loss = 5.76950511
Iteration 98, loss = 5.76802101
Iteration 99, loss = 5.76660761
Iteration 100, loss = 5.76545404
Iteration 101, loss = 5.76434626
Iteration 102, loss = 5.76291140
Iteration 103, loss = 5.76159035
Iteration 104, loss = 5.76047920
Iteration 105, loss = 5.75923794
Iteration 106, loss = 5.75820226
Iteration 107, loss = 5.75723697
Iteration 108, loss = 5.75594950
Iteration 109, loss = 5.75488418
Iteration 110, loss = 5.75378511
Iteration 111, loss = 5.75272745
Iteration 112, loss = 5.75174142
Iteration 113, loss = 5.75076714
Iteration 114, loss = 5.74990681
Iteration 115, loss = 5.74909697
Iteration 116, loss = 5.74800124
Iteration 117, loss = 5.74704295
Iteration 118, loss = 5.74617999
Iteration 119, loss = 5.74519148
Iteration 120, loss = 5.74444218
Iteration 121, loss = 5.74363485
Iteration 122, loss = 5.74275676
Iteration 123, loss = 5.74197249
Iteration 124, loss = 5.74105239
Iteration 125, loss = 5.74035618
Iteration 126, loss = 5.73950116
Iteration 127, loss = 5.73883193
Iteration 128, loss = 5.73822620
Iteration 129, loss = 5.73780603
Iteration 130, loss = 5.73660568
Iteration 131, loss = 5.73594753
Iteration 132, loss = 5.73557193
Iteration 133, loss = 5.73457864
Iteration 134, loss = 5.73383689
Iteration 135, loss = 5.73332007
Iteration 136, loss = 5.73264874
Iteration 137, loss = 5.73195786
Iteration 138, loss = 5.73166338
Iteration 139, loss = 5.73077185
Iteration 140, loss = 5.73023689
Iteration 141, loss = 5.72955097
Iteration 142, loss = 5.72900930
Iteration 143, loss = 5.72842685
Iteration 144, loss = 5.72799013
Iteration 145, loss = 5.72731274
Iteration 146, loss = 5.72675992
Iteration 147, loss = 5.72632755
Iteration 148, loss = 5.72574007
Iteration 149, loss = 5.72523637
Iteration 150, loss = 5.72505402
Iteration 151, loss = 5.72427460
Iteration 152, loss = 5.72366912
Iteration 153, loss = 5.72311364
Iteration 154, loss = 5.72274228
Iteration 155, loss = 5.72226759
Iteration 156, loss = 5.72195533
Iteration 157, loss = 5.72125825
Iteration 158, loss = 5.72095644
Iteration 159, loss = 5.72042329
Iteration 160, loss = 5.71983373
Iteration 161, loss = 5.71963578
Iteration 162, loss = 5.71900122
Iteration 163, loss = 5.71854478
Iteration 164, loss = 5.71834377
Iteration 165, loss = 5.71777868
Iteration 166, loss = 5.71771173
Iteration 167, loss = 5.71719055
Iteration 168, loss = 5.71688309
Iteration 169, loss = 5.71616070
Iteration 170, loss = 5.71577093
Iteration 171, loss = 5.71543627
Iteration 172, loss = 5.71496510
Iteration 173, loss = 5.71458614
Iteration 174, loss = 5.71435941
Iteration 175, loss = 5.71411188
Iteration 176, loss = 5.71386302
Iteration 177, loss = 5.71342214
Iteration 178, loss = 5.71286772
Iteration 179, loss = 5.71262993
Iteration 180, loss = 5.71210093
Iteration 181, loss = 5.71198160
Iteration 182, loss = 5.71149490
Iteration 183, loss = 5.71088729
Iteration 184, loss = 5.71090316
Iteration 185, loss = 5.71040105
Iteration 186, loss = 5.71012977
Iteration 187, loss = 5.71003686
Iteration 188, loss = 5.70950088
Iteration 189, loss = 5.70902998
Iteration 190, loss = 5.70882399
Iteration 191, loss = 5.70849578
Iteration 192, loss = 5.70831055
Iteration 193, loss = 5.70813375
Iteration 194, loss = 5.70773062
Iteration 195, loss = 5.70746888
Iteration 196, loss = 5.70723096
Iteration 197, loss = 5.70692424
Iteration 198, loss = 5.70647434
Iteration 199, loss = 5.70617714
Iteration 200, loss = 5.70601801
Iteration 201, loss = 5.70548271
Iteration 202, loss = 5.70530349
Iteration 203, loss = 5.70498525
Iteration 204, loss = 5.70462104
Iteration 205, loss = 5.70433901
Iteration 206, loss = 5.70400856
Iteration 207, loss = 5.70387748
Iteration 208, loss = 5.70364409
Iteration 209, loss = 5.70339237
Iteration 210, loss = 5.70313005
Iteration 211, loss = 5.70280263
Iteration 212, loss = 5.70261539
Iteration 213, loss = 5.70232525
Iteration 214, loss = 5.70203095
Iteration 215, loss = 5.70240453
Iteration 216, loss = 5.70170965
Iteration 217, loss = 5.70134652
Iteration 218, loss = 5.70139109
Iteration 219, loss = 5.70107315
Iteration 220, loss = 5.70077399
Iteration 221, loss = 5.70096652
Iteration 222, loss = 5.70022892
Iteration 223, loss = 5.70004054
Iteration 224, loss = 5.70027925
Iteration 225, loss = 5.69932183
Iteration 226, loss = 5.69943430
Iteration 227, loss = 5.69892851
Iteration 228, loss = 5.69867582
Iteration 229, loss = 5.69867917
Iteration 230, loss = 5.69871222
Iteration 231, loss = 5.69811049
Iteration 232, loss = 5.69805249
Iteration 233, loss = 5.69775800
Iteration 234, loss = 5.69759791
Iteration 235, loss = 5.69746820
Iteration 236, loss = 5.69703230
Iteration 237, loss = 5.69693020
Iteration 238, loss = 5.69670553
Iteration 239, loss = 5.69632185
Iteration 240, loss = 5.69612707
Iteration 241, loss = 5.69599672
Iteration 242, loss = 5.69612203
Iteration 243, loss = 5.69554529
Iteration 244, loss = 5.69543316
Iteration 245, loss = 5.69515640
Iteration 246, loss = 5.69504059
Iteration 247, loss = 5.69502206
Iteration 248, loss = 5.69451615
Iteration 249, loss = 5.69441337
Iteration 250, loss = 5.69442323
Iteration 251, loss = 5.69390345
Iteration 252, loss = 5.69408296
Iteration 253, loss = 5.69373910
Iteration 254, loss = 5.69367090
Iteration 255, loss = 5.69363427
Iteration 256, loss = 5.69307201
Iteration 257, loss = 5.69290579
Iteration 258, loss = 5.69272743
Iteration 259, loss = 5.69271777
Iteration 260, loss = 5.69224292
Iteration 261, loss = 5.69245009
Iteration 262, loss = 5.69250175
Iteration 263, loss = 5.69185666
Iteration 264, loss = 5.69177191
Iteration 265, loss = 5.69141327
Iteration 266, loss = 5.69112832
Iteration 267, loss = 5.69117181
Iteration 268, loss = 5.69108379
Iteration 269, loss = 5.69068328
Iteration 270, loss = 5.69048437
Iteration 271, loss = 5.69031384
Iteration 272, loss = 5.69014824
Iteration 273, loss = 5.68998456
Iteration 274, loss = 5.68967441
Iteration 275, loss = 5.68997804
Iteration 276, loss = 5.68972196
Iteration 277, loss = 5.68933224
Iteration 278, loss = 5.69000931
Iteration 279, loss = 5.68902819
Iteration 280, loss = 5.68887256
Iteration 281, loss = 5.68922156
Iteration 282, loss = 5.68892175
Iteration 283, loss = 5.68883781
Iteration 284, loss = 5.68836121
Iteration 285, loss = 5.68810315
Iteration 286, loss = 5.68790485
Iteration 287, loss = 5.68769141
Iteration 288, loss = 5.68755350
Iteration 289, loss = 5.68738847
Iteration 290, loss = 5.68770324
Iteration 291, loss = 5.68749262
Iteration 292, loss = 5.68696490
Iteration 293, loss = 5.68707346
Iteration 294, loss = 5.68659099
Iteration 295, loss = 5.68673045
Iteration 296, loss = 5.68663479
Iteration 297, loss = 5.68614764
Iteration 298, loss = 5.68612069
Iteration 299, loss = 5.68601664
Iteration 300, loss = 5.68596042
Iteration 301, loss = 5.68572596
Iteration 302, loss = 5.68543221
Iteration 303, loss = 5.68543211
Iteration 304, loss = 5.68525501
Iteration 305, loss = 5.68522213
Iteration 306, loss = 5.68537858
Iteration 307, loss = 5.68506614
Iteration 308, loss = 5.68482435
Iteration 309, loss = 5.68457830
Iteration 310, loss = 5.68510781
Iteration 311, loss = 5.68415203
Iteration 312, loss = 5.68404317
Iteration 313, loss = 5.68416230
Iteration 314, loss = 5.68375714
Iteration 315, loss = 5.68394929
Iteration 316, loss = 5.68341844
Iteration 317, loss = 5.68363253
Iteration 318, loss = 5.68332402
Iteration 319, loss = 5.68335314
Iteration 320, loss = 5.68309311
Iteration 321, loss = 5.68300445
Iteration 322, loss = 5.68278189
Iteration 323, loss = 5.68262103
Iteration 324, loss = 5.68304879
Iteration 325, loss = 5.68230069
Iteration 326, loss = 5.68227944
Iteration 327, loss = 5.68209100
Iteration 328, loss = 5.68214854
Iteration 329, loss = 5.68174293
Iteration 330, loss = 5.68203967
Iteration 331, loss = 5.68173876
Iteration 332, loss = 5.68184142
Iteration 333, loss = 5.68133663
Iteration 334, loss = 5.68135496
Iteration 335, loss = 5.68132537
Iteration 336, loss = 5.68109867
Iteration 337, loss = 5.68133605
Iteration 338, loss = 5.68091964
Iteration 339, loss = 5.68059855
Iteration 340, loss = 5.68071109
Iteration 341, loss = 5.68059458
Iteration 342, loss = 5.68029792
Iteration 343, loss = 5.68029915
Iteration 344, loss = 5.68042707
Iteration 345, loss = 5.67996469
Iteration 346, loss = 5.68023114
Iteration 347, loss = 5.67958931
Iteration 348, loss = 5.68103760
Iteration 349, loss = 5.67975323
Iteration 350, loss = 5.67948042
Iteration 351, loss = 5.67938063
Iteration 352, loss = 5.67921677
Iteration 353, loss = 5.67918136
Iteration 354, loss = 5.67915837
Iteration 355, loss = 5.67969011
Iteration 356, loss = 5.67880876
Iteration 357, loss = 5.67906867
Iteration 358, loss = 5.67902671
Iteration 359, loss = 5.67921286
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((1, 1, 1, 1, 1), {'error': 2.1742429493261781, 'fit': 0.021499999999999998, 'time': 430.8799999999974})
Iteration 1, loss = 6.51170863
Iteration 2, loss = 6.36565261
Iteration 3, loss = 6.20059501
Iteration 4, loss = 6.07446825
Iteration 5, loss = 5.99256539
Iteration 6, loss = 5.93863474
Iteration 7, loss = 5.89826246
Iteration 8, loss = 5.86473100
Iteration 9, loss = 5.83607802
Iteration 10, loss = 5.80963665
Iteration 11, loss = 5.78500037
Iteration 12, loss = 5.75898839
Iteration 13, loss = 5.73046061
Iteration 14, loss = 5.69991816
Iteration 15, loss = 5.66860385
Iteration 16, loss = 5.64019278
Iteration 17, loss = 5.61360217
Iteration 18, loss = 5.59056112
Iteration 19, loss = 5.57015877
Iteration 20, loss = 5.55131511
Iteration 21, loss = 5.53397256
Iteration 22, loss = 5.51821254
Iteration 23, loss = 5.50437471
Iteration 24, loss = 5.49052649
Iteration 25, loss = 5.47925904
Iteration 26, loss = 5.46691578
Iteration 27, loss = 5.45662163
Iteration 28, loss = 5.44687823
Iteration 29, loss = 5.43710343
Iteration 30, loss = 5.42837511
Iteration 31, loss = 5.42078539
Iteration 32, loss = 5.41187336
Iteration 33, loss = 5.40495909
Iteration 34, loss = 5.39724845
Iteration 35, loss = 5.39029708
Iteration 36, loss = 5.38401736
Iteration 37, loss = 5.37803138
Iteration 38, loss = 5.37133664
Iteration 39, loss = 5.36603593
Iteration 40, loss = 5.35986530
Iteration 41, loss = 5.35505863
Iteration 42, loss = 5.34907432
Iteration 43, loss = 5.34472950
Iteration 44, loss = 5.33995554
Iteration 45, loss = 5.33440488
Iteration 46, loss = 5.33008060
Iteration 47, loss = 5.32532083
Iteration 48, loss = 5.32058275
Iteration 49, loss = 5.31589098
Iteration 50, loss = 5.31194236
Iteration 51, loss = 5.30723655
Iteration 52, loss = 5.30277169
Iteration 53, loss = 5.29986195
Iteration 54, loss = 5.29527364
Iteration 55, loss = 5.29030685
Iteration 56, loss = 5.28790580
Iteration 57, loss = 5.28350289
Iteration 58, loss = 5.27914737
Iteration 59, loss = 5.27610330
Iteration 60, loss = 5.27321510
Iteration 61, loss = 5.26890713
Iteration 62, loss = 5.26521269
Iteration 63, loss = 5.26121377
Iteration 64, loss = 5.25797058
Iteration 65, loss = 5.25464662
Iteration 66, loss = 5.25063723
Iteration 67, loss = 5.24757997
Iteration 68, loss = 5.24537433
Iteration 69, loss = 5.24286109
Iteration 70, loss = 5.23926018
Iteration 71, loss = 5.23689524
Iteration 72, loss = 5.23335249
Iteration 73, loss = 5.22930984
Iteration 74, loss = 5.22654304
Iteration 75, loss = 5.22356242
Iteration 76, loss = 5.22103376
Iteration 77, loss = 5.21784937
Iteration 78, loss = 5.21483162
Iteration 79, loss = 5.21304390
Iteration 80, loss = 5.20940871
Iteration 81, loss = 5.20760873
Iteration 82, loss = 5.20486471
Iteration 83, loss = 5.20159775
Iteration 84, loss = 5.19996401
Iteration 85, loss = 5.19689002
Iteration 86, loss = 5.19360529
Iteration 87, loss = 5.19213838
Iteration 88, loss = 5.18931560
Iteration 89, loss = 5.18644902
Iteration 90, loss = 5.18367366
Iteration 91, loss = 5.18166006
Iteration 92, loss = 5.17916393
Iteration 93, loss = 5.17604624
Iteration 94, loss = 5.17419628
Iteration 95, loss = 5.17115547
Iteration 96, loss = 5.16867390
Iteration 97, loss = 5.16544010
Iteration 98, loss = 5.16396260
Iteration 99, loss = 5.16093009
Iteration 100, loss = 5.15795946
Iteration 101, loss = 5.15509400
Iteration 102, loss = 5.15362675
Iteration 103, loss = 5.14941360
Iteration 104, loss = 5.14805276
Iteration 105, loss = 5.14641928
Iteration 106, loss = 5.14281173
Iteration 107, loss = 5.13893522
Iteration 108, loss = 5.13719715
Iteration 109, loss = 5.13433788
Iteration 110, loss = 5.13150326
Iteration 111, loss = 5.12896140
Iteration 112, loss = 5.12735890
Iteration 113, loss = 5.12453312
Iteration 114, loss = 5.12048103
Iteration 115, loss = 5.11955814
Iteration 116, loss = 5.11678254
Iteration 117, loss = 5.11478970
Iteration 118, loss = 5.11392453
Iteration 119, loss = 5.11091898
Iteration 120, loss = 5.10705373
Iteration 121, loss = 5.10619925
Iteration 122, loss = 5.10467045
Iteration 123, loss = 5.10147699
Iteration 124, loss = 5.09919644
Iteration 125, loss = 5.09844040
Iteration 126, loss = 5.09553078
Iteration 127, loss = 5.09443478
Iteration 128, loss = 5.09119267
Iteration 129, loss = 5.09027960
Iteration 130, loss = 5.08746403
Iteration 131, loss = 5.08634411
Iteration 132, loss = 5.08492047
Iteration 133, loss = 5.08194043
Iteration 134, loss = 5.08281115
Iteration 135, loss = 5.07979920
Iteration 136, loss = 5.07808502
Iteration 137, loss = 5.07600851
Iteration 138, loss = 5.07381819
Iteration 139, loss = 5.07098890
Iteration 140, loss = 5.07004921
Iteration 141, loss = 5.06840730
Iteration 142, loss = 5.06722069
Iteration 143, loss = 5.06578255
Iteration 144, loss = 5.06310282
Iteration 145, loss = 5.06310744
Iteration 146, loss = 5.06202497
Iteration 147, loss = 5.05821791
Iteration 148, loss = 5.05735931
Iteration 149, loss = 5.05613105
Iteration 150, loss = 5.05537356
Iteration 151, loss = 5.05216080
Iteration 152, loss = 5.05010997
Iteration 153, loss = 5.04946888
Iteration 154, loss = 5.04792531
Iteration 155, loss = 5.04638455
Iteration 156, loss = 5.04409322
Iteration 157, loss = 5.04473232
Iteration 158, loss = 5.04257812
Iteration 159, loss = 5.04113622
Iteration 160, loss = 5.04244370
Iteration 161, loss = 5.03955516
Iteration 162, loss = 5.03610542
Iteration 163, loss = 5.03559032
Iteration 164, loss = 5.03506074
Iteration 165, loss = 5.03329670
Iteration 166, loss = 5.03218496
Iteration 167, loss = 5.03073388
Iteration 168, loss = 5.03002510
Iteration 169, loss = 5.02743593
Iteration 170, loss = 5.02544421
Iteration 171, loss = 5.02481271
Iteration 172, loss = 5.02310981
Iteration 173, loss = 5.01879351
Iteration 174, loss = 5.01768540
Iteration 175, loss = 5.01491277
Iteration 176, loss = 5.01087656
Iteration 177, loss = 5.00764368
Iteration 178, loss = 5.00479954
Iteration 179, loss = 5.00294495
Iteration 180, loss = 4.99986261
Iteration 181, loss = 4.99804650
Iteration 182, loss = 4.99212237
Iteration 183, loss = 4.99059550
Iteration 184, loss = 4.98625813
Iteration 185, loss = 4.98306192
Iteration 186, loss = 4.98030925
Iteration 187, loss = 4.97887233
Iteration 188, loss = 4.97596150
Iteration 189, loss = 4.97194959
Iteration 190, loss = 4.96993363
Iteration 191, loss = 4.96613531
Iteration 192, loss = 4.96463074
Iteration 193, loss = 4.96503170
Iteration 194, loss = 4.95775943
Iteration 195, loss = 4.95715075
Iteration 196, loss = 4.95132631
Iteration 197, loss = 4.94828545
Iteration 198, loss = 4.94631560
Iteration 199, loss = 4.94167993
Iteration 200, loss = 4.93941842
Iteration 201, loss = 4.93654436
Iteration 202, loss = 4.93270892
Iteration 203, loss = 4.92933378
Iteration 204, loss = 4.92662065
Iteration 205, loss = 4.92325907
Iteration 206, loss = 4.92074870
Iteration 207, loss = 4.91856061
Iteration 208, loss = 4.91636033
Iteration 209, loss = 4.91262157
Iteration 210, loss = 4.91222572
Iteration 211, loss = 4.90814577
Iteration 212, loss = 4.90387397
Iteration 213, loss = 4.90345511
Iteration 214, loss = 4.90139682
Iteration 215, loss = 4.89844924
Iteration 216, loss = 4.89565374
Iteration 217, loss = 4.89419775
Iteration 218, loss = 4.89235612
Iteration 219, loss = 4.88904273
Iteration 220, loss = 4.88772177
Iteration 221, loss = 4.88544037
Iteration 222, loss = 4.88604395
Iteration 223, loss = 4.88588793
Iteration 224, loss = 4.88319894
Iteration 225, loss = 4.88024715
Iteration 226, loss = 4.87990391
Iteration 227, loss = 4.87633497
Iteration 228, loss = 4.87754282
Iteration 229, loss = 4.87404475
Iteration 230, loss = 4.87059028
Iteration 231, loss = 4.86877907
Iteration 232, loss = 4.86644441
Iteration 233, loss = 4.86670898
Iteration 234, loss = 4.86677433
Iteration 235, loss = 4.86437673
Iteration 236, loss = 4.86171503
Iteration 237, loss = 4.86150940
Iteration 238, loss = 4.85940888
Iteration 239, loss = 4.85740820
Iteration 240, loss = 4.85727462
Iteration 241, loss = 4.85716153
Iteration 242, loss = 4.85410421
Iteration 243, loss = 4.85488615
Iteration 244, loss = 4.85429402
Iteration 245, loss = 4.85158794
Iteration 246, loss = 4.84795394
Iteration 247, loss = 4.84782627
Iteration 248, loss = 4.84642330
Iteration 249, loss = 4.84603556
Iteration 250, loss = 4.84518567
Iteration 251, loss = 4.84381268
Iteration 252, loss = 4.84241821
Iteration 253, loss = 4.84130986
Iteration 254, loss = 4.84207296
Iteration 255, loss = 4.84069193
Iteration 256, loss = 4.83876511
Iteration 257, loss = 4.83430030
Iteration 258, loss = 4.83347560
Iteration 259, loss = 4.83190125
Iteration 260, loss = 4.83243454
Iteration 261, loss = 4.83182576
Iteration 262, loss = 4.83132809
Iteration 263, loss = 4.82942433
Iteration 264, loss = 4.82899619
Iteration 265, loss = 4.82723140
Iteration 266, loss = 4.82786240
Iteration 267, loss = 4.82915293
Iteration 268, loss = 4.82673137
Iteration 269, loss = 4.82383926
Iteration 270, loss = 4.82371741
Iteration 271, loss = 4.82188469
Iteration 272, loss = 4.82088718
Iteration 273, loss = 4.81954276
Iteration 274, loss = 4.82145267
Iteration 275, loss = 4.81785038
Iteration 276, loss = 4.81591758
Iteration 277, loss = 4.81752965
Iteration 278, loss = 4.81423307
Iteration 279, loss = 4.81320642
Iteration 280, loss = 4.81280388
Iteration 281, loss = 4.81396716
Iteration 282, loss = 4.81211985
Iteration 283, loss = 4.81605276
Iteration 284, loss = 4.81079223
Iteration 285, loss = 4.81132790
Iteration 286, loss = 4.80936535
Iteration 287, loss = 4.80675072
Iteration 288, loss = 4.80702068
Iteration 289, loss = 4.80471872
Iteration 290, loss = 4.80845623
Iteration 291, loss = 4.80827178
Iteration 292, loss = 4.80373609
Iteration 293, loss = 4.80186492
Iteration 294, loss = 4.80326457
Iteration 295, loss = 4.80063365
Iteration 296, loss = 4.80118612
Iteration 297, loss = 4.79737063
Iteration 298, loss = 4.79711584
Iteration 299, loss = 4.79797281
Iteration 300, loss = 4.79679147
Iteration 301, loss = 4.79760684
Iteration 302, loss = 4.79916336
Iteration 303, loss = 4.79668604
Iteration 304, loss = 4.79663731
Iteration 305, loss = 4.79471604
Iteration 306, loss = 4.79413509
Iteration 307, loss = 4.79240729
Iteration 308, loss = 4.79122396
Iteration 309, loss = 4.79189214
Iteration 310, loss = 4.79346930
Iteration 311, loss = 4.79185165
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((10, 10, 10, 10, 10), {'error': 0.81656874602544838, 'fit': 0.037249999999999998, 'time': 411.0499999999993})
Iteration 1, loss = 6.17024044
Iteration 2, loss = 5.73201669
Iteration 3, loss = 5.55886856
Iteration 4, loss = 5.46334996
Iteration 5, loss = 5.40486099
Iteration 6, loss = 5.34314165
Iteration 7, loss = 5.29123458
Iteration 8, loss = 5.23156704
Iteration 9, loss = 5.18004819
Iteration 10, loss = 5.11932177
Iteration 11, loss = 5.06280045
Iteration 12, loss = 5.00627002
Iteration 13, loss = 4.95229686
Iteration 14, loss = 4.90693758
Iteration 15, loss = 4.86578642
Iteration 16, loss = 4.82262778
Iteration 17, loss = 4.78795594
Iteration 18, loss = 4.75203683
Iteration 19, loss = 4.71634670
Iteration 20, loss = 4.68467806
Iteration 21, loss = 4.65133247
Iteration 22, loss = 4.62710215
Iteration 23, loss = 4.59372074
Iteration 24, loss = 4.56770005
Iteration 25, loss = 4.54138913
Iteration 26, loss = 4.52316893
Iteration 27, loss = 4.49910637
Iteration 28, loss = 4.47288218
Iteration 29, loss = 4.44440608
Iteration 30, loss = 4.42746530
Iteration 31, loss = 4.39906047
Iteration 32, loss = 4.37580206
Iteration 33, loss = 4.35639927
Iteration 34, loss = 4.33935129
Iteration 35, loss = 4.32278985
Iteration 36, loss = 4.28940480
Iteration 37, loss = 4.26936515
Iteration 38, loss = 4.24484179
Iteration 39, loss = 4.22474990
Iteration 40, loss = 4.20276118
Iteration 41, loss = 4.18224243
Iteration 42, loss = 4.16477375
Iteration 43, loss = 4.14252373
Iteration 44, loss = 4.12123122
Iteration 45, loss = 4.10587233
Iteration 46, loss = 4.08171477
Iteration 47, loss = 4.07245311
Iteration 48, loss = 4.05018173
Iteration 49, loss = 4.02630951
Iteration 50, loss = 4.00024137
Iteration 51, loss = 3.99139330
Iteration 52, loss = 3.97052837
Iteration 53, loss = 3.94825214
Iteration 54, loss = 3.93185328
Iteration 55, loss = 3.91000659
Iteration 56, loss = 3.89738757
Iteration 57, loss = 3.87977533
Iteration 58, loss = 3.85421870
Iteration 59, loss = 3.84074797
Iteration 60, loss = 3.82164146
Iteration 61, loss = 3.80676937
Iteration 62, loss = 3.79048723
Iteration 63, loss = 3.76841003
Iteration 64, loss = 3.75174341
Iteration 65, loss = 3.72616779
Iteration 66, loss = 3.72209699
Iteration 67, loss = 3.69882662
Iteration 68, loss = 3.68251134
Iteration 69, loss = 3.67228367
Iteration 70, loss = 3.65068832
Iteration 71, loss = 3.62634964
Iteration 72, loss = 3.60864701
Iteration 73, loss = 3.59815598
Iteration 74, loss = 3.57739779
Iteration 75, loss = 3.56668042
Iteration 76, loss = 3.54715694
Iteration 77, loss = 3.52904338
Iteration 78, loss = 3.50632944
Iteration 79, loss = 3.49061889
Iteration 80, loss = 3.47746771
Iteration 81, loss = 3.45850106
Iteration 82, loss = 3.44567497
Iteration 83, loss = 3.43200174
Iteration 84, loss = 3.41276252
Iteration 85, loss = 3.39914094
Iteration 86, loss = 3.37878322
Iteration 87, loss = 3.36970489
Iteration 88, loss = 3.34684222
Iteration 89, loss = 3.33113828
Iteration 90, loss = 3.32024324
Iteration 91, loss = 3.29931378
Iteration 92, loss = 3.28413469
Iteration 93, loss = 3.27194530
Iteration 94, loss = 3.25199761
Iteration 95, loss = 3.23251579
Iteration 96, loss = 3.22449157
Iteration 97, loss = 3.20890062
Iteration 98, loss = 3.19448922
Iteration 99, loss = 3.17664167
Iteration 100, loss = 3.16695615
Iteration 101, loss = 3.14831803
Iteration 102, loss = 3.13458106
Iteration 103, loss = 3.12061460
Iteration 104, loss = 3.10625763
Iteration 105, loss = 3.08537789
Iteration 106, loss = 3.07917046
Iteration 107, loss = 3.05624111
Iteration 108, loss = 3.04403846
Iteration 109, loss = 3.03671406
Iteration 110, loss = 3.01015942
Iteration 111, loss = 3.00253261
Iteration 112, loss = 2.98227217
Iteration 113, loss = 2.96954097
Iteration 114, loss = 2.95417179
Iteration 115, loss = 2.93642652
Iteration 116, loss = 2.92356260
Iteration 117, loss = 2.91588491
Iteration 118, loss = 2.89771755
Iteration 119, loss = 2.88377216
Iteration 120, loss = 2.87400522
Iteration 121, loss = 2.86255854
Iteration 122, loss = 2.84673353
Iteration 123, loss = 2.83504894
Iteration 124, loss = 2.81016016
Iteration 125, loss = 2.79523378
Iteration 126, loss = 2.78286622
Iteration 127, loss = 2.76568078
Iteration 128, loss = 2.75973410
Iteration 129, loss = 2.73932242
Iteration 130, loss = 2.73324428
Iteration 131, loss = 2.72172394
Iteration 132, loss = 2.70681958
Iteration 133, loss = 2.68665145
Iteration 134, loss = 2.67576553
Iteration 135, loss = 2.66820302
Iteration 136, loss = 2.65657054
Iteration 137, loss = 2.64180103
Iteration 138, loss = 2.63825911
Iteration 139, loss = 2.61086953
Iteration 140, loss = 2.59898238
Iteration 141, loss = 2.58566004
Iteration 142, loss = 2.58160796
Iteration 143, loss = 2.56488916
Iteration 144, loss = 2.54370971
Iteration 145, loss = 2.53137317
Iteration 146, loss = 2.52655126
Iteration 147, loss = 2.52080082
Iteration 148, loss = 2.50027947
Iteration 149, loss = 2.49108345
Iteration 150, loss = 2.47859830
Iteration 151, loss = 2.46037077
Iteration 152, loss = 2.44150112
Iteration 153, loss = 2.42688510
Iteration 154, loss = 2.42140550
Iteration 155, loss = 2.40603459
Iteration 156, loss = 2.40523992
Iteration 157, loss = 2.38311886
Iteration 158, loss = 2.37634401
Iteration 159, loss = 2.36076168
Iteration 160, loss = 2.35313856
Iteration 161, loss = 2.33274112
Iteration 162, loss = 2.32629615
Iteration 163, loss = 2.31435446
Iteration 164, loss = 2.30780911
Iteration 165, loss = 2.29445661
Iteration 166, loss = 2.28197786
Iteration 167, loss = 2.26467259
Iteration 168, loss = 2.26185415
Iteration 169, loss = 2.25166664
Iteration 170, loss = 2.23026459
Iteration 171, loss = 2.21438918
Iteration 172, loss = 2.20567376
Iteration 173, loss = 2.19468909
Iteration 174, loss = 2.18660039
Iteration 175, loss = 2.17850935
Iteration 176, loss = 2.16902092
Iteration 177, loss = 2.15371432
Iteration 178, loss = 2.13397330
Iteration 179, loss = 2.12909486
Iteration 180, loss = 2.11377325
Iteration 181, loss = 2.10529921
Iteration 182, loss = 2.09897825
Iteration 183, loss = 2.09065948
Iteration 184, loss = 2.07039490
Iteration 185, loss = 2.05722220
Iteration 186, loss = 2.04919680
Iteration 187, loss = 2.03607686
Iteration 188, loss = 2.02931194
Iteration 189, loss = 2.02719506
Iteration 190, loss = 2.01247980
Iteration 191, loss = 1.99829873
Iteration 192, loss = 1.99510359
Iteration 193, loss = 1.98890136
Iteration 194, loss = 1.97432161
Iteration 195, loss = 1.95447419
Iteration 196, loss = 1.95014372
Iteration 197, loss = 1.93956146
Iteration 198, loss = 1.92937049
Iteration 199, loss = 1.91617246
Iteration 200, loss = 1.91309586
Iteration 201, loss = 1.89420270
Iteration 202, loss = 1.88789218
Iteration 203, loss = 1.87311219
Iteration 204, loss = 1.87257068
Iteration 205, loss = 1.85398739
Iteration 206, loss = 1.85258565
Iteration 207, loss = 1.84419359
Iteration 208, loss = 1.83347051
Iteration 209, loss = 1.82042118
Iteration 210, loss = 1.81331464
Iteration 211, loss = 1.79732691
Iteration 212, loss = 1.78233356
Iteration 213, loss = 1.77354435
Iteration 214, loss = 1.76426898
Iteration 215, loss = 1.75082112
Iteration 216, loss = 1.74736627
Iteration 217, loss = 1.74465121
Iteration 218, loss = 1.74797428
Iteration 219, loss = 1.71726692
Iteration 220, loss = 1.71154926
Iteration 221, loss = 1.69559375
Iteration 222, loss = 1.68910113
Iteration 223, loss = 1.67686671
Iteration 224, loss = 1.66815033
Iteration 225, loss = 1.66082531
Iteration 226, loss = 1.65521931
Iteration 227, loss = 1.64936124
Iteration 228, loss = 1.63920465
Iteration 229, loss = 1.62599678
Iteration 230, loss = 1.62826393
Iteration 231, loss = 1.61721164
Iteration 232, loss = 1.60539184
Iteration 233, loss = 1.58797966
Iteration 234, loss = 1.58780888
Iteration 235, loss = 1.57779921
Iteration 236, loss = 1.57461289
Iteration 237, loss = 1.55868859
Iteration 238, loss = 1.55543690
Iteration 239, loss = 1.54620483
Iteration 240, loss = 1.53123788
Iteration 241, loss = 1.52365208
Iteration 242, loss = 1.51824963
Iteration 243, loss = 1.51707539
Iteration 244, loss = 1.50993798
Iteration 245, loss = 1.50268125
Iteration 246, loss = 1.48586055
Iteration 247, loss = 1.47518610
Iteration 248, loss = 1.46642231
Iteration 249, loss = 1.46158481
Iteration 250, loss = 1.45151663
Iteration 251, loss = 1.44103451
Iteration 252, loss = 1.42907377
Iteration 253, loss = 1.41847748
Iteration 254, loss = 1.41604012
Iteration 255, loss = 1.40927442
Iteration 256, loss = 1.40860135
Iteration 257, loss = 1.40002503
Iteration 258, loss = 1.38852219
Iteration 259, loss = 1.37681882
Iteration 260, loss = 1.36709094
Iteration 261, loss = 1.36154102
Iteration 262, loss = 1.35970201
Iteration 263, loss = 1.34777674
Iteration 264, loss = 1.34053473
Iteration 265, loss = 1.33762593
Iteration 266, loss = 1.33347642
Iteration 267, loss = 1.32615524
Iteration 268, loss = 1.31295287
Iteration 269, loss = 1.30210006
Iteration 270, loss = 1.30013445
Iteration 271, loss = 1.29358981
Iteration 272, loss = 1.27793064
Iteration 273, loss = 1.27316452
Iteration 274, loss = 1.26609860
Iteration 275, loss = 1.26623179
Iteration 276, loss = 1.27453401
Iteration 277, loss = 1.24919934
Iteration 278, loss = 1.25214869
Iteration 279, loss = 1.23340509
Iteration 280, loss = 1.22720380
Iteration 281, loss = 1.22141729
Iteration 282, loss = 1.21698725
Iteration 283, loss = 1.21517657
Iteration 284, loss = 1.20375946
Iteration 285, loss = 1.19012189
Iteration 286, loss = 1.18171318
Iteration 287, loss = 1.17859686
Iteration 288, loss = 1.17612291
Iteration 289, loss = 1.18134688
Iteration 290, loss = 1.15852463
Iteration 291, loss = 1.14835142
Iteration 292, loss = 1.14039544
Iteration 293, loss = 1.13780034
Iteration 294, loss = 1.12531395
Iteration 295, loss = 1.12225954
Iteration 296, loss = 1.12431332
Iteration 297, loss = 1.12417350
Iteration 298, loss = 1.10694138
Iteration 299, loss = 1.11285344
Iteration 300, loss = 1.10196052
Iteration 301, loss = 1.08450164
Iteration 302, loss = 1.07524468
Iteration 303, loss = 1.07587453
Iteration 304, loss = 1.06813495
Iteration 305, loss = 1.07450748
Iteration 306, loss = 1.06611402
Iteration 307, loss = 1.05547656
Iteration 308, loss = 1.03998359
Iteration 309, loss = 1.03864192
Iteration 310, loss = 1.03282107
Iteration 311, loss = 1.02536556
Iteration 312, loss = 1.01985618
Iteration 313, loss = 1.01858892
Iteration 314, loss = 1.01031824
Iteration 315, loss = 1.01191226
Iteration 316, loss = 1.01154157
Iteration 317, loss = 0.99294566
Iteration 318, loss = 0.98808684
Iteration 319, loss = 0.98732388
Iteration 320, loss = 0.99185641
Iteration 321, loss = 0.97458146
Iteration 322, loss = 0.96738976
Iteration 323, loss = 0.95435943
Iteration 324, loss = 0.95357404
Iteration 325, loss = 0.95159424
Iteration 326, loss = 0.94586810
Iteration 327, loss = 0.94075528
Iteration 328, loss = 0.93424897
Iteration 329, loss = 0.92901023
Iteration 330, loss = 0.93539110
Iteration 331, loss = 0.94922470
Iteration 332, loss = 0.92122246
Iteration 333, loss = 0.90889800
Iteration 334, loss = 0.90252127
Iteration 335, loss = 0.90751567
Iteration 336, loss = 0.89952644
Iteration 337, loss = 0.89150108
Iteration 338, loss = 0.88424911
Iteration 339, loss = 0.87272914
Iteration 340, loss = 0.86858997
Iteration 341, loss = 0.86371116
Iteration 342, loss = 0.85536583
Iteration 343, loss = 0.85089288
Iteration 344, loss = 0.85427080
Iteration 345, loss = 0.84952297
Iteration 346, loss = 0.84304077
Iteration 347, loss = 0.83592408
Iteration 348, loss = 0.83253728
Iteration 349, loss = 0.83131259
Iteration 350, loss = 0.82030814
Iteration 351, loss = 0.82936456
Iteration 352, loss = 0.82138203
Iteration 353, loss = 0.81037187
Iteration 354, loss = 0.80382610
Iteration 355, loss = 0.80304239
Iteration 356, loss = 0.79926329
Iteration 357, loss = 0.80380818
Iteration 358, loss = 0.79113034
Iteration 359, loss = 0.78014785
Iteration 360, loss = 0.77434845
Iteration 361, loss = 0.77343635
Iteration 362, loss = 0.78194711
Iteration 363, loss = 0.76483597
Iteration 364, loss = 0.76321952
Iteration 365, loss = 0.76365106
Iteration 366, loss = 0.74995593
Iteration 367, loss = 0.74785733
Iteration 368, loss = 0.74770304
Iteration 369, loss = 0.74206732
Iteration 370, loss = 0.74689560
Iteration 371, loss = 0.72680358
Iteration 372, loss = 0.73014897
Iteration 373, loss = 0.72517724
Iteration 374, loss = 0.71660594
Iteration 375, loss = 0.70795263
Iteration 376, loss = 0.70119949
Iteration 377, loss = 0.70060981
Iteration 378, loss = 0.70829767
Iteration 379, loss = 0.70778108
Iteration 380, loss = 0.70594188
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100), {'error': 0.74879827525791576, 'fit': 0.89412499999999995, 'time': 845.8400000000001})
Iteration 1, loss = 6.09815183
Iteration 2, loss = 5.80542864
Iteration 3, loss = 5.67723350
Iteration 4, loss = 5.59764900
Iteration 5, loss = 5.52077786
Iteration 6, loss = 5.47576323
Iteration 7, loss = 5.39060509
Iteration 8, loss = 5.33490677
Iteration 9, loss = 5.26363272
Iteration 10, loss = 5.20656813
Iteration 11, loss = 5.12479401
Iteration 12, loss = 4.97008556
Iteration 13, loss = 4.86972530
Iteration 14, loss = 4.70673369
Iteration 15, loss = 4.52741822
Iteration 16, loss = 4.36437104
Iteration 17, loss = 4.19617524
Iteration 18, loss = 4.01988431
Iteration 19, loss = 3.80347904
Iteration 20, loss = 3.61834683
Iteration 21, loss = 3.44073302
Iteration 22, loss = 3.26605014
Iteration 23, loss = 3.05324914
Iteration 24, loss = 2.87647898
Iteration 25, loss = 2.69711327
Iteration 26, loss = 2.52877473
Iteration 27, loss = 2.35870805
Iteration 28, loss = 2.17249929
Iteration 29, loss = 2.02676141
Iteration 30, loss = 1.88104219
Iteration 31, loss = 1.75102352
Iteration 32, loss = 1.61409083
Iteration 33, loss = 1.48892784
Iteration 34, loss = 1.39823801
Iteration 35, loss = 1.27860458
Iteration 36, loss = 1.16778514
Iteration 37, loss = 1.03701539
Iteration 38, loss = 0.97479012
Iteration 39, loss = 0.89947102
Iteration 40, loss = 0.83117867
Iteration 41, loss = 0.77397939
Iteration 42, loss = 0.75848452
Iteration 43, loss = 0.66813200
Iteration 44, loss = 0.64562319
Iteration 45, loss = 0.54778044
Iteration 46, loss = 0.51075634
Iteration 47, loss = 0.50737813
Iteration 48, loss = 0.48290767
Iteration 49, loss = 0.45489874
Iteration 50, loss = 0.40634886
Iteration 51, loss = 0.38937726
Iteration 52, loss = 0.37282684
Iteration 53, loss = 0.35703452
Iteration 54, loss = 0.38479876
Iteration 55, loss = 0.33221598
Iteration 56, loss = 0.30569165
Iteration 57, loss = 0.33330717
Iteration 58, loss = 0.33589105
Iteration 59, loss = 0.32822993
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((1000, 1000, 1000, 1000, 1000), {'error': 0.84518391613395227, 'fit': 0.95999999999999996, 'time': 1834.25})
Iteration 1, loss = 6.53804451
Iteration 2, loss = 6.51238140
Iteration 3, loss = 6.48202050
Iteration 4, loss = 6.44714926
Iteration 5, loss = 6.40910162
Iteration 6, loss = 6.37010778
Iteration 7, loss = 6.33286395
Iteration 8, loss = 6.29805436
Iteration 9, loss = 6.26691471
Iteration 10, loss = 6.23993559
Iteration 11, loss = 6.21647415
Iteration 12, loss = 6.19579059
Iteration 13, loss = 6.17734439
Iteration 14, loss = 6.16070389
Iteration 15, loss = 6.14572755
Iteration 16, loss = 6.13187051
Iteration 17, loss = 6.11868367
Iteration 18, loss = 6.10598419
Iteration 19, loss = 6.09423278
Iteration 20, loss = 6.08347489
Iteration 21, loss = 6.07350459
Iteration 22, loss = 6.06420055
Iteration 23, loss = 6.05527264
Iteration 24, loss = 6.04673336
Iteration 25, loss = 6.03861055
Iteration 26, loss = 6.03067118
Iteration 27, loss = 6.02274350
Iteration 28, loss = 6.01482297
Iteration 29, loss = 6.00687197
Iteration 30, loss = 5.99879385
Iteration 31, loss = 5.99085431
Iteration 32, loss = 5.98291407
Iteration 33, loss = 5.97527330
Iteration 34, loss = 5.96795136
Iteration 35, loss = 5.96094161
Iteration 36, loss = 5.95436779
Iteration 37, loss = 5.94801381
Iteration 38, loss = 5.94211764
Iteration 39, loss = 5.93646275
Iteration 40, loss = 5.93110137
Iteration 41, loss = 5.92591848
Iteration 42, loss = 5.92103561
Iteration 43, loss = 5.91628290
Iteration 44, loss = 5.91175037
Iteration 45, loss = 5.90747657
Iteration 46, loss = 5.90317569
Iteration 47, loss = 5.89920332
Iteration 48, loss = 5.89517784
Iteration 49, loss = 5.89141057
Iteration 50, loss = 5.88768526
Iteration 51, loss = 5.88418317
Iteration 52, loss = 5.88071625
Iteration 53, loss = 5.87729152
Iteration 54, loss = 5.87396956
Iteration 55, loss = 5.87081064
Iteration 56, loss = 5.86764381
Iteration 57, loss = 5.86455335
Iteration 58, loss = 5.86146421
Iteration 59, loss = 5.85855750
Iteration 60, loss = 5.85558511
Iteration 61, loss = 5.85269434
Iteration 62, loss = 5.84986688
Iteration 63, loss = 5.84706632
Iteration 64, loss = 5.84425539
Iteration 65, loss = 5.84160583
Iteration 66, loss = 5.83884713
Iteration 67, loss = 5.83623988
Iteration 68, loss = 5.83370990
Iteration 69, loss = 5.83116502
Iteration 70, loss = 5.82873034
Iteration 71, loss = 5.82618184
Iteration 72, loss = 5.82383233
Iteration 73, loss = 5.82156616
Iteration 74, loss = 5.81949050
Iteration 75, loss = 5.81694118
Iteration 76, loss = 5.81470446
Iteration 77, loss = 5.81257296
Iteration 78, loss = 5.81040785
Iteration 79, loss = 5.80828856
Iteration 80, loss = 5.80625642
Iteration 81, loss = 5.80417915
Iteration 82, loss = 5.80220951
Iteration 83, loss = 5.80019452
Iteration 84, loss = 5.79838712
Iteration 85, loss = 5.79646105
Iteration 86, loss = 5.79459190
Iteration 87, loss = 5.79275904
Iteration 88, loss = 5.79108111
Iteration 89, loss = 5.78925983
Iteration 90, loss = 5.78754940
Iteration 91, loss = 5.78593873
Iteration 92, loss = 5.78424936
Iteration 93, loss = 5.78261253
Iteration 94, loss = 5.78111312
Iteration 95, loss = 5.77946965
Iteration 96, loss = 5.77782199
Iteration 97, loss = 5.77652055
Iteration 98, loss = 5.77486541
Iteration 99, loss = 5.77341881
Iteration 100, loss = 5.77209027
Iteration 101, loss = 5.77071308
Iteration 102, loss = 5.76932109
Iteration 103, loss = 5.76797065
Iteration 104, loss = 5.76672236
Iteration 105, loss = 5.76543848
Iteration 106, loss = 5.76403775
Iteration 107, loss = 5.76276229
Iteration 108, loss = 5.76155663
Iteration 109, loss = 5.76024420
Iteration 110, loss = 5.75918456
Iteration 111, loss = 5.75797698
Iteration 112, loss = 5.75725056
Iteration 113, loss = 5.75580553
Iteration 114, loss = 5.75470141
Iteration 115, loss = 5.75350646
Iteration 116, loss = 5.75252405
Iteration 117, loss = 5.75142567
Iteration 118, loss = 5.75048978
Iteration 119, loss = 5.74946110
Iteration 120, loss = 5.74846330
Iteration 121, loss = 5.74732458
Iteration 122, loss = 5.74640116
Iteration 123, loss = 5.74554676
Iteration 124, loss = 5.74444351
Iteration 125, loss = 5.74359089
Iteration 126, loss = 5.74268973
Iteration 127, loss = 5.74214436
Iteration 128, loss = 5.74103143
Iteration 129, loss = 5.74021304
Iteration 130, loss = 5.73931991
Iteration 131, loss = 5.73839059
Iteration 132, loss = 5.73769757
Iteration 133, loss = 5.73667766
Iteration 134, loss = 5.73584089
Iteration 135, loss = 5.73546696
Iteration 136, loss = 5.73446186
Iteration 137, loss = 5.73372483
Iteration 138, loss = 5.73303138
Iteration 139, loss = 5.73218150
Iteration 140, loss = 5.73150516
Iteration 141, loss = 5.73098818
Iteration 142, loss = 5.73031410
Iteration 143, loss = 5.72997213
Iteration 144, loss = 5.72873394
Iteration 145, loss = 5.72820554
Iteration 146, loss = 5.72781248
Iteration 147, loss = 5.72726462
Iteration 148, loss = 5.72619402
Iteration 149, loss = 5.72545966
Iteration 150, loss = 5.72504563
Iteration 151, loss = 5.72427559
Iteration 152, loss = 5.72369931
Iteration 153, loss = 5.72311388
Iteration 154, loss = 5.72257904
Iteration 155, loss = 5.72243143
Iteration 156, loss = 5.72159302
Iteration 157, loss = 5.72086244
Iteration 158, loss = 5.72065870
Iteration 159, loss = 5.71978347
Iteration 160, loss = 5.71958173
Iteration 161, loss = 5.71878885
Iteration 162, loss = 5.71860121
Iteration 163, loss = 5.71754711
Iteration 164, loss = 5.71762716
Iteration 165, loss = 5.71701271
Iteration 166, loss = 5.71637914
Iteration 167, loss = 5.71642433
Iteration 168, loss = 5.71563752
Iteration 169, loss = 5.71508524
Iteration 170, loss = 5.71480021
Iteration 171, loss = 5.71406440
Iteration 172, loss = 5.71365551
Iteration 173, loss = 5.71304563
Iteration 174, loss = 5.71266234
Iteration 175, loss = 5.71209840
Iteration 176, loss = 5.71158948
Iteration 177, loss = 5.71188279
Iteration 178, loss = 5.71111390
Iteration 179, loss = 5.71113338
Iteration 180, loss = 5.71019767
Iteration 181, loss = 5.71032741
Iteration 182, loss = 5.70972095
Iteration 183, loss = 5.70977292
Iteration 184, loss = 5.70930225
Iteration 185, loss = 5.70881370
Iteration 186, loss = 5.70818427
Iteration 187, loss = 5.70787276
Iteration 188, loss = 5.70732436
Iteration 189, loss = 5.70685714
Iteration 190, loss = 5.70661593
Iteration 191, loss = 5.70649733
Iteration 192, loss = 5.70619182
Iteration 193, loss = 5.70547423
Iteration 194, loss = 5.70540057
Iteration 195, loss = 5.70502169
Iteration 196, loss = 5.70496168
Iteration 197, loss = 5.70463060
Iteration 198, loss = 5.70418215
Iteration 199, loss = 5.70376603
Iteration 200, loss = 5.70320724
Iteration 201, loss = 5.70331528
Iteration 202, loss = 5.70334360
Iteration 203, loss = 5.70262959
Iteration 204, loss = 5.70222356
Iteration 205, loss = 5.70202545
Iteration 206, loss = 5.70181944
Iteration 207, loss = 5.70124320
Iteration 208, loss = 5.70098086
Iteration 209, loss = 5.70088801
Iteration 210, loss = 5.70035394
Iteration 211, loss = 5.70038551
Iteration 212, loss = 5.70000226
Iteration 213, loss = 5.70042504
Iteration 214, loss = 5.69953194
Iteration 215, loss = 5.69898348
Iteration 216, loss = 5.69892990
Iteration 217, loss = 5.69863812
Iteration 218, loss = 5.69853203
Iteration 219, loss = 5.69828685
Iteration 220, loss = 5.69787944
Iteration 221, loss = 5.69756124
Iteration 222, loss = 5.69721894
Iteration 223, loss = 5.69784239
Iteration 224, loss = 5.69699706
Iteration 225, loss = 5.69677541
Iteration 226, loss = 5.69652770
Iteration 227, loss = 5.69636066
Iteration 228, loss = 5.69631316
Iteration 229, loss = 5.69584297
Iteration 230, loss = 5.69578458
Iteration 231, loss = 5.69548904
Iteration 232, loss = 5.69512634
Iteration 233, loss = 5.69486496
Iteration 234, loss = 5.69457275
Iteration 235, loss = 5.69519934
Iteration 236, loss = 5.69399627
Iteration 237, loss = 5.69379101
Iteration 238, loss = 5.69369442
Iteration 239, loss = 5.69350958
Iteration 240, loss = 5.69397817
Iteration 241, loss = 5.69313107
Iteration 242, loss = 5.69429543
Iteration 243, loss = 5.69341101
Iteration 244, loss = 5.69294800
Iteration 245, loss = 5.69201476
Iteration 246, loss = 5.69209221
Iteration 247, loss = 5.69187151
Iteration 248, loss = 5.69191483
Iteration 249, loss = 5.69147121
Iteration 250, loss = 5.69187502
Iteration 251, loss = 5.69144612
Iteration 252, loss = 5.69090263
Iteration 253, loss = 5.69081830
Iteration 254, loss = 5.69116783
Iteration 255, loss = 5.69044560
Iteration 256, loss = 5.69052323
Iteration 257, loss = 5.68990646
Iteration 258, loss = 5.69015507
Iteration 259, loss = 5.68975428
Iteration 260, loss = 5.68980488
Iteration 261, loss = 5.68986628
Iteration 262, loss = 5.68965763
Iteration 263, loss = 5.68925226
Iteration 264, loss = 5.68880408
Iteration 265, loss = 5.68868843
Iteration 266, loss = 5.68902130
Iteration 267, loss = 5.68866049
Iteration 268, loss = 5.68996629
Iteration 269, loss = 5.68862962
Iteration 270, loss = 5.68747727
Iteration 271, loss = 5.68770232
Iteration 272, loss = 5.68754418
Iteration 273, loss = 5.68831081
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((1, 1, 1, 1, 1, 1), {'error': 2.2881283935068173, 'fit': 0.020875000000000001, 'time': 330.41999999999825})
Iteration 1, loss = 6.48911948
Iteration 2, loss = 6.32823129
Iteration 3, loss = 6.17816803
Iteration 4, loss = 6.09371941
Iteration 5, loss = 6.04965616
Iteration 6, loss = 5.98806085
Iteration 7, loss = 5.94042937
Iteration 8, loss = 5.90589688
Iteration 9, loss = 5.87374272
Iteration 10, loss = 5.84157591
Iteration 11, loss = 5.80705417
Iteration 12, loss = 5.76661047
Iteration 13, loss = 5.72436356
Iteration 14, loss = 5.68591566
Iteration 15, loss = 5.65021684
Iteration 16, loss = 5.61915304
Iteration 17, loss = 5.59109680
Iteration 18, loss = 5.56602456
Iteration 19, loss = 5.54303093
Iteration 20, loss = 5.52171964
Iteration 21, loss = 5.50274573
Iteration 22, loss = 5.48480488
Iteration 23, loss = 5.46902664
Iteration 24, loss = 5.45335512
Iteration 25, loss = 5.43885525
Iteration 26, loss = 5.42523689
Iteration 27, loss = 5.41427119
Iteration 28, loss = 5.40083647
Iteration 29, loss = 5.39041638
Iteration 30, loss = 5.37882988
Iteration 31, loss = 5.36766020
Iteration 32, loss = 5.35827969
Iteration 33, loss = 5.34894042
Iteration 34, loss = 5.34013688
Iteration 35, loss = 5.33062705
Iteration 36, loss = 5.32096925
Iteration 37, loss = 5.31405118
Iteration 38, loss = 5.30369022
Iteration 39, loss = 5.29484081
Iteration 40, loss = 5.28620887
Iteration 41, loss = 5.27464260
Iteration 42, loss = 5.26557447
Iteration 43, loss = 5.25569475
Iteration 44, loss = 5.24771268
Iteration 45, loss = 5.23975388
Iteration 46, loss = 5.23170735
Iteration 47, loss = 5.22270649
Iteration 48, loss = 5.21637633
Iteration 49, loss = 5.21113721
Iteration 50, loss = 5.20298897
Iteration 51, loss = 5.19652455
Iteration 52, loss = 5.19056740
Iteration 53, loss = 5.18500134
Iteration 54, loss = 5.17932942
Iteration 55, loss = 5.17287422
Iteration 56, loss = 5.16915761
Iteration 57, loss = 5.16333541
Iteration 58, loss = 5.15805129
Iteration 59, loss = 5.15453123
Iteration 60, loss = 5.14842671
Iteration 61, loss = 5.14545175
Iteration 62, loss = 5.14259224
Iteration 63, loss = 5.13996054
Iteration 64, loss = 5.13227494
Iteration 65, loss = 5.12787926
Iteration 66, loss = 5.12571846
Iteration 67, loss = 5.12181653
Iteration 68, loss = 5.11700445
Iteration 69, loss = 5.11401054
Iteration 70, loss = 5.11089279
Iteration 71, loss = 5.10841239
Iteration 72, loss = 5.10640830
Iteration 73, loss = 5.10238833
Iteration 74, loss = 5.10020942
Iteration 75, loss = 5.09647981
Iteration 76, loss = 5.09373439
Iteration 77, loss = 5.09065822
Iteration 78, loss = 5.08683204
Iteration 79, loss = 5.08459018
Iteration 80, loss = 5.08181471
Iteration 81, loss = 5.07935647
Iteration 82, loss = 5.07570195
Iteration 83, loss = 5.07363262
Iteration 84, loss = 5.07111468
Iteration 85, loss = 5.07047200
Iteration 86, loss = 5.06935170
Iteration 87, loss = 5.06632022
Iteration 88, loss = 5.06500179
Iteration 89, loss = 5.06186441
Iteration 90, loss = 5.05843743
Iteration 91, loss = 5.05627253
Iteration 92, loss = 5.05145651
Iteration 93, loss = 5.05211669
Iteration 94, loss = 5.05371452
Iteration 95, loss = 5.04725430
Iteration 96, loss = 5.04655842
Iteration 97, loss = 5.04597426
Iteration 98, loss = 5.04160474
Iteration 99, loss = 5.04130684
Iteration 100, loss = 5.03747348
Iteration 101, loss = 5.03514732
Iteration 102, loss = 5.03454649
Iteration 103, loss = 5.03255467
Iteration 104, loss = 5.02984443
Iteration 105, loss = 5.02909901
Iteration 106, loss = 5.02752961
Iteration 107, loss = 5.02570071
Iteration 108, loss = 5.02382688
Iteration 109, loss = 5.02269970
Iteration 110, loss = 5.01904960
Iteration 111, loss = 5.02109098
Iteration 112, loss = 5.01575808
Iteration 113, loss = 5.01426364
Iteration 114, loss = 5.01312498
Iteration 115, loss = 5.01345022
Iteration 116, loss = 5.01073556
Iteration 117, loss = 5.00985093
Iteration 118, loss = 5.00975373
Iteration 119, loss = 5.00614541
Iteration 120, loss = 5.00667405
Iteration 121, loss = 5.00286853
Iteration 122, loss = 5.00158308
Iteration 123, loss = 4.99933051
Iteration 124, loss = 4.99838231
Iteration 125, loss = 4.99824667
Iteration 126, loss = 4.99866577
Iteration 127, loss = 4.99611494
Iteration 128, loss = 4.99562968
Iteration 129, loss = 4.99128749
Iteration 130, loss = 4.99128376
Iteration 131, loss = 4.98951562
Iteration 132, loss = 4.99041842
Iteration 133, loss = 4.98964410
Iteration 134, loss = 4.98925776
Iteration 135, loss = 4.98505508
Iteration 136, loss = 4.98440162
Iteration 137, loss = 4.98211116
Iteration 138, loss = 4.98174301
Iteration 139, loss = 4.98030674
Iteration 140, loss = 4.97808972
Iteration 141, loss = 4.97848823
Iteration 142, loss = 4.97801060
Iteration 143, loss = 4.97465593
Iteration 144, loss = 4.97497780
Iteration 145, loss = 4.97301351
Iteration 146, loss = 4.97307885
Iteration 147, loss = 4.96981186
Iteration 148, loss = 4.97040100
Iteration 149, loss = 4.96832627
Iteration 150, loss = 4.96848120
Iteration 151, loss = 4.96705027
Iteration 152, loss = 4.96487712
Iteration 153, loss = 4.96441503
Iteration 154, loss = 4.96259667
Iteration 155, loss = 4.96174580
Iteration 156, loss = 4.96066902
Iteration 157, loss = 4.96355681
Iteration 158, loss = 4.95951065
Iteration 159, loss = 4.95572136
Iteration 160, loss = 4.95666163
Iteration 161, loss = 4.95667968
Iteration 162, loss = 4.95372558
Iteration 163, loss = 4.95424958
Iteration 164, loss = 4.95488422
Iteration 165, loss = 4.95096630
Iteration 166, loss = 4.95050948
Iteration 167, loss = 4.95184122
Iteration 168, loss = 4.95035970
Iteration 169, loss = 4.94840575
Iteration 170, loss = 4.94927751
Iteration 171, loss = 4.94575810
Iteration 172, loss = 4.94632889
Iteration 173, loss = 4.94658095
Iteration 174, loss = 4.94316732
Iteration 175, loss = 4.94368070
Iteration 176, loss = 4.94216432
Iteration 177, loss = 4.93912222
Iteration 178, loss = 4.93993738
Iteration 179, loss = 4.93993683
Iteration 180, loss = 4.93831645
Iteration 181, loss = 4.93631094
Iteration 182, loss = 4.93728438
Iteration 183, loss = 4.93500963
Iteration 184, loss = 4.93571583
Iteration 185, loss = 4.93628629
Iteration 186, loss = 4.93409276
Iteration 187, loss = 4.93354674
Iteration 188, loss = 4.93178665
Iteration 189, loss = 4.93119119
Iteration 190, loss = 4.92951669
Iteration 191, loss = 4.92964503
Iteration 192, loss = 4.93161519
Iteration 193, loss = 4.92770138
Iteration 194, loss = 4.92656241
Iteration 195, loss = 4.92467965
Iteration 196, loss = 4.92478866
Iteration 197, loss = 4.92663128
Iteration 198, loss = 4.92440587
Iteration 199, loss = 4.92442731
Iteration 200, loss = 4.92277889
Iteration 201, loss = 4.92156868
Iteration 202, loss = 4.92250528
Iteration 203, loss = 4.92109606
Iteration 204, loss = 4.92231136
Iteration 205, loss = 4.91972698
Iteration 206, loss = 4.91944470
Iteration 207, loss = 4.91802402
Iteration 208, loss = 4.91720010
Iteration 209, loss = 4.91744626
Iteration 210, loss = 4.91614885
Iteration 211, loss = 4.91592848
Iteration 212, loss = 4.91629287
Iteration 213, loss = 4.91264743
Iteration 214, loss = 4.91373774
Iteration 215, loss = 4.91101213
Iteration 216, loss = 4.91116543
Iteration 217, loss = 4.91290188
Iteration 218, loss = 4.91095847
Iteration 219, loss = 4.90918498
Iteration 220, loss = 4.90727731
Iteration 221, loss = 4.90647990
Iteration 222, loss = 4.90824799
Iteration 223, loss = 4.90668162
Iteration 224, loss = 4.90676262
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((10, 10, 10, 10, 10, 10), {'error': 1.1491317543895458, 'fit': 0.032125000000000001, 'time': 299.0699999999997})
Iteration 1, loss = 6.15699178
Iteration 2, loss = 5.72368194
Iteration 3, loss = 5.54918586
Iteration 4, loss = 5.45619470
Iteration 5, loss = 5.39360159
Iteration 6, loss = 5.32581186
Iteration 7, loss = 5.25373934
Iteration 8, loss = 5.18189076
Iteration 9, loss = 5.11150712
Iteration 10, loss = 5.04532426
Iteration 11, loss = 4.99639754
Iteration 12, loss = 4.93975094
Iteration 13, loss = 4.89867603
Iteration 14, loss = 4.85905007
Iteration 15, loss = 4.82366068
Iteration 16, loss = 4.79964999
Iteration 17, loss = 4.77141536
Iteration 18, loss = 4.74460077
Iteration 19, loss = 4.71786835
Iteration 20, loss = 4.69079017
Iteration 21, loss = 4.66214213
Iteration 22, loss = 4.64639364
Iteration 23, loss = 4.61844550
Iteration 24, loss = 4.60663507
Iteration 25, loss = 4.60004027
Iteration 26, loss = 4.55686800
Iteration 27, loss = 4.54027925
Iteration 28, loss = 4.51425779
Iteration 29, loss = 4.48878915
Iteration 30, loss = 4.47419561
Iteration 31, loss = 4.45969799
Iteration 32, loss = 4.45491792
Iteration 33, loss = 4.41958620
Iteration 34, loss = 4.39411423
Iteration 35, loss = 4.38815529
Iteration 36, loss = 4.36065459
Iteration 37, loss = 4.34690858
Iteration 38, loss = 4.32194438
Iteration 39, loss = 4.31041481
Iteration 40, loss = 4.29647306
Iteration 41, loss = 4.26956123
Iteration 42, loss = 4.25655550
Iteration 43, loss = 4.23026311
Iteration 44, loss = 4.21496565
Iteration 45, loss = 4.20287874
Iteration 46, loss = 4.18197391
Iteration 47, loss = 4.17025534
Iteration 48, loss = 4.15864208
Iteration 49, loss = 4.12885886
Iteration 50, loss = 4.12913998
Iteration 51, loss = 4.10778769
Iteration 52, loss = 4.11392824
Iteration 53, loss = 4.06828919
Iteration 54, loss = 4.04736263
Iteration 55, loss = 4.03161744
Iteration 56, loss = 4.01856588
Iteration 57, loss = 4.00053909
Iteration 58, loss = 3.97769575
Iteration 59, loss = 3.96135311
Iteration 60, loss = 3.94711595
Iteration 61, loss = 3.92634468
Iteration 62, loss = 3.91049823
Iteration 63, loss = 3.89491682
Iteration 64, loss = 3.87874453
Iteration 65, loss = 3.86753606
Iteration 66, loss = 3.85717873
Iteration 67, loss = 3.83539627
Iteration 68, loss = 3.80950378
Iteration 69, loss = 3.80939036
Iteration 70, loss = 3.79054918
Iteration 71, loss = 3.77084415
Iteration 72, loss = 3.75721377
Iteration 73, loss = 3.74171791
Iteration 74, loss = 3.72287183
Iteration 75, loss = 3.70016482
Iteration 76, loss = 3.69457630
Iteration 77, loss = 3.67213482
Iteration 78, loss = 3.66746499
Iteration 79, loss = 3.64945038
Iteration 80, loss = 3.63657858
Iteration 81, loss = 3.62510822
Iteration 82, loss = 3.60817712
Iteration 83, loss = 3.58141370
Iteration 84, loss = 3.59700851
Iteration 85, loss = 3.55843622
Iteration 86, loss = 3.54266863
Iteration 87, loss = 3.52419163
Iteration 88, loss = 3.51785294
Iteration 89, loss = 3.50851789
Iteration 90, loss = 3.48706622
Iteration 91, loss = 3.46880970
Iteration 92, loss = 3.45541519
Iteration 93, loss = 3.44427283
Iteration 94, loss = 3.42959465
Iteration 95, loss = 3.40427101
Iteration 96, loss = 3.38717593
Iteration 97, loss = 3.38997409
Iteration 98, loss = 3.37113131
Iteration 99, loss = 3.35369654
Iteration 100, loss = 3.33385006
Iteration 101, loss = 3.32187559
Iteration 102, loss = 3.31304130
Iteration 103, loss = 3.29425160
Iteration 104, loss = 3.28416889
Iteration 105, loss = 3.28324973
Iteration 106, loss = 3.27215631
Iteration 107, loss = 3.23836325
Iteration 108, loss = 3.21816615
Iteration 109, loss = 3.22043313
Iteration 110, loss = 3.20507572
Iteration 111, loss = 3.18146577
Iteration 112, loss = 3.16590944
Iteration 113, loss = 3.15958749
Iteration 114, loss = 3.14175806
Iteration 115, loss = 3.13304216
Iteration 116, loss = 3.11598901
Iteration 117, loss = 3.10128407
Iteration 118, loss = 3.09275917
Iteration 119, loss = 3.06689364
Iteration 120, loss = 3.06967815
Iteration 121, loss = 3.05510177
Iteration 122, loss = 3.03943130
Iteration 123, loss = 3.02286267
Iteration 124, loss = 3.02064906
Iteration 125, loss = 2.99979891
Iteration 126, loss = 2.98053056
Iteration 127, loss = 2.97063306
Iteration 128, loss = 2.96007389
Iteration 129, loss = 2.96619078
Iteration 130, loss = 2.93563846
Iteration 131, loss = 2.93075710
Iteration 132, loss = 2.90983448
Iteration 133, loss = 2.88908483
Iteration 134, loss = 2.87770936
Iteration 135, loss = 2.86437164
Iteration 136, loss = 2.86183659
Iteration 137, loss = 2.85433217
Iteration 138, loss = 2.83483294
Iteration 139, loss = 2.81068414
Iteration 140, loss = 2.80230848
Iteration 141, loss = 2.80136945
Iteration 142, loss = 2.78774993
Iteration 143, loss = 2.76639145
Iteration 144, loss = 2.76117203
Iteration 145, loss = 2.72763273
Iteration 146, loss = 2.73329435
Iteration 147, loss = 2.73684732
Iteration 148, loss = 2.73359435
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100), {'error': 0.68685981153381437, 'fit': 0.36887500000000001, 'time': 353.4800000000032})
Iteration 1, loss = 6.12989106
Iteration 2, loss = 5.83240094
Iteration 3, loss = 5.65136408
Iteration 4, loss = 5.52780398
Iteration 5, loss = 5.36344599
Iteration 6, loss = 5.20870328
Iteration 7, loss = 4.96490741
Iteration 8, loss = 4.76328151
Iteration 9, loss = 4.59707151
Iteration 10, loss = 4.43923252
Iteration 11, loss = 4.21347639
Iteration 12, loss = 4.05036713
Iteration 13, loss = 3.85569790
Iteration 14, loss = 3.67171665
Iteration 15, loss = 3.47596845
Iteration 16, loss = 3.34834007
Iteration 17, loss = 3.21039301
Iteration 18, loss = 3.00680708
Iteration 19, loss = 2.79072459
Iteration 20, loss = 2.63992835
Iteration 21, loss = 2.48231426
Iteration 22, loss = 2.35702652
Iteration 23, loss = 2.19870936
Iteration 24, loss = 2.07589780
Iteration 25, loss = 1.93671064
Iteration 26, loss = 1.74846567
Iteration 27, loss = 1.60040488
Iteration 28, loss = 1.53415492
Iteration 29, loss = 1.43615886
Iteration 30, loss = 1.31519197
Iteration 31, loss = 1.20757262
Iteration 32, loss = 1.09644634
Iteration 33, loss = 1.01458418
Iteration 34, loss = 0.91633156
Iteration 35, loss = 0.87907833
Iteration 36, loss = 0.86637966
Iteration 37, loss = 0.74353717
Iteration 38, loss = 0.71314777
Iteration 39, loss = 0.65297149
Iteration 40, loss = 0.58282948
Iteration 41, loss = 0.57875539
Iteration 42, loss = 0.57344715
Iteration 43, loss = 0.53985997
Iteration 44, loss = 0.48187601
Iteration 45, loss = 0.48348324
Iteration 46, loss = 0.53933810
Iteration 47, loss = 0.50226545
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((1000, 1000, 1000, 1000, 1000, 1000), {'error': 0.7593370267647932, 'fit': 0.92312499999999997, 'time': 1773.1800000000003})
Iteration 1, loss = 6.53009231
Iteration 2, loss = 6.49279876
Iteration 3, loss = 6.45823804
Iteration 4, loss = 6.42581205
Iteration 5, loss = 6.39540815
Iteration 6, loss = 6.36691095
Iteration 7, loss = 6.34027344
Iteration 8, loss = 6.31544715
Iteration 9, loss = 6.29213347
Iteration 10, loss = 6.27049880
Iteration 11, loss = 6.25022200
Iteration 12, loss = 6.23138065
Iteration 13, loss = 6.21380820
Iteration 14, loss = 6.19732159
Iteration 15, loss = 6.18209438
Iteration 16, loss = 6.16785163
Iteration 17, loss = 6.15462964
Iteration 18, loss = 6.14237176
Iteration 19, loss = 6.13088251
Iteration 20, loss = 6.12026660
Iteration 21, loss = 6.11047484
Iteration 22, loss = 6.10144678
Iteration 23, loss = 6.09293202
Iteration 24, loss = 6.08519880
Iteration 25, loss = 6.07804096
Iteration 26, loss = 6.07136063
Iteration 27, loss = 6.06521790
Iteration 28, loss = 6.05963754
Iteration 29, loss = 6.05443785
Iteration 30, loss = 6.04966315
Iteration 31, loss = 6.04537855
Iteration 32, loss = 6.04132988
Iteration 33, loss = 6.03771181
Iteration 34, loss = 6.03437724
Iteration 35, loss = 6.03128698
Iteration 36, loss = 6.02847856
Iteration 37, loss = 6.02594154
Iteration 38, loss = 6.02358978
Iteration 39, loss = 6.02152978
Iteration 40, loss = 6.01950885
Iteration 41, loss = 6.01782910
Iteration 42, loss = 6.01625098
Iteration 43, loss = 6.01475084
Iteration 44, loss = 6.01339172
Iteration 45, loss = 6.01216383
Iteration 46, loss = 6.01102774
Iteration 47, loss = 6.01005486
Iteration 48, loss = 6.00900527
Iteration 49, loss = 6.00818435
Iteration 50, loss = 6.00733788
Iteration 51, loss = 6.00657626
Iteration 52, loss = 6.00587860
Iteration 53, loss = 6.00528260
Iteration 54, loss = 6.00471050
Iteration 55, loss = 6.00411612
Iteration 56, loss = 6.00367269
Iteration 57, loss = 6.00315153
Iteration 58, loss = 6.00271638
Iteration 59, loss = 6.00222589
Iteration 60, loss = 6.00188876
Iteration 61, loss = 6.00151671
Iteration 62, loss = 6.00119529
Iteration 63, loss = 6.00090656
Iteration 64, loss = 6.00053393
Iteration 65, loss = 6.00025390
Iteration 66, loss = 6.00000473
Iteration 67, loss = 5.99974299
Iteration 68, loss = 5.99950009
Iteration 69, loss = 5.99935366
Iteration 70, loss = 5.99912226
Iteration 71, loss = 5.99889631
Iteration 72, loss = 5.99872777
Iteration 73, loss = 5.99849254
Iteration 74, loss = 5.99835951
Iteration 75, loss = 5.99819335
Iteration 76, loss = 5.99805067
Iteration 77, loss = 5.99794541
Iteration 78, loss = 5.99775550
Iteration 79, loss = 5.99762830
Iteration 80, loss = 5.99755632
Iteration 81, loss = 5.99738590
Iteration 82, loss = 5.99722734
Iteration 83, loss = 5.99717080
Iteration 84, loss = 5.99701421
Iteration 85, loss = 5.99695097
Iteration 86, loss = 5.99689954
Iteration 87, loss = 5.99679026
Iteration 88, loss = 5.99668385
Iteration 89, loss = 5.99658353
Iteration 90, loss = 5.99656591
Iteration 91, loss = 5.99645621
Iteration 92, loss = 5.99636159
Iteration 93, loss = 5.99632536
Iteration 94, loss = 5.99624991
Iteration 95, loss = 5.99620083
Iteration 96, loss = 5.99620285
Iteration 97, loss = 5.99608032
Iteration 98, loss = 5.99601474
Iteration 99, loss = 5.99598241
Iteration 100, loss = 5.99593740
Iteration 101, loss = 5.99591097
Iteration 102, loss = 5.99588544
Iteration 103, loss = 5.99582864
Iteration 104, loss = 5.99580290
Iteration 105, loss = 5.99574533
Iteration 106, loss = 5.99568191
Iteration 107, loss = 5.99570923
Iteration 108, loss = 5.99563299
Iteration 109, loss = 5.99564736
Iteration 110, loss = 5.99558229
Iteration 111, loss = 5.99553524
Iteration 112, loss = 5.99548233
Iteration 113, loss = 5.99550384
Iteration 114, loss = 5.99547395
Iteration 115, loss = 5.99549229
Iteration 116, loss = 5.99537272
Iteration 117, loss = 5.99536123
Iteration 118, loss = 5.99535037
Iteration 119, loss = 5.99529989
Iteration 120, loss = 5.99529982
Iteration 121, loss = 5.99531249
Iteration 122, loss = 5.99531102
Iteration 123, loss = 5.99527796
Iteration 124, loss = 5.99524621
Iteration 125, loss = 5.99522227
Iteration 126, loss = 5.99520546
Iteration 127, loss = 5.99519909
Iteration 128, loss = 5.99520935
Iteration 129, loss = 5.99517791
Iteration 130, loss = 5.99518028
Iteration 131, loss = 5.99515809
Iteration 132, loss = 5.99515657
Iteration 133, loss = 5.99504629
Iteration 134, loss = 5.99512951
Iteration 135, loss = 5.99507948
Iteration 136, loss = 5.99511137
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((1, 1, 1, 1, 1, 1, 1), {'error': 3.8585056800337765, 'fit': 0.016500000000000001, 'time': 155.25})
Iteration 1, loss = 6.50515658
Iteration 2, loss = 6.34616944
Iteration 3, loss = 6.18501156
Iteration 4, loss = 6.06546264
Iteration 5, loss = 5.99318058
Iteration 6, loss = 5.94515668
Iteration 7, loss = 5.91257389
Iteration 8, loss = 5.88701600
Iteration 9, loss = 5.86412648
Iteration 10, loss = 5.83903584
Iteration 11, loss = 5.81069384
Iteration 12, loss = 5.78166509
Iteration 13, loss = 5.75254088
Iteration 14, loss = 5.71916472
Iteration 15, loss = 5.68566470
Iteration 16, loss = 5.65224999
Iteration 17, loss = 5.62380509
Iteration 18, loss = 5.59519922
Iteration 19, loss = 5.57129752
Iteration 20, loss = 5.55107215
Iteration 21, loss = 5.53164170
Iteration 22, loss = 5.51263317
Iteration 23, loss = 5.49577441
Iteration 24, loss = 5.48121299
Iteration 25, loss = 5.46756269
Iteration 26, loss = 5.45430346
Iteration 27, loss = 5.44255514
Iteration 28, loss = 5.42914211
Iteration 29, loss = 5.41821580
Iteration 30, loss = 5.40697723
Iteration 31, loss = 5.39502857
Iteration 32, loss = 5.38584894
Iteration 33, loss = 5.37453740
Iteration 34, loss = 5.36517394
Iteration 35, loss = 5.35478757
Iteration 36, loss = 5.34698123
Iteration 37, loss = 5.33713098
Iteration 38, loss = 5.32759135
Iteration 39, loss = 5.32117045
Iteration 40, loss = 5.31219924
Iteration 41, loss = 5.30247096
Iteration 42, loss = 5.29675482
Iteration 43, loss = 5.28824589
Iteration 44, loss = 5.27953130
Iteration 45, loss = 5.27314835
Iteration 46, loss = 5.26428030
Iteration 47, loss = 5.25749809
Iteration 48, loss = 5.25205636
Iteration 49, loss = 5.24563911
Iteration 50, loss = 5.23946702
Iteration 51, loss = 5.23282539
Iteration 52, loss = 5.22829062
Iteration 53, loss = 5.22055766
Iteration 54, loss = 5.21633227
Iteration 55, loss = 5.21160765
Iteration 56, loss = 5.20776133
Iteration 57, loss = 5.20357773
Iteration 58, loss = 5.19668371
Iteration 59, loss = 5.19733140
Iteration 60, loss = 5.19124573
Iteration 61, loss = 5.18449987
Iteration 62, loss = 5.18167441
Iteration 63, loss = 5.17744461
Iteration 64, loss = 5.17277715
Iteration 65, loss = 5.16936068
Iteration 66, loss = 5.16647449
Iteration 67, loss = 5.16286202
Iteration 68, loss = 5.15924939
Iteration 69, loss = 5.15583553
Iteration 70, loss = 5.15200123
Iteration 71, loss = 5.14601736
Iteration 72, loss = 5.14040199
Iteration 73, loss = 5.13673530
Iteration 74, loss = 5.13275169
Iteration 75, loss = 5.12853772
Iteration 76, loss = 5.12664969
Iteration 77, loss = 5.12161834
Iteration 78, loss = 5.12188912
Iteration 79, loss = 5.11433207
Iteration 80, loss = 5.10968633
Iteration 81, loss = 5.10740932
Iteration 82, loss = 5.10355475
Iteration 83, loss = 5.10137959
Iteration 84, loss = 5.09805899
Iteration 85, loss = 5.09421436
Iteration 86, loss = 5.09283917
Iteration 87, loss = 5.08926626
Iteration 88, loss = 5.08395776
Iteration 89, loss = 5.08710230
Iteration 90, loss = 5.08071682
Iteration 91, loss = 5.08118247
Iteration 92, loss = 5.07608998
Iteration 93, loss = 5.07500257
Iteration 94, loss = 5.07306161
Iteration 95, loss = 5.06658507
Iteration 96, loss = 5.06641484
Iteration 97, loss = 5.06422428
Iteration 98, loss = 5.06197301
Iteration 99, loss = 5.06229759
Iteration 100, loss = 5.05709639
Iteration 101, loss = 5.05610951
Iteration 102, loss = 5.05553915
Iteration 103, loss = 5.05417080
Iteration 104, loss = 5.05415059
Iteration 105, loss = 5.04745823
Iteration 106, loss = 5.04324678
Iteration 107, loss = 5.04169147
Iteration 108, loss = 5.04070885
Iteration 109, loss = 5.03983468
Iteration 110, loss = 5.03621337
Iteration 111, loss = 5.03559244
Iteration 112, loss = 5.03107993
Iteration 113, loss = 5.03120380
Iteration 114, loss = 5.02955932
Iteration 115, loss = 5.02684435
Iteration 116, loss = 5.02757410
Iteration 117, loss = 5.02876768
Iteration 118, loss = 5.02262295
Iteration 119, loss = 5.02126607
Iteration 120, loss = 5.02102228
Iteration 121, loss = 5.02232990
Iteration 122, loss = 5.01504878
Iteration 123, loss = 5.01604428
Iteration 124, loss = 5.01383233
Iteration 125, loss = 5.01119293
Iteration 126, loss = 5.01077470
Iteration 127, loss = 5.00873926
Iteration 128, loss = 5.01335390
Iteration 129, loss = 5.01178261
Iteration 130, loss = 5.00571869
Iteration 131, loss = 5.00698088
Iteration 132, loss = 5.00279035
Iteration 133, loss = 5.00030280
Iteration 134, loss = 4.99779960
Iteration 135, loss = 4.99955681
Iteration 136, loss = 4.99810503
Iteration 137, loss = 4.99657400
Iteration 138, loss = 4.99434085
Iteration 139, loss = 4.99368047
Iteration 140, loss = 4.99335075
Iteration 141, loss = 4.99003811
Iteration 142, loss = 4.99056527
Iteration 143, loss = 4.99138959
Iteration 144, loss = 4.98816632
Iteration 145, loss = 4.98713204
Iteration 146, loss = 4.98595020
Iteration 147, loss = 4.98165488
Iteration 148, loss = 4.98396910
Iteration 149, loss = 4.98153406
Iteration 150, loss = 4.98144734
Iteration 151, loss = 4.97917951
Iteration 152, loss = 4.98285688
Iteration 153, loss = 4.97863732
Iteration 154, loss = 4.97624645
Iteration 155, loss = 4.97460211
Iteration 156, loss = 4.97336364
Iteration 157, loss = 4.97694385
Iteration 158, loss = 4.97083622
Iteration 159, loss = 4.97455337
Iteration 160, loss = 4.97085684
Iteration 161, loss = 4.96877290
Iteration 162, loss = 4.96774746
Iteration 163, loss = 4.96727572
Iteration 164, loss = 4.96901732
Iteration 165, loss = 4.97255474
Iteration 166, loss = 4.96534929
Iteration 167, loss = 4.96322264
Iteration 168, loss = 4.96266941
Iteration 169, loss = 4.96150881
Iteration 170, loss = 4.96114785
Iteration 171, loss = 4.95863662
Iteration 172, loss = 4.95843068
Iteration 173, loss = 4.95620712
Iteration 174, loss = 4.95847221
Iteration 175, loss = 4.95728342
Iteration 176, loss = 4.95425607
Iteration 177, loss = 4.95405839
Iteration 178, loss = 4.95278379
Iteration 179, loss = 4.95155773
Iteration 180, loss = 4.95211936
Iteration 181, loss = 4.95371088
Iteration 182, loss = 4.94910234
Iteration 183, loss = 4.94697615
Iteration 184, loss = 4.94932515
Iteration 185, loss = 4.94620213
Iteration 186, loss = 4.94894781
Iteration 187, loss = 4.94943629
Iteration 188, loss = 4.94161319
Iteration 189, loss = 4.94618716
Iteration 190, loss = 4.94585864
Iteration 191, loss = 4.94383139
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((10, 10, 10, 10, 10, 10, 10), {'error': 1.19436527341259, 'fit': 0.030124999999999999, 'time': 259.3899999999994})
Iteration 1, loss = 6.16670739
Iteration 2, loss = 5.71864107
Iteration 3, loss = 5.54224938
Iteration 4, loss = 5.45853579
Iteration 5, loss = 5.38658686
Iteration 6, loss = 5.30629164
Iteration 7, loss = 5.22520548
Iteration 8, loss = 5.14810015
Iteration 9, loss = 5.08333164
Iteration 10, loss = 5.01220029
Iteration 11, loss = 4.96131799
Iteration 12, loss = 4.91925773
Iteration 13, loss = 4.88965008
Iteration 14, loss = 4.86101335
Iteration 15, loss = 4.81532081
Iteration 16, loss = 4.79227082
Iteration 17, loss = 4.76636972
Iteration 18, loss = 4.73657002
Iteration 19, loss = 4.71520134
Iteration 20, loss = 4.69334513
Iteration 21, loss = 4.67327401
Iteration 22, loss = 4.65655713
Iteration 23, loss = 4.62902315
Iteration 24, loss = 4.60392460
Iteration 25, loss = 4.58774573
Iteration 26, loss = 4.58136650
Iteration 27, loss = 4.56490587
Iteration 28, loss = 4.53219216
Iteration 29, loss = 4.50602823
Iteration 30, loss = 4.50318436
Iteration 31, loss = 4.48084123
Iteration 32, loss = 4.45283313
Iteration 33, loss = 4.44252415
Iteration 34, loss = 4.42382789
Iteration 35, loss = 4.40489762
Iteration 36, loss = 4.39013601
Iteration 37, loss = 4.36300919
Iteration 38, loss = 4.34950244
Iteration 39, loss = 4.34029708
Iteration 40, loss = 4.31499708
Iteration 41, loss = 4.30696675
Iteration 42, loss = 4.28229927
Iteration 43, loss = 4.28558197
Iteration 44, loss = 4.25651489
Iteration 45, loss = 4.23797053
Iteration 46, loss = 4.22184506
Iteration 47, loss = 4.20379768
Iteration 48, loss = 4.18923907
Iteration 49, loss = 4.16647269
Iteration 50, loss = 4.16486490
Iteration 51, loss = 4.14404623
Iteration 52, loss = 4.13394859
Iteration 53, loss = 4.10664004
Iteration 54, loss = 4.09230838
Iteration 55, loss = 4.07568229
Iteration 56, loss = 4.07097295
Iteration 57, loss = 4.06696782
Iteration 58, loss = 4.03652938
Iteration 59, loss = 4.02129855
Iteration 60, loss = 4.00826308
Iteration 61, loss = 3.98583491
Iteration 62, loss = 3.97753799
Iteration 63, loss = 3.95781134
Iteration 64, loss = 3.94522630
Iteration 65, loss = 3.92884648
Iteration 66, loss = 3.90995685
Iteration 67, loss = 3.90280464
Iteration 68, loss = 3.89635775
Iteration 69, loss = 3.87506627
Iteration 70, loss = 3.86187686
Iteration 71, loss = 3.83315748
Iteration 72, loss = 3.83148895
Iteration 73, loss = 3.81593725
Iteration 74, loss = 3.79697867
Iteration 75, loss = 3.77760963
Iteration 76, loss = 3.76449217
Iteration 77, loss = 3.74288086
Iteration 78, loss = 3.74128031
Iteration 79, loss = 3.72422767
Iteration 80, loss = 3.69936422
Iteration 81, loss = 3.70147765
Iteration 82, loss = 3.69080461
Iteration 83, loss = 3.68025212
Iteration 84, loss = 3.65264268
Iteration 85, loss = 3.65965686
Iteration 86, loss = 3.64129648
Iteration 87, loss = 3.63122693
Iteration 88, loss = 3.59997697
Iteration 89, loss = 3.59981308
Iteration 90, loss = 3.57476568
Iteration 91, loss = 3.55255500
Iteration 92, loss = 3.54510509
Iteration 93, loss = 3.53529138
Iteration 94, loss = 3.53119802
Iteration 95, loss = 3.49571191
Iteration 96, loss = 3.50614235
Iteration 97, loss = 3.47741508
Iteration 98, loss = 3.45915029
Iteration 99, loss = 3.45344841
Iteration 100, loss = 3.43619666
Iteration 101, loss = 3.41213290
Iteration 102, loss = 3.40860382
Iteration 103, loss = 3.39717497
Iteration 104, loss = 3.38753367
Iteration 105, loss = 3.38538377
Iteration 106, loss = 3.36345912
Iteration 107, loss = 3.35606570
Iteration 108, loss = 3.33135462
Iteration 109, loss = 3.31352563
Iteration 110, loss = 3.28979598
Iteration 111, loss = 3.28151107
Iteration 112, loss = 3.27939032
Iteration 113, loss = 3.26007519
Iteration 114, loss = 3.23869649
Iteration 115, loss = 3.24765985
Iteration 116, loss = 3.23348063
Iteration 117, loss = 3.22483643
Iteration 118, loss = 3.19722189
Iteration 119, loss = 3.19512336
Iteration 120, loss = 3.18087003
Iteration 121, loss = 3.15229915
Iteration 122, loss = 3.14893521
Iteration 123, loss = 3.13455976
Iteration 124, loss = 3.12675716
Iteration 125, loss = 3.11520411
Iteration 126, loss = 3.09384754
Iteration 127, loss = 3.08467680
Iteration 128, loss = 3.08859105
Iteration 129, loss = 3.07688069
Iteration 130, loss = 3.04310836
Iteration 131, loss = 3.02255450
Iteration 132, loss = 3.02174840
Iteration 133, loss = 3.01737840
Iteration 134, loss = 2.99810284
Iteration 135, loss = 2.97788923
Iteration 136, loss = 2.97070509
Iteration 137, loss = 2.95494881
Iteration 138, loss = 2.93915147
Iteration 139, loss = 2.95935492
Iteration 140, loss = 2.92434952
Iteration 141, loss = 2.89677138
Iteration 142, loss = 2.89306265
Iteration 143, loss = 2.88714471
Iteration 144, loss = 2.86510029
Iteration 145, loss = 2.86352861
Iteration 146, loss = 2.83394797
Iteration 147, loss = 2.83866609
Iteration 148, loss = 2.81583314
Iteration 149, loss = 2.81830492
Iteration 150, loss = 2.80000823
Iteration 151, loss = 2.78458108
Iteration 152, loss = 2.78447820
Iteration 153, loss = 2.76754459
Iteration 154, loss = 2.74532971
Iteration 155, loss = 2.74756658
Iteration 156, loss = 2.74923352
Iteration 157, loss = 2.72708755
Iteration 158, loss = 2.70797160
Iteration 159, loss = 2.70250594
Iteration 160, loss = 2.71950053
Iteration 161, loss = 2.66366686
Iteration 162, loss = 2.64854104
Iteration 163, loss = 2.63220388
Iteration 164, loss = 2.63465997
Iteration 165, loss = 2.62302217
Iteration 166, loss = 2.61534937
Iteration 167, loss = 2.60179631
Iteration 168, loss = 2.59606615
Iteration 169, loss = 2.59129854
Iteration 170, loss = 2.56959840
Iteration 171, loss = 2.56019550
Iteration 172, loss = 2.55021651
Iteration 173, loss = 2.53216418
Iteration 174, loss = 2.53129165
Iteration 175, loss = 2.51562444
Iteration 176, loss = 2.48718289
Iteration 177, loss = 2.52141994
Iteration 178, loss = 2.50834893
Iteration 179, loss = 2.47043337
Iteration 180, loss = 2.46398140
Iteration 181, loss = 2.45199235
Iteration 182, loss = 2.43736328
Iteration 183, loss = 2.42084894
Iteration 184, loss = 2.40270433
Iteration 185, loss = 2.40630468
Iteration 186, loss = 2.38763780
Iteration 187, loss = 2.37291291
Iteration 188, loss = 2.36269017
Iteration 189, loss = 2.34757354
Iteration 190, loss = 2.35824386
Iteration 191, loss = 2.34335429
Iteration 192, loss = 2.34348931
Iteration 193, loss = 2.31783598
Iteration 194, loss = 2.30991551
Iteration 195, loss = 2.29515384
Iteration 196, loss = 2.28456065
Iteration 197, loss = 2.28823208
Iteration 198, loss = 2.27392458
Iteration 199, loss = 2.25888055
Iteration 200, loss = 2.24224207
Iteration 201, loss = 2.23497881
Iteration 202, loss = 2.21238087
Iteration 203, loss = 2.20628327
Iteration 204, loss = 2.20474765
Iteration 205, loss = 2.20517712
Iteration 206, loss = 2.18662100
Iteration 207, loss = 2.17795077
Iteration 208, loss = 2.16833817
Iteration 209, loss = 2.14955231
Iteration 210, loss = 2.14523155
Iteration 211, loss = 2.13159916
Iteration 212, loss = 2.16840099
Iteration 213, loss = 2.14261562
Iteration 214, loss = 2.11313067
Iteration 215, loss = 2.11683919
Iteration 216, loss = 2.10651183
Iteration 217, loss = 2.08255725
Iteration 218, loss = 2.06763311
Iteration 219, loss = 2.05222048
Iteration 220, loss = 2.04132570
Iteration 221, loss = 2.03562622
Iteration 222, loss = 2.01247721
Iteration 223, loss = 2.00901001
Iteration 224, loss = 2.03821694
Iteration 225, loss = 1.99347775
Iteration 226, loss = 1.98010688
Iteration 227, loss = 2.00880484
Iteration 228, loss = 1.96929324
Iteration 229, loss = 1.97750482
Iteration 230, loss = 1.95581550
Iteration 231, loss = 1.95931663
Iteration 232, loss = 1.93591159
Iteration 233, loss = 1.91354397
Iteration 234, loss = 1.89983779
Iteration 235, loss = 1.91484255
Iteration 236, loss = 1.91791495
Iteration 237, loss = 1.90973383
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100), {'error': 0.7222167673632065, 'fit': 0.57099999999999995, 'time': 605.4100000000035})
Iteration 1, loss = 6.12243535
Iteration 2, loss = 5.82070099
Iteration 3, loss = 5.66513814
Iteration 4, loss = 5.55566128
Iteration 5, loss = 5.43215556
Iteration 6, loss = 5.36314564
Iteration 7, loss = 5.27683265
Iteration 8, loss = 5.14263679
Iteration 9, loss = 5.10893002
Iteration 10, loss = 5.05373687
Iteration 11, loss = 4.95423964
Iteration 12, loss = 4.82778117
Iteration 13, loss = 4.76081740
Iteration 14, loss = 4.65760532
Iteration 15, loss = 4.62392379
Iteration 16, loss = 4.51155975
Iteration 17, loss = 4.41889319
Iteration 18, loss = 4.33203892
Iteration 19, loss = 4.31714092
Iteration 20, loss = 4.20527135
Iteration 21, loss = 4.05580293
Iteration 22, loss = 3.98388005
Iteration 23, loss = 3.89849313
Iteration 24, loss = 3.90151212
Iteration 25, loss = 3.79598374
Iteration 26, loss = 3.65981189
Iteration 27, loss = 3.56792012
Iteration 28, loss = 3.48478277
Iteration 29, loss = 3.40991177
Iteration 30, loss = 3.38163685
Iteration 31, loss = 3.24795400
Iteration 32, loss = 3.19405638
Iteration 33, loss = 3.13381038
Iteration 34, loss = 2.96232650
Iteration 35, loss = 2.88985831
Iteration 36, loss = 2.84010566
Iteration 37, loss = 2.76517436
Iteration 38, loss = 2.71529506
Iteration 39, loss = 2.58973261
Iteration 40, loss = 2.49549763
Iteration 41, loss = 2.42380455
Iteration 42, loss = 2.38002400
Iteration 43, loss = 2.34194968
Iteration 44, loss = 2.31934781
Iteration 45, loss = 2.24179901
Iteration 46, loss = 2.30792905
Iteration 47, loss = 2.20782556
Iteration 48, loss = 2.06789230
Iteration 49, loss = 2.03608363
Iteration 50, loss = 1.94564131
Iteration 51, loss = 1.81817868
Iteration 52, loss = 1.73576991
Iteration 53, loss = 1.66001986
Iteration 54, loss = 1.64966077
Iteration 55, loss = 1.57011998
Iteration 56, loss = 1.55991964
Iteration 57, loss = 1.47594534
Iteration 58, loss = 1.41945287
Iteration 59, loss = 1.37631244
Iteration 60, loss = 1.30711848
Iteration 61, loss = 1.35376738
Iteration 62, loss = 1.41703966
Iteration 63, loss = 1.26572064
Iteration 64, loss = 1.14315451
Iteration 65, loss = 1.17543036
Iteration 66, loss = 1.10050254
Iteration 67, loss = 1.01617243
Iteration 68, loss = 0.98133126
Iteration 69, loss = 1.01454192
Iteration 70, loss = 0.97698356
Iteration 71, loss = 0.99262024
Iteration 72, loss = 0.93293010
Iteration 73, loss = 0.85279477
Iteration 74, loss = 0.92737236
Iteration 75, loss = 1.04944677
Iteration 76, loss = 0.96508924
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((1000, 1000, 1000, 1000, 1000, 1000, 1000), {'error': 0.70153030381299453, 'fit': 0.81137499999999996, 'time': 3298.3600000000006})
Iteration 1, loss = 6.53157568
Iteration 2, loss = 6.49468004
Iteration 3, loss = 6.46013315
Iteration 4, loss = 6.42742553
Iteration 5, loss = 6.39664129
Iteration 6, loss = 6.36771807
Iteration 7, loss = 6.34078824
Iteration 8, loss = 6.31561318
Iteration 9, loss = 6.29212263
Iteration 10, loss = 6.27009199
Iteration 11, loss = 6.24976874
Iteration 12, loss = 6.23071477
Iteration 13, loss = 6.21299922
Iteration 14, loss = 6.19656676
Iteration 15, loss = 6.18126327
Iteration 16, loss = 6.16694319
Iteration 17, loss = 6.15379688
Iteration 18, loss = 6.14148030
Iteration 19, loss = 6.13006509
Iteration 20, loss = 6.11950560
Iteration 21, loss = 6.10970904
Iteration 22, loss = 6.10065150
Iteration 23, loss = 6.09227713
Iteration 24, loss = 6.08456585
Iteration 25, loss = 6.07738837
Iteration 26, loss = 6.07081014
Iteration 27, loss = 6.06470432
Iteration 28, loss = 6.05920420
Iteration 29, loss = 6.05398353
Iteration 30, loss = 6.04930171
Iteration 31, loss = 6.04494967
Iteration 32, loss = 6.04105436
Iteration 33, loss = 6.03738426
Iteration 34, loss = 6.03411473
Iteration 35, loss = 6.03108017
Iteration 36, loss = 6.02827179
Iteration 37, loss = 6.02580698
Iteration 38, loss = 6.02347421
Iteration 39, loss = 6.02139657
Iteration 40, loss = 6.01945041
Iteration 41, loss = 6.01768773
Iteration 42, loss = 6.01607882
Iteration 43, loss = 6.01465780
Iteration 44, loss = 6.01330718
Iteration 45, loss = 6.01202231
Iteration 46, loss = 6.01093490
Iteration 47, loss = 6.00988134
Iteration 48, loss = 6.00896948
Iteration 49, loss = 6.00809330
Iteration 50, loss = 6.00722299
Iteration 51, loss = 6.00649839
Iteration 52, loss = 6.00582364
Iteration 53, loss = 6.00516005
Iteration 54, loss = 6.00462568
Iteration 55, loss = 6.00406278
Iteration 56, loss = 6.00358130
Iteration 57, loss = 6.00303033
Iteration 58, loss = 6.00263158
Iteration 59, loss = 6.00218606
Iteration 60, loss = 6.00184621
Iteration 61, loss = 6.00150850
Iteration 62, loss = 6.00116156
Iteration 63, loss = 6.00086474
Iteration 64, loss = 6.00054119
Iteration 65, loss = 6.00021832
Iteration 66, loss = 5.99997414
Iteration 67, loss = 5.99978210
Iteration 68, loss = 5.99947436
Iteration 69, loss = 5.99925654
Iteration 70, loss = 5.99906843
Iteration 71, loss = 5.99881035
Iteration 72, loss = 5.99868377
Iteration 73, loss = 5.99848136
Iteration 74, loss = 5.99827683
Iteration 75, loss = 5.99814811
Iteration 76, loss = 5.99801156
Iteration 77, loss = 5.99785617
Iteration 78, loss = 5.99770963
Iteration 79, loss = 5.99764328
Iteration 80, loss = 5.99745765
Iteration 81, loss = 5.99737251
Iteration 82, loss = 5.99724093
Iteration 83, loss = 5.99712809
Iteration 84, loss = 5.99704671
Iteration 85, loss = 5.99701109
Iteration 86, loss = 5.99685692
Iteration 87, loss = 5.99677877
Iteration 88, loss = 5.99664251
Iteration 89, loss = 5.99659487
Iteration 90, loss = 5.99654616
Iteration 91, loss = 5.99644055
Iteration 92, loss = 5.99639400
Iteration 93, loss = 5.99633239
Iteration 94, loss = 5.99624981
Iteration 95, loss = 5.99620849
Iteration 96, loss = 5.99613186
Iteration 97, loss = 5.99607213
Iteration 98, loss = 5.99603812
Iteration 99, loss = 5.99601588
Iteration 100, loss = 5.99597890
Iteration 101, loss = 5.99591549
Iteration 102, loss = 5.99582181
Iteration 103, loss = 5.99588588
Iteration 104, loss = 5.99576725
Iteration 105, loss = 5.99573518
Iteration 106, loss = 5.99569521
Iteration 107, loss = 5.99569882
Iteration 108, loss = 5.99559287
Iteration 109, loss = 5.99559487
Iteration 110, loss = 5.99553455
Iteration 111, loss = 5.99555428
Iteration 112, loss = 5.99549161
Iteration 113, loss = 5.99545420
Iteration 114, loss = 5.99540438
Iteration 115, loss = 5.99545028
Iteration 116, loss = 5.99539277
Iteration 117, loss = 5.99536894
Iteration 118, loss = 5.99534346
Iteration 119, loss = 5.99533291
Iteration 120, loss = 5.99533823
Iteration 121, loss = 5.99531647
Iteration 122, loss = 5.99531108
Iteration 123, loss = 5.99523206
Iteration 124, loss = 5.99529364
Iteration 125, loss = 5.99521914
Iteration 126, loss = 5.99518749
Iteration 127, loss = 5.99521420
Iteration 128, loss = 5.99515073
Iteration 129, loss = 5.99517461
Iteration 130, loss = 5.99511113
Iteration 131, loss = 5.99517824
Iteration 132, loss = 5.99517869
Iteration 133, loss = 5.99506050
Iteration 134, loss = 5.99507373
Iteration 135, loss = 5.99509399
Iteration 136, loss = 5.99507006
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((1, 1, 1, 1, 1, 1, 1, 1), {'error': 3.8585056800337765, 'fit': 0.016500000000000001, 'time': 157.4100000000035})
Iteration 1, loss = 6.49192856
Iteration 2, loss = 6.31892372
Iteration 3, loss = 6.17686641
Iteration 4, loss = 6.08791111
Iteration 5, loss = 6.01386226
Iteration 6, loss = 5.96271172
Iteration 7, loss = 5.92242226
Iteration 8, loss = 5.88911552
Iteration 9, loss = 5.85864816
Iteration 10, loss = 5.82718949
Iteration 11, loss = 5.79395260
Iteration 12, loss = 5.76007839
Iteration 13, loss = 5.72426320
Iteration 14, loss = 5.68797815
Iteration 15, loss = 5.65305414
Iteration 16, loss = 5.61975051
Iteration 17, loss = 5.58960051
Iteration 18, loss = 5.56254069
Iteration 19, loss = 5.53935555
Iteration 20, loss = 5.51604052
Iteration 21, loss = 5.49621170
Iteration 22, loss = 5.47715497
Iteration 23, loss = 5.45735121
Iteration 24, loss = 5.43758480
Iteration 25, loss = 5.42299513
Iteration 26, loss = 5.40658241
Iteration 27, loss = 5.39014112
Iteration 28, loss = 5.37352194
Iteration 29, loss = 5.35938490
Iteration 30, loss = 5.34527639
Iteration 31, loss = 5.33038545
Iteration 32, loss = 5.31780296
Iteration 33, loss = 5.30547475
Iteration 34, loss = 5.29219161
Iteration 35, loss = 5.28414329
Iteration 36, loss = 5.27416735
Iteration 37, loss = 5.26435459
Iteration 38, loss = 5.25527571
Iteration 39, loss = 5.24560607
Iteration 40, loss = 5.23775326
Iteration 41, loss = 5.23018175
Iteration 42, loss = 5.22353024
Iteration 43, loss = 5.21631285
Iteration 44, loss = 5.20979448
Iteration 45, loss = 5.20088970
Iteration 46, loss = 5.19523861
Iteration 47, loss = 5.18971110
Iteration 48, loss = 5.19015647
Iteration 49, loss = 5.18008705
Iteration 50, loss = 5.17332017
Iteration 51, loss = 5.16949973
Iteration 52, loss = 5.16518386
Iteration 53, loss = 5.16299062
Iteration 54, loss = 5.15986432
Iteration 55, loss = 5.15206104
Iteration 56, loss = 5.15125440
Iteration 57, loss = 5.14471315
Iteration 58, loss = 5.13907010
Iteration 59, loss = 5.13663333
Iteration 60, loss = 5.13340468
Iteration 61, loss = 5.13002046
Iteration 62, loss = 5.12615259
Iteration 63, loss = 5.12501816
Iteration 64, loss = 5.12005381
Iteration 65, loss = 5.11642637
Iteration 66, loss = 5.11801940
Iteration 67, loss = 5.10980192
Iteration 68, loss = 5.11047541
Iteration 69, loss = 5.11477677
Iteration 70, loss = 5.10685198
Iteration 71, loss = 5.10092718
Iteration 72, loss = 5.09903207
Iteration 73, loss = 5.09334150
Iteration 74, loss = 5.09223081
Iteration 75, loss = 5.09067991
Iteration 76, loss = 5.09088621
Iteration 77, loss = 5.08689857
Iteration 78, loss = 5.08387113
Iteration 79, loss = 5.08114633
Iteration 80, loss = 5.07634714
Iteration 81, loss = 5.07765371
Iteration 82, loss = 5.07198279
Iteration 83, loss = 5.07062640
Iteration 84, loss = 5.06977457
Iteration 85, loss = 5.06546363
Iteration 86, loss = 5.06402712
Iteration 87, loss = 5.06151395
Iteration 88, loss = 5.06721945
Iteration 89, loss = 5.05880755
Iteration 90, loss = 5.05698255
Iteration 91, loss = 5.05525495
Iteration 92, loss = 5.05329890
Iteration 93, loss = 5.05164601
Iteration 94, loss = 5.05231049
Iteration 95, loss = 5.04934864
Iteration 96, loss = 5.05247939
Iteration 97, loss = 5.04750890
Iteration 98, loss = 5.04357640
Iteration 99, loss = 5.04051506
Iteration 100, loss = 5.03705367
Iteration 101, loss = 5.03690044
Iteration 102, loss = 5.03288708
Iteration 103, loss = 5.03348576
Iteration 104, loss = 5.03586658
Iteration 105, loss = 5.02998801
Iteration 106, loss = 5.02875966
Iteration 107, loss = 5.02821307
Iteration 108, loss = 5.03248041
Iteration 109, loss = 5.02587219
Iteration 110, loss = 5.02131240
Iteration 111, loss = 5.02039712
Iteration 112, loss = 5.01828941
Iteration 113, loss = 5.01642268
Iteration 114, loss = 5.01826848
Iteration 115, loss = 5.01630957
Iteration 116, loss = 5.01289928
Iteration 117, loss = 5.01177309
Iteration 118, loss = 5.00915735
Iteration 119, loss = 5.00687213
Iteration 120, loss = 5.01154699
Iteration 121, loss = 5.00629139
Iteration 122, loss = 5.00447199
Iteration 123, loss = 5.00269236
Iteration 124, loss = 4.99967209
Iteration 125, loss = 5.00021677
Iteration 126, loss = 4.99835268
Iteration 127, loss = 4.99696592
Iteration 128, loss = 4.99382778
Iteration 129, loss = 4.99557148
Iteration 130, loss = 4.99274175
Iteration 131, loss = 4.99286020
Iteration 132, loss = 4.99148767
Iteration 133, loss = 4.99236675
Iteration 134, loss = 4.99119850
Iteration 135, loss = 4.98893978
Iteration 136, loss = 4.98567232
Iteration 137, loss = 4.98218597
Iteration 138, loss = 4.98058742
Iteration 139, loss = 4.98333989
Iteration 140, loss = 4.98031706
Iteration 141, loss = 4.98222748
Iteration 142, loss = 4.97831999
Iteration 143, loss = 4.98255154
Iteration 144, loss = 4.97373556
Iteration 145, loss = 4.97566034
Iteration 146, loss = 4.97250074
Iteration 147, loss = 4.97169134
Iteration 148, loss = 4.97330110
Iteration 149, loss = 4.97024035
Iteration 150, loss = 4.97119260
Iteration 151, loss = 4.96917710
Iteration 152, loss = 4.97047596
Iteration 153, loss = 4.96656996
Iteration 154, loss = 4.96637948
Iteration 155, loss = 4.96376524
Iteration 156, loss = 4.96711046
Iteration 157, loss = 4.96097589
Iteration 158, loss = 4.95781324
Iteration 159, loss = 4.96064167
Iteration 160, loss = 4.96263947
Iteration 161, loss = 4.95816476
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((10, 10, 10, 10, 10, 10, 10, 10), {'error': 0.99109738363848243, 'fit': 0.031375, 'time': 221.8199999999997})
Iteration 1, loss = 6.18691091
Iteration 2, loss = 5.70912896
Iteration 3, loss = 5.52561485
Iteration 4, loss = 5.43533001
Iteration 5, loss = 5.35401294
Iteration 6, loss = 5.26200463
Iteration 7, loss = 5.18582080
Iteration 8, loss = 5.10831894
Iteration 9, loss = 5.03849649
Iteration 10, loss = 4.98970306
Iteration 11, loss = 4.94701623
Iteration 12, loss = 4.91350938
Iteration 13, loss = 4.87521570
Iteration 14, loss = 4.85099402
Iteration 15, loss = 4.82324003
Iteration 16, loss = 4.80043361
Iteration 17, loss = 4.76757412
Iteration 18, loss = 4.76031135
Iteration 19, loss = 4.73341998
Iteration 20, loss = 4.70538117
Iteration 21, loss = 4.69219185
Iteration 22, loss = 4.66191050
Iteration 23, loss = 4.64813033
Iteration 24, loss = 4.62888652
Iteration 25, loss = 4.60732800
Iteration 26, loss = 4.58491665
Iteration 27, loss = 4.57611852
Iteration 28, loss = 4.57777380
Iteration 29, loss = 4.54627171
Iteration 30, loss = 4.52670679
Iteration 31, loss = 4.51278906
Iteration 32, loss = 4.49564353
Iteration 33, loss = 4.48819490
Iteration 34, loss = 4.47147791
Iteration 35, loss = 4.46006060
Iteration 36, loss = 4.42949384
Iteration 37, loss = 4.41645516
Iteration 38, loss = 4.39804595
Iteration 39, loss = 4.39125061
Iteration 40, loss = 4.41397616
Iteration 41, loss = 4.33901722
Iteration 42, loss = 4.34471619
Iteration 43, loss = 4.32526210
Iteration 44, loss = 4.31773780
Iteration 45, loss = 4.28788266
Iteration 46, loss = 4.29360341
Iteration 47, loss = 4.26369722
Iteration 48, loss = 4.24111350
Iteration 49, loss = 4.23565598
Iteration 50, loss = 4.24188357
Iteration 51, loss = 4.21230999
Iteration 52, loss = 4.19322028
Iteration 53, loss = 4.17616496
Iteration 54, loss = 4.16751828
Iteration 55, loss = 4.14492103
Iteration 56, loss = 4.15080289
Iteration 57, loss = 4.12276254
Iteration 58, loss = 4.09735614
Iteration 59, loss = 4.11781334
Iteration 60, loss = 4.08846376
Iteration 61, loss = 4.06610811
Iteration 62, loss = 4.04415185
Iteration 63, loss = 4.03886252
Iteration 64, loss = 4.04346338
Iteration 65, loss = 4.02410423
Iteration 66, loss = 3.99620696
Iteration 67, loss = 3.99079663
Iteration 68, loss = 3.97685782
Iteration 69, loss = 3.95336782
Iteration 70, loss = 3.93113687
Iteration 71, loss = 3.93175725
Iteration 72, loss = 3.92108248
Iteration 73, loss = 3.91065944
Iteration 74, loss = 3.89429735
Iteration 75, loss = 3.87415759
Iteration 76, loss = 3.85840721
Iteration 77, loss = 3.85786275
Iteration 78, loss = 3.82842617
Iteration 79, loss = 3.81445973
Iteration 80, loss = 3.80405531
Iteration 81, loss = 3.81488786
Iteration 82, loss = 3.78834675
Iteration 83, loss = 3.77078614
Iteration 84, loss = 3.77463389
Iteration 85, loss = 3.74036610
Iteration 86, loss = 3.71817121
Iteration 87, loss = 3.73786670
Iteration 88, loss = 3.71345554
Iteration 89, loss = 3.68523607
Iteration 90, loss = 3.68188665
Iteration 91, loss = 3.67144664
Iteration 92, loss = 3.65303458
Iteration 93, loss = 3.63909543
Iteration 94, loss = 3.62756789
Iteration 95, loss = 3.62521300
Iteration 96, loss = 3.60109160
Iteration 97, loss = 3.60170050
Iteration 98, loss = 3.57687823
Iteration 99, loss = 3.57119306
Iteration 100, loss = 3.57687159
Iteration 101, loss = 3.55381060
Iteration 102, loss = 3.55542572
Iteration 103, loss = 3.52026122
Iteration 104, loss = 3.49332912
Iteration 105, loss = 3.48166195
Iteration 106, loss = 3.48538793
Iteration 107, loss = 3.47476775
Iteration 108, loss = 3.45413404
Iteration 109, loss = 3.43814126
Iteration 110, loss = 3.42219086
Iteration 111, loss = 3.41457595
Iteration 112, loss = 3.41675333
Iteration 113, loss = 3.39623020
Iteration 114, loss = 3.38981308
Iteration 115, loss = 3.38630754
Iteration 116, loss = 3.38113931
Iteration 117, loss = 3.34207452
Iteration 118, loss = 3.33296438
Iteration 119, loss = 3.30987938
Iteration 120, loss = 3.30079767
Iteration 121, loss = 3.28613145
Iteration 122, loss = 3.27814208
Iteration 123, loss = 3.26496157
Iteration 124, loss = 3.25723022
Iteration 125, loss = 3.23120732
Iteration 126, loss = 3.22914693
Iteration 127, loss = 3.27553396
Iteration 128, loss = 3.26662685
Iteration 129, loss = 3.23080210
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.73137999729486602, 'fit': 0.23599999999999999, 'time': 349.4400000000023})
Iteration 1, loss = 6.09273624
Iteration 2, loss = 5.87715367
Iteration 3, loss = 5.72944396
Iteration 4, loss = 5.65336227
Iteration 5, loss = 5.60426770
Iteration 6, loss = 5.47928410
Iteration 7, loss = 5.41528833
Iteration 8, loss = 5.36334214
Iteration 9, loss = 5.31072075
Iteration 10, loss = 5.22987662
Iteration 11, loss = 5.18867154
Iteration 12, loss = 5.08655748
Iteration 13, loss = 5.02226155
Iteration 14, loss = 4.93962599
Iteration 15, loss = 4.91066542
Iteration 16, loss = 4.83386721
Iteration 17, loss = 4.78584462
Iteration 18, loss = 4.73005479
Iteration 19, loss = 4.62134220
Iteration 20, loss = 4.63584140
Iteration 21, loss = 4.53141178
Iteration 22, loss = 4.46466019
Iteration 23, loss = 4.45621943
Iteration 24, loss = 4.38340352
Iteration 25, loss = 4.30330506
Iteration 26, loss = 4.26011888
Iteration 27, loss = 4.24854865
Iteration 28, loss = 4.12929411
Iteration 29, loss = 4.10664221
Iteration 30, loss = 4.00441120
Iteration 31, loss = 3.96302494
Iteration 32, loss = 3.94327447
Iteration 33, loss = 3.82143845
Iteration 34, loss = 3.78884799
Iteration 35, loss = 3.71342942
Iteration 36, loss = 3.66915221
Iteration 37, loss = 3.71310092
Iteration 38, loss = 3.61271610
Iteration 39, loss = 3.54211508
Iteration 40, loss = 3.48391825
Iteration 41, loss = 3.47527782
Iteration 42, loss = 3.41345937
Iteration 43, loss = 3.35108811
Iteration 44, loss = 3.32495010
Iteration 45, loss = 3.30098622
Iteration 46, loss = 3.22047623
Iteration 47, loss = 3.24294051
Iteration 48, loss = 3.18259503
Iteration 49, loss = 3.10551221
Iteration 50, loss = 3.07949919
Iteration 51, loss = 2.99803048
Iteration 52, loss = 2.92210761
Iteration 53, loss = 2.95770006
Iteration 54, loss = 2.89765056
Iteration 55, loss = 2.84299056
Iteration 56, loss = 2.88171974
Iteration 57, loss = 2.81456122
Iteration 58, loss = 2.78064379
Iteration 59, loss = 2.86912105
Iteration 60, loss = 2.77009758
Iteration 61, loss = 2.67001188
Iteration 62, loss = 2.65641565
Iteration 63, loss = 2.67951164
Iteration 64, loss = 2.55838023
Iteration 65, loss = 2.48053153
Iteration 66, loss = 2.46306682
Iteration 67, loss = 2.42659744
Iteration 68, loss = 2.38468172
Iteration 69, loss = 2.35906384
Iteration 70, loss = 2.33698555
Iteration 71, loss = 2.26794320
Iteration 72, loss = 2.21577657
Iteration 73, loss = 2.23886374
Iteration 74, loss = 2.24053397
Iteration 75, loss = 2.25972970
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000), {'error': 0.7041531678800832, 'fit': 0.393125, 'time': 3708.050000000003})
Iteration 1, loss = 6.53933402
Iteration 2, loss = 6.50643511
Iteration 3, loss = 6.47045522
Iteration 4, loss = 6.43428588
Iteration 5, loss = 6.39998569
Iteration 6, loss = 6.36818366
Iteration 7, loss = 6.33902502
Iteration 8, loss = 6.31220685
Iteration 9, loss = 6.28753681
Iteration 10, loss = 6.26478871
Iteration 11, loss = 6.24366234
Iteration 12, loss = 6.22426424
Iteration 13, loss = 6.20636833
Iteration 14, loss = 6.18970421
Iteration 15, loss = 6.17440445
Iteration 16, loss = 6.16019847
Iteration 17, loss = 6.14709825
Iteration 18, loss = 6.13497045
Iteration 19, loss = 6.12377331
Iteration 20, loss = 6.11348999
Iteration 21, loss = 6.10386029
Iteration 22, loss = 6.09507287
Iteration 23, loss = 6.08700559
Iteration 24, loss = 6.07943482
Iteration 25, loss = 6.07259769
Iteration 26, loss = 6.06628713
Iteration 27, loss = 6.06039642
Iteration 28, loss = 6.05510320
Iteration 29, loss = 6.05025475
Iteration 30, loss = 6.04570130
Iteration 31, loss = 6.04161533
Iteration 32, loss = 6.03787574
Iteration 33, loss = 6.03450165
Iteration 34, loss = 6.03135701
Iteration 35, loss = 6.02847184
Iteration 36, loss = 6.02586105
Iteration 37, loss = 6.02360583
Iteration 38, loss = 6.02139234
Iteration 39, loss = 6.01940005
Iteration 40, loss = 6.01757720
Iteration 41, loss = 6.01603609
Iteration 42, loss = 6.01446230
Iteration 43, loss = 6.01313165
Iteration 44, loss = 6.01195290
Iteration 45, loss = 6.01077884
Iteration 46, loss = 6.00972306
Iteration 47, loss = 6.00877582
Iteration 48, loss = 6.00791037
Iteration 49, loss = 6.00709469
Iteration 50, loss = 6.00633639
Iteration 51, loss = 6.00570066
Iteration 52, loss = 6.00499591
Iteration 53, loss = 6.00444806
Iteration 54, loss = 6.00394108
Iteration 55, loss = 6.00343612
Iteration 56, loss = 6.00296102
Iteration 57, loss = 6.00249005
Iteration 58, loss = 6.00209728
Iteration 59, loss = 6.00170266
Iteration 60, loss = 6.00137273
Iteration 61, loss = 6.00099855
Iteration 62, loss = 6.00072787
Iteration 63, loss = 6.00044711
Iteration 64, loss = 6.00015557
Iteration 65, loss = 5.99981693
Iteration 66, loss = 5.99963332
Iteration 67, loss = 5.99940731
Iteration 68, loss = 5.99919220
Iteration 69, loss = 5.99893949
Iteration 70, loss = 5.99881757
Iteration 71, loss = 5.99851800
Iteration 72, loss = 5.99840364
Iteration 73, loss = 5.99825570
Iteration 74, loss = 5.99811493
Iteration 75, loss = 5.99794228
Iteration 76, loss = 5.99781729
Iteration 77, loss = 5.99768222
Iteration 78, loss = 5.99753378
Iteration 79, loss = 5.99741654
Iteration 80, loss = 5.99731074
Iteration 81, loss = 5.99724876
Iteration 82, loss = 5.99710323
Iteration 83, loss = 5.99698967
Iteration 84, loss = 5.99690746
Iteration 85, loss = 5.99682891
Iteration 86, loss = 5.99673825
Iteration 87, loss = 5.99669877
Iteration 88, loss = 5.99658151
Iteration 89, loss = 5.99653768
Iteration 90, loss = 5.99637016
Iteration 91, loss = 5.99632679
Iteration 92, loss = 5.99632137
Iteration 93, loss = 5.99619631
Iteration 94, loss = 5.99620352
Iteration 95, loss = 5.99617853
Iteration 96, loss = 5.99612460
Iteration 97, loss = 5.99605755
Iteration 98, loss = 5.99599389
Iteration 99, loss = 5.99592204
Iteration 100, loss = 5.99593493
Iteration 101, loss = 5.99581734
Iteration 102, loss = 5.99579143
Iteration 103, loss = 5.99576244
Iteration 104, loss = 5.99566571
Iteration 105, loss = 5.99564462
Iteration 106, loss = 5.99564371
Iteration 107, loss = 5.99563355
Iteration 108, loss = 5.99557205
Iteration 109, loss = 5.99556799
Iteration 110, loss = 5.99547955
Iteration 111, loss = 5.99525237
Iteration 112, loss = 5.99511283
Iteration 113, loss = 5.99484346
Iteration 114, loss = 5.99366528
Iteration 115, loss = 5.97706154
Iteration 116, loss = 5.93299018
Iteration 117, loss = 5.92474354
Iteration 118, loss = 5.91847468
Iteration 119, loss = 5.91205667
Iteration 120, loss = 5.90769462
Iteration 121, loss = 5.90347797
Iteration 122, loss = 5.89946485
Iteration 123, loss = 5.89590555
Iteration 124, loss = 5.89242607
Iteration 125, loss = 5.88885251
Iteration 126, loss = 5.88535338
Iteration 127, loss = 5.88212445
Iteration 128, loss = 5.87889010
Iteration 129, loss = 5.87591902
Iteration 130, loss = 5.87311891
Iteration 131, loss = 5.87014497
Iteration 132, loss = 5.86690953
Iteration 133, loss = 5.86377847
Iteration 134, loss = 5.86112434
Iteration 135, loss = 5.85808683
Iteration 136, loss = 5.85517352
Iteration 137, loss = 5.85234437
Iteration 138, loss = 5.85070636
Iteration 139, loss = 5.84744787
Iteration 140, loss = 5.84430570
Iteration 141, loss = 5.84151152
Iteration 142, loss = 5.83891145
Iteration 143, loss = 5.83612040
Iteration 144, loss = 5.83378858
Iteration 145, loss = 5.83111908
Iteration 146, loss = 5.82828877
Iteration 147, loss = 5.82578238
Iteration 148, loss = 5.82336993
Iteration 149, loss = 5.82050739
Iteration 150, loss = 5.81837231
Iteration 151, loss = 5.81621039
Iteration 152, loss = 5.81330236
Iteration 153, loss = 5.81081170
Iteration 154, loss = 5.80851712
Iteration 155, loss = 5.80633176
Iteration 156, loss = 5.80399463
Iteration 157, loss = 5.80156669
Iteration 158, loss = 5.79969291
Iteration 159, loss = 5.79760170
Iteration 160, loss = 5.79536048
Iteration 161, loss = 5.79337995
Iteration 162, loss = 5.79119863
Iteration 163, loss = 5.78948297
Iteration 164, loss = 5.78746057
Iteration 165, loss = 5.78557885
Iteration 166, loss = 5.78369123
Iteration 167, loss = 5.78229758
Iteration 168, loss = 5.77975298
Iteration 169, loss = 5.77817477
Iteration 170, loss = 5.77659583
Iteration 171, loss = 5.77488014
Iteration 172, loss = 5.77323862
Iteration 173, loss = 5.77153200
Iteration 174, loss = 5.76983226
Iteration 175, loss = 5.76848914
Iteration 176, loss = 5.76693255
Iteration 177, loss = 5.76600395
Iteration 178, loss = 5.76404899
Iteration 179, loss = 5.76281963
Iteration 180, loss = 5.76199549
Iteration 181, loss = 5.76003150
Iteration 182, loss = 5.75856390
Iteration 183, loss = 5.75705768
Iteration 184, loss = 5.75619426
Iteration 185, loss = 5.75467597
Iteration 186, loss = 5.75331012
Iteration 187, loss = 5.75253264
Iteration 188, loss = 5.75194872
Iteration 189, loss = 5.75055431
Iteration 190, loss = 5.74927467
Iteration 191, loss = 5.74786772
Iteration 192, loss = 5.74678142
Iteration 193, loss = 5.74574163
Iteration 194, loss = 5.74474965
Iteration 195, loss = 5.74403337
Iteration 196, loss = 5.74282239
Iteration 197, loss = 5.74214500
Iteration 198, loss = 5.74168033
Iteration 199, loss = 5.73998282
Iteration 200, loss = 5.73930679
Iteration 201, loss = 5.73822826
Iteration 202, loss = 5.73716428
Iteration 203, loss = 5.73633713
Iteration 204, loss = 5.73546673
Iteration 205, loss = 5.73513797
Iteration 206, loss = 5.73436955
Iteration 207, loss = 5.73354815
Iteration 208, loss = 5.73268037
Iteration 209, loss = 5.73220592
Iteration 210, loss = 5.73122090
Iteration 211, loss = 5.73038161
Iteration 212, loss = 5.72997439
Iteration 213, loss = 5.72893195
Iteration 214, loss = 5.72883852
Iteration 215, loss = 5.72734595
Iteration 216, loss = 5.72707245
Iteration 217, loss = 5.72687448
Iteration 218, loss = 5.72558947
Iteration 219, loss = 5.72460501
Iteration 220, loss = 5.72462044
Iteration 221, loss = 5.72521933
Iteration 222, loss = 5.72370194
Iteration 223, loss = 5.72344108
Iteration 224, loss = 5.72242684
Iteration 225, loss = 5.72132351
Iteration 226, loss = 5.72085995
Iteration 227, loss = 5.72185768
Iteration 228, loss = 5.72098713
Iteration 229, loss = 5.71965194
Iteration 230, loss = 5.71906006
Iteration 231, loss = 5.71807068
Iteration 232, loss = 5.71769727
Iteration 233, loss = 5.71803828
Iteration 234, loss = 5.71689530
Iteration 235, loss = 5.71610648
Iteration 236, loss = 5.71661090
Iteration 237, loss = 5.71568932
Iteration 238, loss = 5.71481233
Iteration 239, loss = 5.71419041
Iteration 240, loss = 5.71370323
Iteration 241, loss = 5.71369605
Iteration 242, loss = 5.71307270
Iteration 243, loss = 5.71294818
Iteration 244, loss = 5.71260399
Iteration 245, loss = 5.71173696
Iteration 246, loss = 5.71166231
Iteration 247, loss = 5.71111990
Iteration 248, loss = 5.71117421
Iteration 249, loss = 5.71136906
Iteration 250, loss = 5.71143239
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((1, 1, 1, 1, 1, 1, 1, 1, 1), {'error': 2.3981508618401248, 'fit': 0.020375000000000001, 'time': 302.5})
Iteration 1, loss = 6.50238620
Iteration 2, loss = 6.33264450
Iteration 3, loss = 6.17708036
Iteration 4, loss = 6.09426299
Iteration 5, loss = 6.05214876
Iteration 6, loss = 6.02987810
Iteration 7, loss = 5.98956453
Iteration 8, loss = 5.93928095
Iteration 9, loss = 5.90914079
Iteration 10, loss = 5.87619996
Iteration 11, loss = 5.84217876
Iteration 12, loss = 5.80792421
Iteration 13, loss = 5.77316950
Iteration 14, loss = 5.73614713
Iteration 15, loss = 5.70033092
Iteration 16, loss = 5.66506893
Iteration 17, loss = 5.63164880
Iteration 18, loss = 5.59975657
Iteration 19, loss = 5.56947888
Iteration 20, loss = 5.54121830
Iteration 21, loss = 5.51585384
Iteration 22, loss = 5.49378700
Iteration 23, loss = 5.47128425
Iteration 24, loss = 5.45375028
Iteration 25, loss = 5.43492969
Iteration 26, loss = 5.41555599
Iteration 27, loss = 5.39985378
Iteration 28, loss = 5.38366510
Iteration 29, loss = 5.37203103
Iteration 30, loss = 5.35665148
Iteration 31, loss = 5.34558034
Iteration 32, loss = 5.33451219
Iteration 33, loss = 5.32592052
Iteration 34, loss = 5.32131639
Iteration 35, loss = 5.30957368
Iteration 36, loss = 5.29913827
Iteration 37, loss = 5.29030828
Iteration 38, loss = 5.28390070
Iteration 39, loss = 5.27572285
Iteration 40, loss = 5.27196803
Iteration 41, loss = 5.26461759
Iteration 42, loss = 5.26006896
Iteration 43, loss = 5.25475730
Iteration 44, loss = 5.24680214
Iteration 45, loss = 5.24022083
Iteration 46, loss = 5.23677944
Iteration 47, loss = 5.22982895
Iteration 48, loss = 5.22450999
Iteration 49, loss = 5.22088745
Iteration 50, loss = 5.21769593
Iteration 51, loss = 5.21010755
Iteration 52, loss = 5.20524932
Iteration 53, loss = 5.20195283
Iteration 54, loss = 5.19653494
Iteration 55, loss = 5.19570251
Iteration 56, loss = 5.19050537
Iteration 57, loss = 5.18678997
Iteration 58, loss = 5.18150216
Iteration 59, loss = 5.17979557
Iteration 60, loss = 5.17657481
Iteration 61, loss = 5.17230743
Iteration 62, loss = 5.17257994
Iteration 63, loss = 5.16602894
Iteration 64, loss = 5.16218496
Iteration 65, loss = 5.16197732
Iteration 66, loss = 5.15507973
Iteration 67, loss = 5.15170713
Iteration 68, loss = 5.15043215
Iteration 69, loss = 5.14687291
Iteration 70, loss = 5.14405915
Iteration 71, loss = 5.14033452
Iteration 72, loss = 5.14319890
Iteration 73, loss = 5.13461212
Iteration 74, loss = 5.13181518
Iteration 75, loss = 5.13060198
Iteration 76, loss = 5.12879864
Iteration 77, loss = 5.12648650
Iteration 78, loss = 5.12261515
Iteration 79, loss = 5.12920243
Iteration 80, loss = 5.11840603
Iteration 81, loss = 5.11442994
Iteration 82, loss = 5.11207740
Iteration 83, loss = 5.11186847
Iteration 84, loss = 5.10953799
Iteration 85, loss = 5.10810601
Iteration 86, loss = 5.11043912
Iteration 87, loss = 5.10197596
Iteration 88, loss = 5.09971704
Iteration 89, loss = 5.09678114
Iteration 90, loss = 5.09734330
Iteration 91, loss = 5.09417998
Iteration 92, loss = 5.09374050
Iteration 93, loss = 5.09504256
Iteration 94, loss = 5.08873096
Iteration 95, loss = 5.08832286
Iteration 96, loss = 5.08732863
Iteration 97, loss = 5.08356924
Iteration 98, loss = 5.08047267
Iteration 99, loss = 5.08165428
Iteration 100, loss = 5.07690306
Iteration 101, loss = 5.07629724
Iteration 102, loss = 5.07270679
Iteration 103, loss = 5.07213173
Iteration 104, loss = 5.07417157
Iteration 105, loss = 5.07019169
Iteration 106, loss = 5.06918465
Iteration 107, loss = 5.06617031
Iteration 108, loss = 5.06741080
Iteration 109, loss = 5.06337703
Iteration 110, loss = 5.06051829
Iteration 111, loss = 5.05979025
Iteration 112, loss = 5.05882735
Iteration 113, loss = 5.05446784
Iteration 114, loss = 5.05542090
Iteration 115, loss = 5.05827021
Iteration 116, loss = 5.05444215
Iteration 117, loss = 5.05249225
Iteration 118, loss = 5.05085071
Iteration 119, loss = 5.04595877
Iteration 120, loss = 5.04706288
Iteration 121, loss = 5.04793591
Iteration 122, loss = 5.04437139
Iteration 123, loss = 5.04708519
Iteration 124, loss = 5.04291281
Iteration 125, loss = 5.04167776
Iteration 126, loss = 5.03911349
Iteration 127, loss = 5.03557131
Iteration 128, loss = 5.03695573
Iteration 129, loss = 5.03355946
Iteration 130, loss = 5.03134538
Iteration 131, loss = 5.03258536
Iteration 132, loss = 5.03113257
Iteration 133, loss = 5.02944180
Iteration 134, loss = 5.02751655
Iteration 135, loss = 5.02710781
Iteration 136, loss = 5.02443596
Iteration 137, loss = 5.02420790
Iteration 138, loss = 5.02290287
Iteration 139, loss = 5.02764995
Iteration 140, loss = 5.02349719
Iteration 141, loss = 5.02182910
Iteration 142, loss = 5.01605378
Iteration 143, loss = 5.01769580
Iteration 144, loss = 5.01515998
Iteration 145, loss = 5.01387061
Iteration 146, loss = 5.01128693
Iteration 147, loss = 5.01149420
Iteration 148, loss = 5.01338306
Iteration 149, loss = 5.01118293
Iteration 150, loss = 5.01146687
Iteration 151, loss = 5.01079078
Iteration 152, loss = 5.00723400
Iteration 153, loss = 5.00688327
Iteration 154, loss = 5.00764044
Iteration 155, loss = 5.00375499
Iteration 156, loss = 5.00350248
Iteration 157, loss = 5.00148892
Iteration 158, loss = 5.00136265
Iteration 159, loss = 4.99921305
Iteration 160, loss = 4.99786273
Iteration 161, loss = 4.99621575
Iteration 162, loss = 4.99457974
Iteration 163, loss = 4.99591134
Iteration 164, loss = 4.99768414
Iteration 165, loss = 4.99342253
Iteration 166, loss = 4.99125856
Iteration 167, loss = 4.99370344
Iteration 168, loss = 4.99078442
Iteration 169, loss = 4.99162194
Iteration 170, loss = 4.98878369
Iteration 171, loss = 4.98877079
Iteration 172, loss = 4.98778883
Iteration 173, loss = 4.98489014
Iteration 174, loss = 4.99041017
Iteration 175, loss = 4.98437681
Iteration 176, loss = 4.98329356
Iteration 177, loss = 4.98392416
Iteration 178, loss = 4.98020528
Iteration 179, loss = 4.98090253
Iteration 180, loss = 4.97854347
Iteration 181, loss = 4.97792802
Iteration 182, loss = 4.98157480
Iteration 183, loss = 4.97538082
Iteration 184, loss = 4.97741305
Iteration 185, loss = 4.97389772
Iteration 186, loss = 4.97431691
Iteration 187, loss = 4.98132665
Iteration 188, loss = 4.97241369
Iteration 189, loss = 4.97182311
Iteration 190, loss = 4.96815990
Iteration 191, loss = 4.97063033
Iteration 192, loss = 4.96849809
Iteration 193, loss = 4.97057841
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((10, 10, 10, 10, 10, 10, 10, 10, 10), {'error': 1.0802403054364742, 'fit': 0.029250000000000002, 'time': 270.0199999999968})
Iteration 1, loss = 6.17859790
Iteration 2, loss = 5.75751645
Iteration 3, loss = 5.57522660
Iteration 4, loss = 5.46724222
Iteration 5, loss = 5.39589070
Iteration 6, loss = 5.30507845
Iteration 7, loss = 5.21965438
Iteration 8, loss = 5.13946165
Iteration 9, loss = 5.09727572
Iteration 10, loss = 5.03157582
Iteration 11, loss = 4.98439033
Iteration 12, loss = 4.94541872
Iteration 13, loss = 4.90022953
Iteration 14, loss = 4.88576393
Iteration 15, loss = 4.86183405
Iteration 16, loss = 4.82783850
Iteration 17, loss = 4.80256654
Iteration 18, loss = 4.79046502
Iteration 19, loss = 4.76315970
Iteration 20, loss = 4.76371720
Iteration 21, loss = 4.72429073
Iteration 22, loss = 4.70624987
Iteration 23, loss = 4.68807215
Iteration 24, loss = 4.66417315
Iteration 25, loss = 4.67330198
Iteration 26, loss = 4.65507337
Iteration 27, loss = 4.61082244
Iteration 28, loss = 4.60271329
Iteration 29, loss = 4.60880984
Iteration 30, loss = 4.58485836
Iteration 31, loss = 4.56109260
Iteration 32, loss = 4.54942543
Iteration 33, loss = 4.53212868
Iteration 34, loss = 4.51790448
Iteration 35, loss = 4.49587250
Iteration 36, loss = 4.47015679
Iteration 37, loss = 4.47484721
Iteration 38, loss = 4.45067462
Iteration 39, loss = 4.42734982
Iteration 40, loss = 4.42787746
Iteration 41, loss = 4.40223021
Iteration 42, loss = 4.38989234
Iteration 43, loss = 4.39775233
Iteration 44, loss = 4.37316084
Iteration 45, loss = 4.33947682
Iteration 46, loss = 4.32752421
Iteration 47, loss = 4.31672223
Iteration 48, loss = 4.30319171
Iteration 49, loss = 4.29725330
Iteration 50, loss = 4.29632886
Iteration 51, loss = 4.26922799
Iteration 52, loss = 4.24572886
Iteration 53, loss = 4.23164167
Iteration 54, loss = 4.20791404
Iteration 55, loss = 4.20533258
Iteration 56, loss = 4.19684923
Iteration 57, loss = 4.19516277
Iteration 58, loss = 4.19614513
Iteration 59, loss = 4.16537040
Iteration 60, loss = 4.13864387
Iteration 61, loss = 4.11955585
Iteration 62, loss = 4.13068636
Iteration 63, loss = 4.09462508
Iteration 64, loss = 4.08488815
Iteration 65, loss = 4.05771238
Iteration 66, loss = 4.06639993
Iteration 67, loss = 4.04655155
Iteration 68, loss = 4.03644850
Iteration 69, loss = 4.03537323
Iteration 70, loss = 4.02951265
Iteration 71, loss = 3.98955601
Iteration 72, loss = 3.97130042
Iteration 73, loss = 3.96367824
Iteration 74, loss = 3.93876030
Iteration 75, loss = 3.93887133
Iteration 76, loss = 3.94287698
Iteration 77, loss = 3.90826580
Iteration 78, loss = 3.89513349
Iteration 79, loss = 3.88923514
Iteration 80, loss = 3.87239115
Iteration 81, loss = 3.86425716
Iteration 82, loss = 3.86020990
Iteration 83, loss = 3.85929072
Iteration 84, loss = 3.82903695
Iteration 85, loss = 3.80646851
Iteration 86, loss = 3.79061841
Iteration 87, loss = 3.78422710
Iteration 88, loss = 3.78981302
Iteration 89, loss = 3.77982215
Iteration 90, loss = 3.73316712
Iteration 91, loss = 3.74183902
Iteration 92, loss = 3.74891619
Iteration 93, loss = 3.76022568
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.6828810335611587, 'fit': 0.13375000000000001, 'time': 267.2700000000041})
Iteration 1, loss = 6.11278246
Iteration 2, loss = 5.81559335
Iteration 3, loss = 5.79156787
Iteration 4, loss = 5.67883354
Iteration 5, loss = 5.66500365
Iteration 6, loss = 5.60168027
Iteration 7, loss = 5.54900911
Iteration 8, loss = 5.47120367
Iteration 9, loss = 5.42051140
Iteration 10, loss = 5.37987870
Iteration 11, loss = 5.39505651
Iteration 12, loss = 5.32411350
Iteration 13, loss = 5.36745822
Iteration 14, loss = 5.25668991
Iteration 15, loss = 5.21009563
Iteration 16, loss = 5.20646396
Iteration 17, loss = 5.11546066
Iteration 18, loss = 5.10384819
Iteration 19, loss = 5.11848493
Iteration 20, loss = 5.05292642
Iteration 21, loss = 5.00070826
Iteration 22, loss = 4.99424327
Iteration 23, loss = 4.96675379
Iteration 24, loss = 4.98229216
Iteration 25, loss = 4.93533953
Iteration 26, loss = 4.87281674
Iteration 27, loss = 4.86593148
Iteration 28, loss = 4.83728188
Iteration 29, loss = 4.76122578
Iteration 30, loss = 4.73611119
Iteration 31, loss = 4.74016837
Iteration 32, loss = 4.70067533
Iteration 33, loss = 4.67778875
Iteration 34, loss = 4.74558849
Iteration 35, loss = 4.67028480
Iteration 36, loss = 4.63847147
Iteration 37, loss = 4.56727970
Iteration 38, loss = 4.56498305
Iteration 39, loss = 4.59773620
Iteration 40, loss = 4.60547317
Iteration 41, loss = 4.50648319
Iteration 42, loss = 4.48930799
Iteration 43, loss = 4.50443902
Iteration 44, loss = 4.43381607
Iteration 45, loss = 4.43543818
Iteration 46, loss = 4.47417547
Iteration 47, loss = 4.36691167
Iteration 48, loss = 4.32541953
Iteration 49, loss = 4.30230770
Iteration 50, loss = 4.32398989
Iteration 51, loss = 4.30253993
Iteration 52, loss = 4.25786290
Iteration 53, loss = 4.21550899
Iteration 54, loss = 4.20895291
Iteration 55, loss = 4.18564173
Iteration 56, loss = 4.12815696
Iteration 57, loss = 4.12456570
Iteration 58, loss = 4.10907824
Iteration 59, loss = 4.10820095
Iteration 60, loss = 4.10125569
Iteration 61, loss = 4.04164222
Iteration 62, loss = 4.04767289
Iteration 63, loss = 4.03456148
Iteration 64, loss = 4.05858401
Iteration 65, loss = 4.03873368
Iteration 66, loss = 3.98596194
Iteration 67, loss = 3.90790634
Iteration 68, loss = 3.91965351
Iteration 69, loss = 3.95653595
Iteration 70, loss = 3.87289545
Iteration 71, loss = 3.87911130
Iteration 72, loss = 3.82403788
Iteration 73, loss = 3.79170385
Iteration 74, loss = 3.79315292
Iteration 75, loss = 3.79523008
Iteration 76, loss = 3.78459518
Iteration 77, loss = 3.75632466
Iteration 78, loss = 3.74696406
Iteration 79, loss = 3.77701402
Iteration 80, loss = 3.75337373
Iteration 81, loss = 3.69148805
Iteration 82, loss = 3.64897114
Iteration 83, loss = 3.60541988
Iteration 84, loss = 3.61145066
Iteration 85, loss = 3.59391299
Iteration 86, loss = 3.56342069
Iteration 87, loss = 3.58720060
Iteration 88, loss = 3.56387546
Iteration 89, loss = 3.50282023
Iteration 90, loss = 3.47661505
Iteration 91, loss = 3.45226789
Iteration 92, loss = 3.40614919
Iteration 93, loss = 3.46965997
Iteration 94, loss = 3.46293059
Iteration 95, loss = 3.46527941
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000), {'error': 0.72478143475790202, 'fit': 0.15837499999999999, 'time': 5152.18})
