Iteration 1, loss = 6.43973045
Iteration 2, loss = 6.11265870
Iteration 3, loss = 5.81239904
Iteration 4, loss = 5.66118485
Iteration 5, loss = 5.59054854
Iteration 6, loss = 5.54907406
Iteration 7, loss = 5.51904186
Iteration 8, loss = 5.49594561
Iteration 9, loss = 5.47682748
Iteration 10, loss = 5.46087891
Iteration 11, loss = 5.44703389
Iteration 12, loss = 5.43511260
Iteration 13, loss = 5.42387628
Iteration 14, loss = 5.41443916
Iteration 15, loss = 5.40657300
Iteration 16, loss = 5.39742447
Iteration 17, loss = 5.39044056
Iteration 18, loss = 5.38309634
Iteration 19, loss = 5.37592736
Iteration 20, loss = 5.36986949
Iteration 21, loss = 5.36283092
Iteration 22, loss = 5.35663102
Iteration 23, loss = 5.35166254
Iteration 24, loss = 5.34552466
Iteration 25, loss = 5.33937032
Iteration 26, loss = 5.33242763
Iteration 27, loss = 5.32743683
Iteration 28, loss = 5.32221508
Iteration 29, loss = 5.31511715
Iteration 30, loss = 5.30873932
Iteration 31, loss = 5.30177119
Iteration 32, loss = 5.29527598
Iteration 33, loss = 5.28900099
Iteration 34, loss = 5.28153332
Iteration 35, loss = 5.27547336
Iteration 36, loss = 5.26799394
Iteration 37, loss = 5.26087563
Iteration 38, loss = 5.25420179
Iteration 39, loss = 5.24624066
Iteration 40, loss = 5.23866331
Iteration 41, loss = 5.23108336
Iteration 42, loss = 5.22286269
Iteration 43, loss = 5.21463240
Iteration 44, loss = 5.20794393
Iteration 45, loss = 5.19916930
Iteration 46, loss = 5.18925649
Iteration 47, loss = 5.18111894
Iteration 48, loss = 5.17217890
Iteration 49, loss = 5.16321080
Iteration 50, loss = 5.15370269
Iteration 51, loss = 5.14497842
Iteration 52, loss = 5.13528269
Iteration 53, loss = 5.12469171
Iteration 54, loss = 5.11598612
Iteration 55, loss = 5.10628580
Iteration 56, loss = 5.09630019
Iteration 57, loss = 5.08591045
Iteration 58, loss = 5.07715493
Iteration 59, loss = 5.06527615
Iteration 60, loss = 5.05526701
Iteration 61, loss = 5.04533270
Iteration 62, loss = 5.03498996
Iteration 63, loss = 5.02407914
Iteration 64, loss = 5.01426641
Iteration 65, loss = 5.00279965
Iteration 66, loss = 4.99289166
Iteration 67, loss = 4.98073139
Iteration 68, loss = 4.97132616
Iteration 69, loss = 4.96020662
Iteration 70, loss = 4.94911021
Iteration 71, loss = 4.93856091
Iteration 72, loss = 4.92828263
Iteration 73, loss = 4.91689218
Iteration 74, loss = 4.90544647
Iteration 75, loss = 4.89480507
Iteration 76, loss = 4.88465902
Iteration 77, loss = 4.87455285
Iteration 78, loss = 4.86337739
Iteration 79, loss = 4.85108722
Iteration 80, loss = 4.84157461
Iteration 81, loss = 4.82982954
Iteration 82, loss = 4.81900312
Iteration 83, loss = 4.80819267
Iteration 84, loss = 4.79766143
Iteration 85, loss = 4.78654868
Iteration 86, loss = 4.77603134
Iteration 87, loss = 4.76558905
Iteration 88, loss = 4.75569491
Iteration 89, loss = 4.74371468
Iteration 90, loss = 4.73308863
Iteration 91, loss = 4.72232769
Iteration 92, loss = 4.71190848
Iteration 93, loss = 4.70229630
Iteration 94, loss = 4.69058289
Iteration 95, loss = 4.67996829
Iteration 96, loss = 4.67057344
Iteration 97, loss = 4.66044845
Iteration 98, loss = 4.64874226
Iteration 99, loss = 4.63907297
Iteration 100, loss = 4.62908385
Iteration 101, loss = 4.61932847
Iteration 102, loss = 4.60911688
Iteration 103, loss = 4.59933299
Iteration 104, loss = 4.58800208
Iteration 105, loss = 4.57745519
Iteration 106, loss = 4.56795213
Iteration 107, loss = 4.55875084
Iteration 108, loss = 4.54858570
Iteration 109, loss = 4.53763609
Iteration 110, loss = 4.52861722
Iteration 111, loss = 4.51769902
Iteration 112, loss = 4.50826272
Iteration 113, loss = 4.49968982
Iteration 114, loss = 4.49021628
Iteration 115, loss = 4.47933401
Iteration 116, loss = 4.47034732
Iteration 117, loss = 4.46126059
Iteration 118, loss = 4.45223716
Iteration 119, loss = 4.44109668
Iteration 120, loss = 4.43272648
Iteration 121, loss = 4.42317952
Iteration 122, loss = 4.41428912
Iteration 123, loss = 4.40460914
Iteration 124, loss = 4.39610847
Iteration 125, loss = 4.38573248
Iteration 126, loss = 4.37722262
Iteration 127, loss = 4.36834109
Iteration 128, loss = 4.35891033
Iteration 129, loss = 4.35018455
Iteration 130, loss = 4.34116065
Iteration 131, loss = 4.33311483
Iteration 132, loss = 4.32320614
Iteration 133, loss = 4.31508444
Iteration 134, loss = 4.30768295
Iteration 135, loss = 4.29723276
Iteration 136, loss = 4.28860412
Iteration 137, loss = 4.27964346
Iteration 138, loss = 4.27093615
Iteration 139, loss = 4.26469267
Iteration 140, loss = 4.25402295
Iteration 141, loss = 4.24727875
Iteration 142, loss = 4.23858495
Iteration 143, loss = 4.22908671
Iteration 144, loss = 4.22082479
Iteration 145, loss = 4.21223016
Iteration 146, loss = 4.20404183
Iteration 147, loss = 4.19702975
Iteration 148, loss = 4.18805653
Iteration 149, loss = 4.17981017
Iteration 150, loss = 4.17194239
Iteration 151, loss = 4.16471387
Iteration 152, loss = 4.15691303
Iteration 153, loss = 4.14774331
Iteration 154, loss = 4.14036678
Iteration 155, loss = 4.13279535
Iteration 156, loss = 4.12513180
Iteration 157, loss = 4.11650036
Iteration 158, loss = 4.10861810
Iteration 159, loss = 4.10046915
Iteration 160, loss = 4.09386392
Iteration 161, loss = 4.08636101
Iteration 162, loss = 4.07710305
Iteration 163, loss = 4.07116506
Iteration 164, loss = 4.06323491
Iteration 165, loss = 4.05664781
Iteration 166, loss = 4.04846021
Iteration 167, loss = 4.04067828
Iteration 168, loss = 4.03373771
Iteration 169, loss = 4.02560464
Iteration 170, loss = 4.01920657
Iteration 171, loss = 4.01155916
Iteration 172, loss = 4.00431152
Iteration 173, loss = 3.99748866
Iteration 174, loss = 3.99054894
Iteration 175, loss = 3.98351826
Iteration 176, loss = 3.97563631
Iteration 177, loss = 3.96874050
Iteration 178, loss = 3.96240380
Iteration 179, loss = 3.95434183
Iteration 180, loss = 3.94711969
Iteration 181, loss = 3.94072392
Iteration 182, loss = 3.93287793
Iteration 183, loss = 3.92700973
Iteration 184, loss = 3.92115597
Iteration 185, loss = 3.91402651
Iteration 186, loss = 3.90629261
Iteration 187, loss = 3.90028733
Iteration 188, loss = 3.89392491
Iteration 189, loss = 3.88632458
Iteration 190, loss = 3.88059823
Iteration 191, loss = 3.87363353
Iteration 192, loss = 3.86668015
Iteration 193, loss = 3.86042463
Iteration 194, loss = 3.85437567
Iteration 195, loss = 3.84754961
Iteration 196, loss = 3.84043155
Iteration 197, loss = 3.83459525
Iteration 198, loss = 3.82831000
Iteration 199, loss = 3.82210721
Iteration 200, loss = 3.81567340
Iteration 201, loss = 3.80926082
Iteration 202, loss = 3.80269428
Iteration 203, loss = 3.79750510
Iteration 204, loss = 3.79029550
Iteration 205, loss = 3.78414852
Iteration 206, loss = 3.77816939
Iteration 207, loss = 3.77238417
Iteration 208, loss = 3.76603717
Iteration 209, loss = 3.76054758
Iteration 210, loss = 3.75379256
Iteration 211, loss = 3.74832342
Iteration 212, loss = 3.74266081
Iteration 213, loss = 3.73679527
Iteration 214, loss = 3.73051490
Iteration 215, loss = 3.72499869
Iteration 216, loss = 3.71865287
Iteration 217, loss = 3.71373554
Iteration 218, loss = 3.70699092
Iteration 219, loss = 3.70197504
Iteration 220, loss = 3.69490629
Iteration 221, loss = 3.68928905
Iteration 222, loss = 3.68385486
Iteration 223, loss = 3.67825260
Iteration 224, loss = 3.67289519
Iteration 225, loss = 3.66751721
Iteration 226, loss = 3.66055992
Iteration 227, loss = 3.65473614
Iteration 228, loss = 3.65109400
Iteration 229, loss = 3.64445390
Iteration 230, loss = 3.64007056
Iteration 231, loss = 3.63433926
Iteration 232, loss = 3.62854703
Iteration 233, loss = 3.62301587
Iteration 234, loss = 3.61656273
Iteration 235, loss = 3.61265552
Iteration 236, loss = 3.60685215
Iteration 237, loss = 3.60099432
Iteration 238, loss = 3.59627713
Iteration 239, loss = 3.59044122
Iteration 240, loss = 3.58562566
Iteration 241, loss = 3.57973252
Iteration 242, loss = 3.57526220
Iteration 243, loss = 3.56984496
Iteration 244, loss = 3.56542404
Iteration 245, loss = 3.55954730
Iteration 246, loss = 3.55439756
Iteration 247, loss = 3.54944366
Iteration 248, loss = 3.54322488
Iteration 249, loss = 3.53896188
Iteration 250, loss = 3.53421199
Iteration 251, loss = 3.52872907
Iteration 252, loss = 3.52350840
Iteration 253, loss = 3.51966297
Iteration 254, loss = 3.51444468
Iteration 255, loss = 3.50879194
Iteration 256, loss = 3.50380349
Iteration 257, loss = 3.49944902
Iteration 258, loss = 3.49470623
Iteration 259, loss = 3.48828836
Iteration 260, loss = 3.48404569
Iteration 261, loss = 3.47981777
Iteration 262, loss = 3.47357232
Iteration 263, loss = 3.47027763
Iteration 264, loss = 3.46523490
Iteration 265, loss = 3.46000202
Iteration 266, loss = 3.45565504
Iteration 267, loss = 3.45162690
Iteration 268, loss = 3.44731962
Iteration 269, loss = 3.44093941
Iteration 270, loss = 3.43708799
Iteration 271, loss = 3.43271527
Iteration 272, loss = 3.42726105
Iteration 273, loss = 3.42350903
Iteration 274, loss = 3.41859783
Iteration 275, loss = 3.41471716
Iteration 276, loss = 3.40899302
Iteration 277, loss = 3.40472078
Iteration 278, loss = 3.40075436
Iteration 279, loss = 3.39557301
Iteration 280, loss = 3.39173786
Iteration 281, loss = 3.38785569
Iteration 282, loss = 3.38372749
Iteration 283, loss = 3.37833295
Iteration 284, loss = 3.37445116
Iteration 285, loss = 3.36959623
Iteration 286, loss = 3.36569575
Iteration 287, loss = 3.36146772
Iteration 288, loss = 3.35561854
Iteration 289, loss = 3.35275664
Iteration 290, loss = 3.34779249
Iteration 291, loss = 3.34444491
Iteration 292, loss = 3.34058421
Iteration 293, loss = 3.33533551
Iteration 294, loss = 3.33084898
Iteration 295, loss = 3.32674976
Iteration 296, loss = 3.32176230
Iteration 297, loss = 3.31898370
Iteration 298, loss = 3.31553167
Iteration 299, loss = 3.30947413
Iteration 300, loss = 3.30588579
Iteration 301, loss = 3.30224952
Iteration 302, loss = 3.29783307
Iteration 303, loss = 3.29438200
Iteration 304, loss = 3.28976702
Iteration 305, loss = 3.28639316
Iteration 306, loss = 3.28204778
Iteration 307, loss = 3.27824100
Iteration 308, loss = 3.27395028
Iteration 309, loss = 3.26930102
Iteration 310, loss = 3.26515379
Iteration 311, loss = 3.26230611
Iteration 312, loss = 3.25713100
Iteration 313, loss = 3.25342565
Iteration 314, loss = 3.24958861
Iteration 315, loss = 3.24708907
Iteration 316, loss = 3.24255164
Iteration 317, loss = 3.23837685
Iteration 318, loss = 3.23478463
Iteration 319, loss = 3.23041314
Iteration 320, loss = 3.22631370
Iteration 321, loss = 3.22289562
Iteration 322, loss = 3.21883882
Iteration 323, loss = 3.21560726
Iteration 324, loss = 3.21188273
Iteration 325, loss = 3.20791067
Iteration 326, loss = 3.20543769
Iteration 327, loss = 3.19993268
Iteration 328, loss = 3.19728055
Iteration 329, loss = 3.19392748
Iteration 330, loss = 3.18958071
Iteration 331, loss = 3.18620388
Iteration 332, loss = 3.18198160
Iteration 333, loss = 3.17907850
Iteration 334, loss = 3.17484453
Iteration 335, loss = 3.17139162
Iteration 336, loss = 3.16821403
Iteration 337, loss = 3.16417149
Iteration 338, loss = 3.16005097
Iteration 339, loss = 3.15656754
Iteration 340, loss = 3.15353332
Iteration 341, loss = 3.14974234
Iteration 342, loss = 3.14671355
Iteration 343, loss = 3.14350287
Iteration 344, loss = 3.13993152
Iteration 345, loss = 3.13610410
Iteration 346, loss = 3.13342658
Iteration 347, loss = 3.12800505
Iteration 348, loss = 3.12537693
Iteration 349, loss = 3.12176368
Iteration 350, loss = 3.11893319
Iteration 351, loss = 3.11612087
Iteration 352, loss = 3.11156596
Iteration 353, loss = 3.10878548
Iteration 354, loss = 3.10460566
Iteration 355, loss = 3.10107677
Iteration 356, loss = 3.09847145
Iteration 357, loss = 3.09624443
Iteration 358, loss = 3.09199842
Iteration 359, loss = 3.08771776
Iteration 360, loss = 3.08559789
Iteration 361, loss = 3.08188786
Iteration 362, loss = 3.07783400
Iteration 363, loss = 3.07612797
Iteration 364, loss = 3.07214746
Iteration 365, loss = 3.06827493
Iteration 366, loss = 3.06623410
Iteration 367, loss = 3.06186754
Iteration 368, loss = 3.05936582
Iteration 369, loss = 3.05532530
Iteration 370, loss = 3.05316165
Iteration 371, loss = 3.05015073
Iteration 372, loss = 3.04686635
Iteration 373, loss = 3.04318533
Iteration 374, loss = 3.04019793
Iteration 375, loss = 3.03620781
Iteration 376, loss = 3.03349706
Iteration 377, loss = 3.03186505
Iteration 378, loss = 3.02793079
Iteration 379, loss = 3.02428388
Iteration 380, loss = 3.02169020
Iteration 381, loss = 3.01840021
Iteration 382, loss = 3.01596091
Iteration 383, loss = 3.01167445
Iteration 384, loss = 3.00991535
Iteration 385, loss = 3.00707602
Iteration 386, loss = 3.00282204
Iteration 387, loss = 2.99984808
Iteration 388, loss = 2.99747678
Iteration 389, loss = 2.99491242
Iteration 390, loss = 2.99166807
Iteration 391, loss = 2.98750636
Iteration 392, loss = 2.98558078
Iteration 393, loss = 2.98264050
Iteration 394, loss = 2.97918342
Iteration 395, loss = 2.97688390
Iteration 396, loss = 2.97387255
Iteration 397, loss = 2.97136726
Iteration 398, loss = 2.96797115
Iteration 399, loss = 2.96475116
Iteration 400, loss = 2.96197515
Iteration 401, loss = 2.95966443
Iteration 402, loss = 2.95570020
Iteration 403, loss = 2.95343043
Iteration 404, loss = 2.95102752
Iteration 405, loss = 2.94803980
Iteration 406, loss = 2.94559746
Iteration 407, loss = 2.94252596
Iteration 408, loss = 2.93992038
Iteration 409, loss = 2.93659452
Iteration 410, loss = 2.93351286
Iteration 411, loss = 2.93084400
Iteration 412, loss = 2.92771233
Iteration 413, loss = 2.92529311
Iteration 414, loss = 2.92259566
Iteration 415, loss = 2.91909641
Iteration 416, loss = 2.91740570
Iteration 417, loss = 2.91509009
Iteration 418, loss = 2.91277200
Iteration 419, loss = 2.90914550
Iteration 420, loss = 2.90708660
Iteration 421, loss = 2.90366313
Iteration 422, loss = 2.90101402
Iteration 423, loss = 2.89852854
Iteration 424, loss = 2.89527977
Iteration 425, loss = 2.89193064
Iteration 426, loss = 2.89054750
Iteration 427, loss = 2.88831294
Iteration 428, loss = 2.88472682
Iteration 429, loss = 2.88264780
Iteration 430, loss = 2.87944479
Iteration 431, loss = 2.87725708
Iteration 432, loss = 2.87458250
Iteration 433, loss = 2.87262736
Iteration 434, loss = 2.86916452
Iteration 435, loss = 2.86732798
Iteration 436, loss = 2.86418633
Iteration 437, loss = 2.86179179
Iteration 438, loss = 2.85849185
Iteration 439, loss = 2.85679237
Iteration 440, loss = 2.85407657
Iteration 441, loss = 2.85167866
Iteration 442, loss = 2.84975400
Iteration 443, loss = 2.84645389
Iteration 444, loss = 2.84350523
Iteration 445, loss = 2.84192908
Iteration 446, loss = 2.83852094
Iteration 447, loss = 2.83669614
Iteration 448, loss = 2.83361830
Iteration 449, loss = 2.83075831
Iteration 450, loss = 2.82898189
Iteration 451, loss = 2.82625916
Iteration 452, loss = 2.82330189
Iteration 453, loss = 2.82160699
Iteration 454, loss = 2.81875925
Iteration 455, loss = 2.81717469
Iteration 456, loss = 2.81492017
Iteration 457, loss = 2.81204402
Iteration 458, loss = 2.80888186
Iteration 459, loss = 2.80674215
Iteration 460, loss = 2.80473445
Iteration 461, loss = 2.80346523
Iteration 462, loss = 2.80106773
Iteration 463, loss = 2.79810194
Iteration 464, loss = 2.79584845
Iteration 465, loss = 2.79223417
Iteration 466, loss = 2.79034863
Iteration 467, loss = 2.78855423
Iteration 468, loss = 2.78658395
Iteration 469, loss = 2.78345677
Iteration 470, loss = 2.78149057
Iteration 471, loss = 2.77898527
Iteration 472, loss = 2.77641106
Iteration 473, loss = 2.77497373
Iteration 474, loss = 2.77254633
Iteration 475, loss = 2.76968209
Iteration 476, loss = 2.76738982
Iteration 477, loss = 2.76500870
Iteration 478, loss = 2.76348446
Iteration 479, loss = 2.76068100
Iteration 480, loss = 2.75923337
Iteration 481, loss = 2.75605252
Iteration 482, loss = 2.75347022
Iteration 483, loss = 2.75095847
Iteration 484, loss = 2.74933220
Iteration 485, loss = 2.74682327
Iteration 486, loss = 2.74592620
Iteration 487, loss = 2.74340420
Iteration 488, loss = 2.74043655
Iteration 489, loss = 2.73756587
Iteration 490, loss = 2.73669459
Iteration 491, loss = 2.73365036
Iteration 492, loss = 2.73129648
Iteration 493, loss = 2.72990375
Iteration 494, loss = 2.72749073
Iteration 495, loss = 2.72513372
Iteration 496, loss = 2.72309488
Iteration 497, loss = 2.72135593
Iteration 498, loss = 2.71927704
Iteration 499, loss = 2.71666314
Iteration 500, loss = 2.71343370
Iteration 501, loss = 2.71256399
Iteration 502, loss = 2.71054729
Iteration 503, loss = 2.70806171
Iteration 504, loss = 2.70676755
Iteration 505, loss = 2.70408193
Iteration 506, loss = 2.70224785
Iteration 507, loss = 2.70028856
Iteration 508, loss = 2.69808537
Iteration 509, loss = 2.69514169
Iteration 510, loss = 2.69398882
Iteration 511, loss = 2.69228346
Iteration 512, loss = 2.68951222
Iteration 513, loss = 2.68724863
Iteration 514, loss = 2.68545236
Iteration 515, loss = 2.68374691
Iteration 516, loss = 2.68091553
Iteration 517, loss = 2.67852686
Iteration 518, loss = 2.67771399
Iteration 519, loss = 2.67427615
Iteration 520, loss = 2.67242476
Iteration 521, loss = 2.67206274
Iteration 522, loss = 2.66948224
Iteration 523, loss = 2.66716855
Iteration 524, loss = 2.66503452
Iteration 525, loss = 2.66321579
Iteration 526, loss = 2.66113894
Iteration 527, loss = 2.65885913
Iteration 528, loss = 2.65685666
Iteration 529, loss = 2.65492016
Iteration 530, loss = 2.65277398
Iteration 531, loss = 2.65179801
Iteration 532, loss = 2.64959460
Iteration 533, loss = 2.64697698
Iteration 534, loss = 2.64587306
Iteration 535, loss = 2.64433143
Iteration 536, loss = 2.64201859
Iteration 537, loss = 2.63966892
Iteration 538, loss = 2.63738526
Iteration 539, loss = 2.63595366
Iteration 540, loss = 2.63411515
Iteration 541, loss = 2.63198881
Iteration 542, loss = 2.63070030
Iteration 543, loss = 2.62915311
Iteration 544, loss = 2.62652626
Iteration 545, loss = 2.62435635
Iteration 546, loss = 2.62272038
Iteration 547, loss = 2.62084568
Iteration 548, loss = 2.61916757
Iteration 549, loss = 2.61629355
Iteration 550, loss = 2.61547523
Iteration 551, loss = 2.61331465
Iteration 552, loss = 2.61165592
Iteration 553, loss = 2.60985306
Iteration 554, loss = 2.60709329
Iteration 555, loss = 2.60607955
Iteration 556, loss = 2.60332236
Iteration 557, loss = 2.60242606
Iteration 558, loss = 2.60096474
Iteration 559, loss = 2.59956853
Iteration 560, loss = 2.59684554
Iteration 561, loss = 2.59487878
Iteration 562, loss = 2.59320512
Iteration 563, loss = 2.59206559
Iteration 564, loss = 2.59016140
Iteration 565, loss = 2.58854926
Iteration 566, loss = 2.58605594
Iteration 567, loss = 2.58473001
Iteration 568, loss = 2.58268092
Iteration 569, loss = 2.58190282
Iteration 570, loss = 2.57912956
Iteration 571, loss = 2.57641431
Iteration 572, loss = 2.57632649
Iteration 573, loss = 2.57415174
Iteration 574, loss = 2.57220649
Iteration 575, loss = 2.56986820
Iteration 576, loss = 2.56914028
Iteration 577, loss = 2.56720308
Iteration 578, loss = 2.56494626
Iteration 579, loss = 2.56355225
Iteration 580, loss = 2.56263270
Iteration 581, loss = 2.56057780
Iteration 582, loss = 2.55779559
Iteration 583, loss = 2.55646512
Iteration 584, loss = 2.55501229
Iteration 585, loss = 2.55301837
Iteration 586, loss = 2.55187788
Iteration 587, loss = 2.54930574
Iteration 588, loss = 2.54916015
Iteration 589, loss = 2.54686817
Iteration 590, loss = 2.54505198
Iteration 591, loss = 2.54340361
Iteration 592, loss = 2.54112649
Iteration 593, loss = 2.53976717
Iteration 594, loss = 2.53862181
Iteration 595, loss = 2.53778977
Iteration 596, loss = 2.53450691
Iteration 597, loss = 2.53195818
Iteration 598, loss = 2.53157171
Iteration 599, loss = 2.52946797
Iteration 600, loss = 2.52850590
Iteration 601, loss = 2.52616556
Iteration 602, loss = 2.52506015
Iteration 603, loss = 2.52318674
Iteration 604, loss = 2.52140404
Iteration 605, loss = 2.52026809
Iteration 606, loss = 2.51889882
Iteration 607, loss = 2.51616344
Iteration 608, loss = 2.51507337
Iteration 609, loss = 2.51269246
Iteration 610, loss = 2.51175188
Iteration 611, loss = 2.51046361
Iteration 612, loss = 2.50887223
Iteration 613, loss = 2.50642346
Iteration 614, loss = 2.50555107
Iteration 615, loss = 2.50409433
Iteration 616, loss = 2.50269076
Iteration 617, loss = 2.50102613
Iteration 618, loss = 2.49941607
Iteration 619, loss = 2.49747950
Iteration 620, loss = 2.49660906
Iteration 621, loss = 2.49384942
Iteration 622, loss = 2.49299857
Iteration 623, loss = 2.49158038
Iteration 624, loss = 2.49031294
Iteration 625, loss = 2.48877012
Iteration 626, loss = 2.48731837
Iteration 627, loss = 2.48562160
Iteration 628, loss = 2.48345185
Iteration 629, loss = 2.48264823
Iteration 630, loss = 2.47975695
Iteration 631, loss = 2.47931241
Iteration 632, loss = 2.47830129
Iteration 633, loss = 2.47692634
Iteration 634, loss = 2.47524150
Iteration 635, loss = 2.47281560
Iteration 636, loss = 2.47229719
Iteration 637, loss = 2.47024264
Iteration 638, loss = 2.46892248
Iteration 639, loss = 2.46750237
Iteration 640, loss = 2.46566970
Iteration 641, loss = 2.46419527
Iteration 642, loss = 2.46233438
Iteration 643, loss = 2.46098605
Iteration 644, loss = 2.45980877
Iteration 645, loss = 2.45913638
Iteration 646, loss = 2.45759394
Iteration 647, loss = 2.45517776
Iteration 648, loss = 2.45432448
Iteration 649, loss = 2.45251230
Iteration 650, loss = 2.45122009
Iteration 651, loss = 2.44982993
Iteration 652, loss = 2.44766377
Iteration 653, loss = 2.44620946
Iteration 654, loss = 2.44569260
Iteration 655, loss = 2.44346323
Iteration 656, loss = 2.44135005
Iteration 657, loss = 2.44109509
Iteration 658, loss = 2.43915466
Iteration 659, loss = 2.43868497
Iteration 660, loss = 2.43698683
Iteration 661, loss = 2.43549997
Iteration 662, loss = 2.43450306
Iteration 663, loss = 2.43324014
Iteration 664, loss = 2.43206892
Iteration 665, loss = 2.42954744
Iteration 666, loss = 2.42847596
Iteration 667, loss = 2.42653062
Iteration 668, loss = 2.42536808
Iteration 669, loss = 2.42461861
Iteration 670, loss = 2.42241122
Iteration 671, loss = 2.42187489
Iteration 672, loss = 2.41984960
Iteration 673, loss = 2.41824020
Iteration 674, loss = 2.41691238
Iteration 675, loss = 2.41679703
Iteration 676, loss = 2.41463600
Iteration 677, loss = 2.41281677
Iteration 678, loss = 2.41156032
Iteration 679, loss = 2.41015929
Iteration 680, loss = 2.40916527
Iteration 681, loss = 2.40727457
Iteration 682, loss = 2.40671255
Iteration 683, loss = 2.40545669
Iteration 684, loss = 2.40289429
Iteration 685, loss = 2.40242614
Iteration 686, loss = 2.40058333
Iteration 687, loss = 2.39973999
Iteration 688, loss = 2.39778780
Iteration 689, loss = 2.39767418
Iteration 690, loss = 2.39673408
Iteration 691, loss = 2.39468039
Iteration 692, loss = 2.39299169
Iteration 693, loss = 2.39148097
Iteration 694, loss = 2.39009270
Iteration 695, loss = 2.38898854
Iteration 696, loss = 2.38703227
Iteration 697, loss = 2.38722221
Iteration 698, loss = 2.38607326
Iteration 699, loss = 2.38470663
Iteration 700, loss = 2.38309574
Iteration 701, loss = 2.38223060
Iteration 702, loss = 2.38001576
Iteration 703, loss = 2.37899458
Iteration 704, loss = 2.37726692
Iteration 705, loss = 2.37613668
Iteration 706, loss = 2.37409957
Iteration 707, loss = 2.37378045
Iteration 708, loss = 2.37292028
Iteration 709, loss = 2.37127146
Iteration 710, loss = 2.36960697
Iteration 711, loss = 2.36788807
Iteration 712, loss = 2.36763296
Iteration 713, loss = 2.36626404
Iteration 714, loss = 2.36436295
Iteration 715, loss = 2.36341829
Iteration 716, loss = 2.36304094
Iteration 717, loss = 2.36043713
Iteration 718, loss = 2.35991285
Iteration 719, loss = 2.35886978
Iteration 720, loss = 2.35732450
Iteration 721, loss = 2.35657288
Iteration 722, loss = 2.35551433
Iteration 723, loss = 2.35435402
Iteration 724, loss = 2.35195363
Iteration 725, loss = 2.35095324
Iteration 726, loss = 2.34987153
Iteration 727, loss = 2.34833441
Iteration 728, loss = 2.34721324
Iteration 729, loss = 2.34648514
Iteration 730, loss = 2.34451744
Iteration 731, loss = 2.34414325
Iteration 732, loss = 2.34214709
Iteration 733, loss = 2.34061035
Iteration 734, loss = 2.33950111
Iteration 735, loss = 2.33882385
Iteration 736, loss = 2.33791030
Iteration 737, loss = 2.33646122
Iteration 738, loss = 2.33518614
Iteration 739, loss = 2.33464231
Iteration 740, loss = 2.33216425
Iteration 741, loss = 2.33232564
Iteration 742, loss = 2.33032145
Iteration 743, loss = 2.32933084
Iteration 744, loss = 2.32840868
Iteration 745, loss = 2.32683223
Iteration 746, loss = 2.32620181
Iteration 747, loss = 2.32439857
Iteration 748, loss = 2.32333808
Iteration 749, loss = 2.32276303
Iteration 750, loss = 2.32118804
Iteration 751, loss = 2.32006189
Iteration 752, loss = 2.31887867
Iteration 753, loss = 2.31736722
Iteration 754, loss = 2.31635832
Iteration 755, loss = 2.31624933
Iteration 756, loss = 2.31431253
Iteration 757, loss = 2.31386060
Iteration 758, loss = 2.31243101
Iteration 759, loss = 2.31066183
Iteration 760, loss = 2.30899065
Iteration 761, loss = 2.30765055
Iteration 762, loss = 2.30786270
Iteration 763, loss = 2.30658392
Iteration 764, loss = 2.30524951
Iteration 765, loss = 2.30397546
Iteration 766, loss = 2.30268583
Iteration 767, loss = 2.30112142
Iteration 768, loss = 2.30010789
Iteration 769, loss = 2.29931640
Iteration 770, loss = 2.29826979
Iteration 771, loss = 2.29793137
Iteration 772, loss = 2.29659173
Iteration 773, loss = 2.29494010
Iteration 774, loss = 2.29347317
Iteration 775, loss = 2.29313503
Iteration 776, loss = 2.29117228
Iteration 777, loss = 2.29030907
Iteration 778, loss = 2.28845278
Iteration 779, loss = 2.28818810
Iteration 780, loss = 2.28753884
Iteration 781, loss = 2.28558457
Iteration 782, loss = 2.28501103
Iteration 783, loss = 2.28331918
Iteration 784, loss = 2.28315884
Iteration 785, loss = 2.28148238
Iteration 786, loss = 2.27992121
Iteration 787, loss = 2.27990781
Iteration 788, loss = 2.27887606
Iteration 789, loss = 2.27735649
Iteration 790, loss = 2.27576727
Iteration 791, loss = 2.27491034
Iteration 792, loss = 2.27428819
Iteration 793, loss = 2.27335886
Iteration 794, loss = 2.27227043
Iteration 795, loss = 2.27040893
Iteration 796, loss = 2.26994486
Iteration 797, loss = 2.26872456
Iteration 798, loss = 2.26802746
Iteration 799, loss = 2.26652835
Iteration 800, loss = 2.26536306
Iteration 801, loss = 2.26408895
Iteration 802, loss = 2.26372281
Iteration 803, loss = 2.26237745
Iteration 804, loss = 2.26125488
Iteration 805, loss = 2.25988735
Iteration 806, loss = 2.25924001
Iteration 807, loss = 2.25810611
Iteration 808, loss = 2.25715095
Iteration 809, loss = 2.25652301
Iteration 810, loss = 2.25453940
Iteration 811, loss = 2.25374467
Iteration 812, loss = 2.25317488
Iteration 813, loss = 2.25211515
Iteration 814, loss = 2.25169814
Iteration 815, loss = 2.24983213
Iteration 816, loss = 2.24862322
Iteration 817, loss = 2.24763971
Iteration 818, loss = 2.24669600
Iteration 819, loss = 2.24541227
Iteration 820, loss = 2.24526451
Iteration 821, loss = 2.24444715
Iteration 822, loss = 2.24358035
Iteration 823, loss = 2.24216474
Iteration 824, loss = 2.24013677
Iteration 825, loss = 2.23991364
Iteration 826, loss = 2.23937608
Iteration 827, loss = 2.23797271
Iteration 828, loss = 2.23754037
Iteration 829, loss = 2.23582249
Iteration 830, loss = 2.23465863
Iteration 831, loss = 2.23376052
Iteration 832, loss = 2.23307077
Iteration 833, loss = 2.23117313
Iteration 834, loss = 2.22980670
Iteration 835, loss = 2.23044405
Iteration 836, loss = 2.22935517
Iteration 837, loss = 2.22716749
Iteration 838, loss = 2.22684077
Iteration 839, loss = 2.22659484
Iteration 840, loss = 2.22606200
Iteration 841, loss = 2.22433328
Iteration 842, loss = 2.22308893
Iteration 843, loss = 2.22203631
Iteration 844, loss = 2.22098450
Iteration 845, loss = 2.22027665
Iteration 846, loss = 2.21877417
Iteration 847, loss = 2.21865486
Iteration 848, loss = 2.21721922
Iteration 849, loss = 2.21681628
Iteration 850, loss = 2.21526069
Iteration 851, loss = 2.21367205
Iteration 852, loss = 2.21298628
Iteration 853, loss = 2.21247956
Iteration 854, loss = 2.21129788
Iteration 855, loss = 2.21044177
Iteration 856, loss = 2.20970620
Iteration 857, loss = 2.20867548
Iteration 858, loss = 2.20717830
Iteration 859, loss = 2.20571427
Iteration 860, loss = 2.20560777
Iteration 861, loss = 2.20446616
Iteration 862, loss = 2.20392914
Iteration 863, loss = 2.20294768
Iteration 864, loss = 2.20212835
Iteration 865, loss = 2.20070757
Iteration 866, loss = 2.20001537
Iteration 867, loss = 2.19864380
Iteration 868, loss = 2.19809427
Iteration 869, loss = 2.19757944
Iteration 870, loss = 2.19674582
Iteration 871, loss = 2.19536206
Iteration 872, loss = 2.19488428
Iteration 873, loss = 2.19341277
Iteration 874, loss = 2.19229731
Iteration 875, loss = 2.19143160
Iteration 876, loss = 2.19072440
Iteration 877, loss = 2.19022837
Iteration 878, loss = 2.18922146
Iteration 879, loss = 2.18707892
Iteration 880, loss = 2.18734479
Iteration 881, loss = 2.18622477
Iteration 882, loss = 2.18566790
Iteration 883, loss = 2.18447774
Iteration 884, loss = 2.18448267
Iteration 885, loss = 2.18288424
Iteration 886, loss = 2.18176311
Iteration 887, loss = 2.18132449
Iteration 888, loss = 2.18056363
Iteration 889, loss = 2.17868776
Iteration 890, loss = 2.17820381
Iteration 891, loss = 2.17751048
Iteration 892, loss = 2.17594302
Iteration 893, loss = 2.17536686
Iteration 894, loss = 2.17369986
Iteration 895, loss = 2.17357404
Iteration 896, loss = 2.17250250
Iteration 897, loss = 2.17071379
Iteration 898, loss = 2.17021971
Iteration 899, loss = 2.16984054
Iteration 900, loss = 2.16858953
Iteration 901, loss = 2.16810148
Iteration 902, loss = 2.16718646
Iteration 903, loss = 2.16691511
Iteration 904, loss = 2.16613014
Iteration 905, loss = 2.16440008
Iteration 906, loss = 2.16407244
Iteration 907, loss = 2.16322652
Iteration 908, loss = 2.16252217
Iteration 909, loss = 2.16146059
Iteration 910, loss = 2.16008771
Iteration 911, loss = 2.15931074
Iteration 912, loss = 2.15822916
Iteration 913, loss = 2.15773134
Iteration 914, loss = 2.15751750
Iteration 915, loss = 2.15642427
Iteration 916, loss = 2.15498148
Iteration 917, loss = 2.15426328
Iteration 918, loss = 2.15335284
Iteration 919, loss = 2.15324679
Iteration 920, loss = 2.15220481
Iteration 921, loss = 2.14956733
Iteration 922, loss = 2.15009080
Iteration 923, loss = 2.14942292
Iteration 924, loss = 2.14826924
Iteration 925, loss = 2.14748813
Iteration 926, loss = 2.14698333
Iteration 927, loss = 2.14634654
Iteration 928, loss = 2.14449360
Iteration 929, loss = 2.14377977
Iteration 930, loss = 2.14402190
Iteration 931, loss = 2.14252583
Iteration 932, loss = 2.14164995
Iteration 933, loss = 2.14012014
Iteration 934, loss = 2.13968167
Iteration 935, loss = 2.13927348
Iteration 936, loss = 2.13856447
Iteration 937, loss = 2.13854330
Iteration 938, loss = 2.13590249
Iteration 939, loss = 2.13570912
Iteration 940, loss = 2.13563783
Iteration 941, loss = 2.13446372
Iteration 942, loss = 2.13396485
Iteration 943, loss = 2.13220086
Iteration 944, loss = 2.13221385
Iteration 945, loss = 2.13101915
Iteration 946, loss = 2.13023578
Iteration 947, loss = 2.13018680
Iteration 948, loss = 2.12867589
Iteration 949, loss = 2.12831851
Iteration 950, loss = 2.12672639
Iteration 951, loss = 2.12699521
Iteration 952, loss = 2.12550579
Iteration 953, loss = 2.12498334
Iteration 954, loss = 2.12404929
Iteration 955, loss = 2.12324523
Iteration 956, loss = 2.12175884
Iteration 957, loss = 2.12106409
Iteration 958, loss = 2.12096695
Iteration 959, loss = 2.11974740
Iteration 960, loss = 2.11880963
Iteration 961, loss = 2.11853535
Iteration 962, loss = 2.11749281
Iteration 963, loss = 2.11647234
Iteration 964, loss = 2.11601349
Iteration 965, loss = 2.11502222
Iteration 966, loss = 2.11473593
Iteration 967, loss = 2.11329244
Iteration 968, loss = 2.11303324
Iteration 969, loss = 2.11192278
Iteration 970, loss = 2.11092101
Iteration 971, loss = 2.11047403
Iteration 972, loss = 2.10938486
Iteration 973, loss = 2.10965067
Iteration 974, loss = 2.10717113
Iteration 975, loss = 2.10692301
Iteration 976, loss = 2.10720755
Iteration 977, loss = 2.10523534
Iteration 978, loss = 2.10524215
Iteration 979, loss = 2.10405794
Iteration 980, loss = 2.10339605
Iteration 981, loss = 2.10232987
Iteration 982, loss = 2.10159470
Iteration 983, loss = 2.10120264
Iteration 984, loss = 2.10062759
Iteration 985, loss = 2.09916306
Iteration 986, loss = 2.09980246
Iteration 987, loss = 2.09769076
Iteration 988, loss = 2.09739110
Iteration 989, loss = 2.09621231
Iteration 990, loss = 2.09600899
Iteration 991, loss = 2.09441565
Iteration 992, loss = 2.09439545
Iteration 993, loss = 2.09410130
Iteration 994, loss = 2.09319577
Iteration 995, loss = 2.09197120
Iteration 996, loss = 2.09157895
Iteration 997, loss = 2.09116391
Iteration 998, loss = 2.08927300
Iteration 999, loss = 2.08895891
Iteration 1000, loss = 2.08880737
Iteration 1001, loss = 2.08772149
Iteration 1002, loss = 2.08703437
Iteration 1003, loss = 2.08611571
Iteration 1004, loss = 2.08580791
Iteration 1005, loss = 2.08467723
Iteration 1006, loss = 2.08429200
Iteration 1007, loss = 2.08265125
Iteration 1008, loss = 2.08219871
Iteration 1009, loss = 2.08110689
Iteration 1010, loss = 2.08115281
Iteration 1011, loss = 2.07940268
Iteration 1012, loss = 2.08005035
Iteration 1013, loss = 2.07917242
Iteration 1014, loss = 2.07800475
Iteration 1015, loss = 2.07716451
Iteration 1016, loss = 2.07614267
Iteration 1017, loss = 2.07596122
Iteration 1018, loss = 2.07552883
Iteration 1019, loss = 2.07460490
Iteration 1020, loss = 2.07407268
Iteration 1021, loss = 2.07266736
Iteration 1022, loss = 2.07260798
Iteration 1023, loss = 2.07088835
Iteration 1024, loss = 2.07105452
Iteration 1025, loss = 2.06959575
Iteration 1026, loss = 2.06981031
Iteration 1027, loss = 2.06899837
Iteration 1028, loss = 2.06772979
Iteration 1029, loss = 2.06692005
Iteration 1030, loss = 2.06642891
Iteration 1031, loss = 2.06487961
Iteration 1032, loss = 2.06407641
Iteration 1033, loss = 2.06508845
Iteration 1034, loss = 2.06248913
Iteration 1035, loss = 2.06331003
Iteration 1036, loss = 2.06136472
Iteration 1037, loss = 2.06132738
Iteration 1038, loss = 2.06122111
Iteration 1039, loss = 2.06017039
Iteration 1040, loss = 2.05944847
Iteration 1041, loss = 2.05837120
Iteration 1042, loss = 2.05769883
Iteration 1043, loss = 2.05695945
Iteration 1044, loss = 2.05697734
Iteration 1045, loss = 2.05578875
Iteration 1046, loss = 2.05586694
Iteration 1047, loss = 2.05374493
Iteration 1048, loss = 2.05311349
Iteration 1049, loss = 2.05308365
Iteration 1050, loss = 2.05298699
Iteration 1051, loss = 2.05189578
Iteration 1052, loss = 2.05176486
Iteration 1053, loss = 2.05025155
Iteration 1054, loss = 2.04970967
Iteration 1055, loss = 2.04907893
Iteration 1056, loss = 2.04806289
Iteration 1057, loss = 2.04763539
Iteration 1058, loss = 2.04720484
Iteration 1059, loss = 2.04615454
Iteration 1060, loss = 2.04606833
Iteration 1061, loss = 2.04380316
Iteration 1062, loss = 2.04421431
Iteration 1063, loss = 2.04309640
Iteration 1064, loss = 2.04286691
Iteration 1065, loss = 2.04160381
Iteration 1066, loss = 2.04184717
Iteration 1067, loss = 2.04072879
Iteration 1068, loss = 2.04029313
Iteration 1069, loss = 2.03971211
Iteration 1070, loss = 2.03899002
Iteration 1071, loss = 2.03745878
Iteration 1072, loss = 2.03682941
Iteration 1073, loss = 2.03697181
Iteration 1074, loss = 2.03599756
Iteration 1075, loss = 2.03595099
Iteration 1076, loss = 2.03513557
Iteration 1077, loss = 2.03362144
Iteration 1078, loss = 2.03311009
Iteration 1079, loss = 2.03266981
Iteration 1080, loss = 2.03163930
Iteration 1081, loss = 2.03119532
Iteration 1082, loss = 2.03029669
Iteration 1083, loss = 2.03025655
Iteration 1084, loss = 2.02894398
Iteration 1085, loss = 2.02931476
Iteration 1086, loss = 2.02838741
Iteration 1087, loss = 2.02666055
Iteration 1088, loss = 2.02766076
Iteration 1089, loss = 2.02629125
Iteration 1090, loss = 2.02555660
Iteration 1091, loss = 2.02532590
Iteration 1092, loss = 2.02477521
Iteration 1093, loss = 2.02371652
Iteration 1094, loss = 2.02400577
Iteration 1095, loss = 2.02199423
Iteration 1096, loss = 2.02180863
Iteration 1097, loss = 2.02100522
Iteration 1098, loss = 2.02068263
Iteration 1099, loss = 2.01957022
Iteration 1100, loss = 2.01917859
Iteration 1101, loss = 2.01749760
Iteration 1102, loss = 2.01748413
Iteration 1103, loss = 2.01646771
Iteration 1104, loss = 2.01709759
Iteration 1105, loss = 2.01562273
Iteration 1106, loss = 2.01610792
Iteration 1107, loss = 2.01456579
Iteration 1108, loss = 2.01407911
Iteration 1109, loss = 2.01346068
Iteration 1110, loss = 2.01303849
Iteration 1111, loss = 2.01210069
Iteration 1112, loss = 2.01126973
Iteration 1113, loss = 2.01087211
Iteration 1114, loss = 2.01046869
Iteration 1115, loss = 2.00935594
Iteration 1116, loss = 2.00937758
Iteration 1117, loss = 2.00901402
Iteration 1118, loss = 2.00775763
Iteration 1119, loss = 2.00701111
Iteration 1120, loss = 2.00591519
Iteration 1121, loss = 2.00592108
Iteration 1122, loss = 2.00454977
Iteration 1123, loss = 2.00481329
Iteration 1124, loss = 2.00383589
Iteration 1125, loss = 2.00366064
Iteration 1126, loss = 2.00310072
Iteration 1127, loss = 2.00276548
Iteration 1128, loss = 2.00162511
Iteration 1129, loss = 2.00055452
Iteration 1130, loss = 2.00010620
Iteration 1131, loss = 1.99896367
Iteration 1132, loss = 1.99898493
Iteration 1133, loss = 1.99856679
Iteration 1134, loss = 1.99710831
Iteration 1135, loss = 1.99652164
Iteration 1136, loss = 1.99627343
Iteration 1137, loss = 1.99632902
Iteration 1138, loss = 1.99483606
Iteration 1139, loss = 1.99416160
Iteration 1140, loss = 1.99426371
Iteration 1141, loss = 1.99356887
Iteration 1142, loss = 1.99302331
Iteration 1143, loss = 1.99302448
Iteration 1144, loss = 1.99171314
Iteration 1145, loss = 1.99043932
Iteration 1146, loss = 1.99057690
Iteration 1147, loss = 1.99062147
Iteration 1148, loss = 1.98898494
Iteration 1149, loss = 1.98887876
Iteration 1150, loss = 1.98803935
Iteration 1151, loss = 1.98730455
Iteration 1152, loss = 1.98677429
Iteration 1153, loss = 1.98643016
Iteration 1154, loss = 1.98579933
Iteration 1155, loss = 1.98474976
Iteration 1156, loss = 1.98462011
Iteration 1157, loss = 1.98388874
Iteration 1158, loss = 1.98446677
Iteration 1159, loss = 1.98312495
Iteration 1160, loss = 1.98231448
Iteration 1161, loss = 1.98153074
Iteration 1162, loss = 1.98046744
Iteration 1163, loss = 1.98000772
Iteration 1164, loss = 1.98038936
Iteration 1165, loss = 1.97902186
Iteration 1166, loss = 1.97827312
Iteration 1167, loss = 1.97799763
Iteration 1168, loss = 1.97749237
Iteration 1169, loss = 1.97670616
Iteration 1170, loss = 1.97586942
Iteration 1171, loss = 1.97556250
Iteration 1172, loss = 1.97590155
Iteration 1173, loss = 1.97425232
Iteration 1174, loss = 1.97437103
Iteration 1175, loss = 1.97310437
Iteration 1176, loss = 1.97301962
Iteration 1177, loss = 1.97224818
Iteration 1178, loss = 1.97228817
Iteration 1179, loss = 1.97025893
Iteration 1180, loss = 1.97091731
Iteration 1181, loss = 1.96980166
Iteration 1182, loss = 1.96929018
Iteration 1183, loss = 1.96853075
Iteration 1184, loss = 1.96781781
Iteration 1185, loss = 1.96751311
Iteration 1186, loss = 1.96727107
Iteration 1187, loss = 1.96686693
Iteration 1188, loss = 1.96640335
Iteration 1189, loss = 1.96517004
Iteration 1190, loss = 1.96446328
Iteration 1191, loss = 1.96396325
Iteration 1192, loss = 1.96386087
Iteration 1193, loss = 1.96321675
Iteration 1194, loss = 1.96208551
Iteration 1195, loss = 1.96156369
Iteration 1196, loss = 1.96144789
Iteration 1197, loss = 1.96074276
Iteration 1198, loss = 1.96103101
Iteration 1199, loss = 1.95982751
Iteration 1200, loss = 1.95900726
Iteration 1201, loss = 1.95829488
Iteration 1202, loss = 1.95749053
Iteration 1203, loss = 1.95720969
Iteration 1204, loss = 1.95700925
Iteration 1205, loss = 1.95622827
Iteration 1206, loss = 1.95546562
Iteration 1207, loss = 1.95468754
Iteration 1208, loss = 1.95525842
Iteration 1209, loss = 1.95434606
Iteration 1210, loss = 1.95364820
Iteration 1211, loss = 1.95288892
Iteration 1212, loss = 1.95219033
Iteration 1213, loss = 1.95139336
Iteration 1214, loss = 1.95104671
Iteration 1215, loss = 1.95138128
Iteration 1216, loss = 1.94958343
Iteration 1217, loss = 1.94980769
Iteration 1218, loss = 1.94929038
Iteration 1219, loss = 1.94849977
Iteration 1220, loss = 1.94785870
Iteration 1221, loss = 1.94771832
Iteration 1222, loss = 1.94753132
Iteration 1223, loss = 1.94682350
Iteration 1224, loss = 1.94539962
Iteration 1225, loss = 1.94449991
Iteration 1226, loss = 1.94513111
Iteration 1227, loss = 1.94409199
Iteration 1228, loss = 1.94415557
Iteration 1229, loss = 1.94367743
Iteration 1230, loss = 1.94264975
Iteration 1231, loss = 1.94235880
Iteration 1232, loss = 1.94134289
Iteration 1233, loss = 1.94096010
Iteration 1234, loss = 1.94085132
Iteration 1235, loss = 1.93992813
Iteration 1236, loss = 1.93889526
Iteration 1237, loss = 1.93929410
Iteration 1238, loss = 1.93878579
Iteration 1239, loss = 1.93816493
Iteration 1240, loss = 1.93737567
Iteration 1241, loss = 1.93677677
Iteration 1242, loss = 1.93618631
Iteration 1243, loss = 1.93601540
Iteration 1244, loss = 1.93492082
Iteration 1245, loss = 1.93434314
Iteration 1246, loss = 1.93401538
Iteration 1247, loss = 1.93353818
Iteration 1248, loss = 1.93247665
Iteration 1249, loss = 1.93262045
Iteration 1250, loss = 1.93263920
Iteration 1251, loss = 1.93090660
Iteration 1252, loss = 1.93103069
Iteration 1253, loss = 1.93037330
Iteration 1254, loss = 1.92962031
Iteration 1255, loss = 1.92922725
Iteration 1256, loss = 1.92927671
Iteration 1257, loss = 1.92904597
Iteration 1258, loss = 1.92826812
Iteration 1259, loss = 1.92770129
Iteration 1260, loss = 1.92645761
Iteration 1261, loss = 1.92607059
Iteration 1262, loss = 1.92596112
Iteration 1263, loss = 1.92583902
Iteration 1264, loss = 1.92445565
Iteration 1265, loss = 1.92435895
Iteration 1266, loss = 1.92336774
Iteration 1267, loss = 1.92279237
Iteration 1268, loss = 1.92204315
Iteration 1269, loss = 1.92322155
Iteration 1270, loss = 1.92161571
Iteration 1271, loss = 1.92080018
Iteration 1272, loss = 1.92079433
Iteration 1273, loss = 1.92000555
Iteration 1274, loss = 1.91979425
Iteration 1275, loss = 1.91931112
Iteration 1276, loss = 1.91828920
Iteration 1277, loss = 1.91878323
Iteration 1278, loss = 1.91704543
Iteration 1279, loss = 1.91684066
Iteration 1280, loss = 1.91633292
Iteration 1281, loss = 1.91592721
Iteration 1282, loss = 1.91497794
Iteration 1283, loss = 1.91506534
Iteration 1284, loss = 1.91457827
Iteration 1285, loss = 1.91472520
Iteration 1286, loss = 1.91423215
Iteration 1287, loss = 1.91383238
Iteration 1288, loss = 1.91283283
Iteration 1289, loss = 1.91228344
Iteration 1290, loss = 1.91176242
Iteration 1291, loss = 1.91127314
Iteration 1292, loss = 1.91058341
Iteration 1293, loss = 1.91050550
Iteration 1294, loss = 1.91006384
Iteration 1295, loss = 1.90912729
Iteration 1296, loss = 1.90892202
Iteration 1297, loss = 1.90812357
Iteration 1298, loss = 1.90722634
Iteration 1299, loss = 1.90761267
Iteration 1300, loss = 1.90720201
Iteration 1301, loss = 1.90574900
Iteration 1302, loss = 1.90562179
Iteration 1303, loss = 1.90537754
Iteration 1304, loss = 1.90563932
Iteration 1305, loss = 1.90449165
Iteration 1306, loss = 1.90329913
Iteration 1307, loss = 1.90393504
Iteration 1308, loss = 1.90282453
Iteration 1309, loss = 1.90317976
Iteration 1310, loss = 1.90153572
Iteration 1311, loss = 1.90178033
Iteration 1312, loss = 1.90082967
Iteration 1313, loss = 1.90078828
Iteration 1314, loss = 1.89904793
Iteration 1315, loss = 1.89918017
Iteration 1316, loss = 1.89827949
Iteration 1317, loss = 1.89800739
Iteration 1318, loss = 1.89838672
Iteration 1319, loss = 1.89782251
Iteration 1320, loss = 1.89735803
Iteration 1321, loss = 1.89579563
Iteration 1322, loss = 1.89646808
Iteration 1323, loss = 1.89518405
Iteration 1324, loss = 1.89479631
Iteration 1325, loss = 1.89382189
Iteration 1326, loss = 1.89319243
Iteration 1327, loss = 1.89371973
Iteration 1328, loss = 1.89264987
Iteration 1329, loss = 1.89282096
Iteration 1330, loss = 1.89198334
Iteration 1331, loss = 1.89198961
Iteration 1332, loss = 1.89131009
Iteration 1333, loss = 1.89111376
Iteration 1334, loss = 1.89052606
Iteration 1335, loss = 1.89041802
Iteration 1336, loss = 1.88988902
Iteration 1337, loss = 1.88932559
Iteration 1338, loss = 1.88825377
Iteration 1339, loss = 1.88762347
Iteration 1340, loss = 1.88746883
Iteration 1341, loss = 1.88694867
Iteration 1342, loss = 1.88599804
Iteration 1343, loss = 1.88555652
Iteration 1344, loss = 1.88476332
Iteration 1345, loss = 1.88468527
Iteration 1346, loss = 1.88517870
Iteration 1347, loss = 1.88448131
Iteration 1348, loss = 1.88381444
Iteration 1349, loss = 1.88308807
Iteration 1350, loss = 1.88207254
Iteration 1351, loss = 1.88153450
Iteration 1352, loss = 1.88197478
Iteration 1353, loss = 1.88123652
Iteration 1354, loss = 1.88036293
Iteration 1355, loss = 1.88061095
Iteration 1356, loss = 1.88087619
Iteration 1357, loss = 1.88000975
Iteration 1358, loss = 1.87896890
Iteration 1359, loss = 1.87788784
Iteration 1360, loss = 1.87808969
Iteration 1361, loss = 1.87772915
Iteration 1362, loss = 1.87708760
Iteration 1363, loss = 1.87676251
Iteration 1364, loss = 1.87616857
Iteration 1365, loss = 1.87544555
Iteration 1366, loss = 1.87463356
Iteration 1367, loss = 1.87467332
Iteration 1368, loss = 1.87469518
Iteration 1369, loss = 1.87455172
Iteration 1370, loss = 1.87419822
Iteration 1371, loss = 1.87247297
Iteration 1372, loss = 1.87201263
Iteration 1373, loss = 1.87240273
Iteration 1374, loss = 1.87126614
Iteration 1375, loss = 1.87095141
Iteration 1376, loss = 1.87135378
Iteration 1377, loss = 1.87121429
Iteration 1378, loss = 1.86919332
Iteration 1379, loss = 1.86938080
Iteration 1380, loss = 1.86867555
Iteration 1381, loss = 1.86888525
Iteration 1382, loss = 1.86832643
Iteration 1383, loss = 1.86683759
Iteration 1384, loss = 1.86693804
Iteration 1385, loss = 1.86724099
Iteration 1386, loss = 1.86619853
Iteration 1387, loss = 1.86619557
Iteration 1388, loss = 1.86547469
Iteration 1389, loss = 1.86540785
Iteration 1390, loss = 1.86494410
Iteration 1391, loss = 1.86431072
Iteration 1392, loss = 1.86441070
Iteration 1393, loss = 1.86331947
Iteration 1394, loss = 1.86211691
Iteration 1395, loss = 1.86301915
Iteration 1396, loss = 1.86211656
Iteration 1397, loss = 1.86158335
Iteration 1398, loss = 1.86084206
Iteration 1399, loss = 1.85995718
Iteration 1400, loss = 1.85950619
Iteration 1401, loss = 1.85907422
Iteration 1402, loss = 1.85933566
Iteration 1403, loss = 1.85910818
Iteration 1404, loss = 1.85847847
Iteration 1405, loss = 1.85677336
Iteration 1406, loss = 1.85697735
Iteration 1407, loss = 1.85738336
Iteration 1408, loss = 1.85653541
Iteration 1409, loss = 1.85634172
Iteration 1410, loss = 1.85561157
Iteration 1411, loss = 1.85580122
Iteration 1412, loss = 1.85440845
Iteration 1413, loss = 1.85407344
Iteration 1414, loss = 1.85383899
Iteration 1415, loss = 1.85282208
Iteration 1416, loss = 1.85319004
Iteration 1417, loss = 1.85351829
Iteration 1418, loss = 1.85239905
Iteration 1419, loss = 1.85242943
Iteration 1420, loss = 1.85161382
Iteration 1421, loss = 1.85042502
Iteration 1422, loss = 1.84967432
Iteration 1423, loss = 1.85040898
Iteration 1424, loss = 1.85035629
Iteration 1425, loss = 1.84876318
Iteration 1426, loss = 1.84912936
Iteration 1427, loss = 1.84723787
Iteration 1428, loss = 1.84745984
Iteration 1429, loss = 1.84758604
Iteration 1430, loss = 1.84681108
Iteration 1431, loss = 1.84691316
Iteration 1432, loss = 1.84544055
Iteration 1433, loss = 1.84583882
Iteration 1434, loss = 1.84519159
Iteration 1435, loss = 1.84564197
Iteration 1436, loss = 1.84400419
Iteration 1437, loss = 1.84394330
Iteration 1438, loss = 1.84412994
Iteration 1439, loss = 1.84304888
Iteration 1440, loss = 1.84306807
Iteration 1441, loss = 1.84274106
Iteration 1442, loss = 1.84177192
Iteration 1443, loss = 1.84147193
Iteration 1444, loss = 1.84119825
Iteration 1445, loss = 1.84071610
Iteration 1446, loss = 1.84054152
Iteration 1447, loss = 1.83981985
Iteration 1448, loss = 1.83978548
Iteration 1449, loss = 1.83836902
Iteration 1450, loss = 1.83830838
Iteration 1451, loss = 1.83918541
Iteration 1452, loss = 1.83838823
Iteration 1453, loss = 1.83762528
Iteration 1454, loss = 1.83725461
Iteration 1455, loss = 1.83599743
Iteration 1456, loss = 1.83574191
Iteration 1457, loss = 1.83562493
Iteration 1458, loss = 1.83640649
Iteration 1459, loss = 1.83473625
Iteration 1460, loss = 1.83454636
Iteration 1461, loss = 1.83499422
Iteration 1462, loss = 1.83379650
Iteration 1463, loss = 1.83355439
Iteration 1464, loss = 1.83315117
Iteration 1465, loss = 1.83313176
Iteration 1466, loss = 1.83165383
Iteration 1467, loss = 1.83167541
Iteration 1468, loss = 1.83097377
Iteration 1469, loss = 1.83083623
Iteration 1470, loss = 1.83049029
Iteration 1471, loss = 1.82968749
Iteration 1472, loss = 1.83070960
Iteration 1473, loss = 1.82969137
Iteration 1474, loss = 1.82839239
Iteration 1475, loss = 1.82902050
Iteration 1476, loss = 1.82836272
Iteration 1477, loss = 1.82774570
Iteration 1478, loss = 1.82738416
Iteration 1479, loss = 1.82680166
Iteration 1480, loss = 1.82609914
Iteration 1481, loss = 1.82569060
Iteration 1482, loss = 1.82611668
Iteration 1483, loss = 1.82564932
Iteration 1484, loss = 1.82430889
Iteration 1485, loss = 1.82445483
Iteration 1486, loss = 1.82383091
Iteration 1487, loss = 1.82379998
Iteration 1488, loss = 1.82303367
Iteration 1489, loss = 1.82220947
Iteration 1490, loss = 1.82299609
Iteration 1491, loss = 1.82232235
Iteration 1492, loss = 1.82188001
Iteration 1493, loss = 1.82093908
Iteration 1494, loss = 1.82061594
Iteration 1495, loss = 1.82080960
Iteration 1496, loss = 1.82036943
Iteration 1497, loss = 1.82034789
Iteration 1498, loss = 1.81918904
Iteration 1499, loss = 1.81867225
Iteration 1500, loss = 1.81841410
Iteration 1501, loss = 1.81774899
Iteration 1502, loss = 1.81805443
Iteration 1503, loss = 1.81755278
Iteration 1504, loss = 1.81692308
Iteration 1505, loss = 1.81657842
Iteration 1506, loss = 1.81682903
Iteration 1507, loss = 1.81677884
Iteration 1508, loss = 1.81538063
Iteration 1509, loss = 1.81469688
Iteration 1510, loss = 1.81508587
Iteration 1511, loss = 1.81456276
Iteration 1512, loss = 1.81333349
Iteration 1513, loss = 1.81363264
Iteration 1514, loss = 1.81331708
Iteration 1515, loss = 1.81275241
Iteration 1516, loss = 1.81160387
Iteration 1517, loss = 1.81213659
Iteration 1518, loss = 1.81121121
Iteration 1519, loss = 1.81111639
Iteration 1520, loss = 1.81083307
Iteration 1521, loss = 1.81027299
Iteration 1522, loss = 1.81088305
Iteration 1523, loss = 1.80960850
Iteration 1524, loss = 1.80948209
Iteration 1525, loss = 1.80831215
Iteration 1526, loss = 1.80823788
Iteration 1527, loss = 1.80773470
Iteration 1528, loss = 1.80760892
Iteration 1529, loss = 1.80797266
Iteration 1530, loss = 1.80669956
Iteration 1531, loss = 1.80678862
Iteration 1532, loss = 1.80548553
Iteration 1533, loss = 1.80551036
Iteration 1534, loss = 1.80554682
Iteration 1535, loss = 1.80453216
Iteration 1536, loss = 1.80434913
Iteration 1537, loss = 1.80337667
Iteration 1538, loss = 1.80365368
Iteration 1539, loss = 1.80348818
Iteration 1540, loss = 1.80280951
Iteration 1541, loss = 1.80187483
Iteration 1542, loss = 1.80250317
Iteration 1543, loss = 1.80202586
Iteration 1544, loss = 1.80116581
Iteration 1545, loss = 1.80082692
Iteration 1546, loss = 1.80136071
Iteration 1547, loss = 1.80094815
Iteration 1548, loss = 1.80027742
Iteration 1549, loss = 1.80063933
Iteration 1550, loss = 1.79967400
Iteration 1551, loss = 1.79898947
Iteration 1552, loss = 1.79817884
Iteration 1553, loss = 1.79828863
Iteration 1554, loss = 1.79834919
Iteration 1555, loss = 1.79667077
Iteration 1556, loss = 1.79689048
Iteration 1557, loss = 1.79659287
Iteration 1558, loss = 1.79606879
Iteration 1559, loss = 1.79656440
Iteration 1560, loss = 1.79566230
Iteration 1561, loss = 1.79476974
Iteration 1562, loss = 1.79454313
Iteration 1563, loss = 1.79474514
Iteration 1564, loss = 1.79441507
Iteration 1565, loss = 1.79382035
Iteration 1566, loss = 1.79298583
Iteration 1567, loss = 1.79294883
Iteration 1568, loss = 1.79258623
Iteration 1569, loss = 1.79184097
Iteration 1570, loss = 1.79179616
Iteration 1571, loss = 1.79212377
Iteration 1572, loss = 1.79170667
Iteration 1573, loss = 1.79086815
Iteration 1574, loss = 1.79105707
Iteration 1575, loss = 1.78925760
Iteration 1576, loss = 1.78976540
Iteration 1577, loss = 1.78937742
Iteration 1578, loss = 1.78968467
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100,), {'error': 0.94185637700063563, 'fit': 0.57225000000000004, 'time': 2675.53})
Iteration 1, loss = 6.32824561
Iteration 2, loss = 5.81206508
Iteration 3, loss = 5.67029145
Iteration 4, loss = 5.58962377
Iteration 5, loss = 5.53118272
Iteration 6, loss = 5.48330211
Iteration 7, loss = 5.44301816
Iteration 8, loss = 5.40708311
Iteration 9, loss = 5.37461464
Iteration 10, loss = 5.34469579
Iteration 11, loss = 5.31488951
Iteration 12, loss = 5.29007286
Iteration 13, loss = 5.26365555
Iteration 14, loss = 5.23767578
Iteration 15, loss = 5.21397417
Iteration 16, loss = 5.18887359
Iteration 17, loss = 5.16386381
Iteration 18, loss = 5.14039664
Iteration 19, loss = 5.11699491
Iteration 20, loss = 5.09183573
Iteration 21, loss = 5.07034871
Iteration 22, loss = 5.04495608
Iteration 23, loss = 5.02076473
Iteration 24, loss = 4.99858006
Iteration 25, loss = 4.97386478
Iteration 26, loss = 4.95148204
Iteration 27, loss = 4.92965222
Iteration 28, loss = 4.90604563
Iteration 29, loss = 4.88454257
Iteration 30, loss = 4.86003283
Iteration 31, loss = 4.83532210
Iteration 32, loss = 4.81307876
Iteration 33, loss = 4.79194890
Iteration 34, loss = 4.76930989
Iteration 35, loss = 4.74777570
Iteration 36, loss = 4.72503280
Iteration 37, loss = 4.70413019
Iteration 38, loss = 4.68209678
Iteration 39, loss = 4.65990227
Iteration 40, loss = 4.63826223
Iteration 41, loss = 4.61654192
Iteration 42, loss = 4.59623743
Iteration 43, loss = 4.57592672
Iteration 44, loss = 4.55422745
Iteration 45, loss = 4.53495325
Iteration 46, loss = 4.51494625
Iteration 47, loss = 4.49475221
Iteration 48, loss = 4.47441213
Iteration 49, loss = 4.45372996
Iteration 50, loss = 4.43494264
Iteration 51, loss = 4.41673937
Iteration 52, loss = 4.39745317
Iteration 53, loss = 4.37910743
Iteration 54, loss = 4.35956622
Iteration 55, loss = 4.34128446
Iteration 56, loss = 4.32301737
Iteration 57, loss = 4.30459469
Iteration 58, loss = 4.28673767
Iteration 59, loss = 4.26861308
Iteration 60, loss = 4.25053982
Iteration 61, loss = 4.23301893
Iteration 62, loss = 4.21563429
Iteration 63, loss = 4.19844262
Iteration 64, loss = 4.17995429
Iteration 65, loss = 4.16391915
Iteration 66, loss = 4.14748737
Iteration 67, loss = 4.13108894
Iteration 68, loss = 4.11361230
Iteration 69, loss = 4.09859077
Iteration 70, loss = 4.08115403
Iteration 71, loss = 4.06441365
Iteration 72, loss = 4.04969703
Iteration 73, loss = 4.03501547
Iteration 74, loss = 4.01892256
Iteration 75, loss = 4.00283005
Iteration 76, loss = 3.98804150
Iteration 77, loss = 3.97363143
Iteration 78, loss = 3.95832416
Iteration 79, loss = 3.94387434
Iteration 80, loss = 3.92854485
Iteration 81, loss = 3.91441724
Iteration 82, loss = 3.89918431
Iteration 83, loss = 3.88547948
Iteration 84, loss = 3.87035659
Iteration 85, loss = 3.85605411
Iteration 86, loss = 3.84271480
Iteration 87, loss = 3.82842670
Iteration 88, loss = 3.81414840
Iteration 89, loss = 3.80046046
Iteration 90, loss = 3.78684522
Iteration 91, loss = 3.77407426
Iteration 92, loss = 3.76039018
Iteration 93, loss = 3.74834165
Iteration 94, loss = 3.73478365
Iteration 95, loss = 3.72121323
Iteration 96, loss = 3.70820946
Iteration 97, loss = 3.69590505
Iteration 98, loss = 3.68282375
Iteration 99, loss = 3.67060267
Iteration 100, loss = 3.65837417
Iteration 101, loss = 3.64675204
Iteration 102, loss = 3.63292664
Iteration 103, loss = 3.62266045
Iteration 104, loss = 3.60907095
Iteration 105, loss = 3.59788725
Iteration 106, loss = 3.58534515
Iteration 107, loss = 3.57442315
Iteration 108, loss = 3.55998303
Iteration 109, loss = 3.55036167
Iteration 110, loss = 3.53753790
Iteration 111, loss = 3.52739929
Iteration 112, loss = 3.51639440
Iteration 113, loss = 3.50486619
Iteration 114, loss = 3.49463866
Iteration 115, loss = 3.48281980
Iteration 116, loss = 3.47049616
Iteration 117, loss = 3.46115998
Iteration 118, loss = 3.44972782
Iteration 119, loss = 3.43811124
Iteration 120, loss = 3.42891104
Iteration 121, loss = 3.41630142
Iteration 122, loss = 3.40749559
Iteration 123, loss = 3.39686625
Iteration 124, loss = 3.38654368
Iteration 125, loss = 3.37522559
Iteration 126, loss = 3.36630776
Iteration 127, loss = 3.35587441
Iteration 128, loss = 3.34467622
Iteration 129, loss = 3.33561121
Iteration 130, loss = 3.32513826
Iteration 131, loss = 3.31481614
Iteration 132, loss = 3.30598248
Iteration 133, loss = 3.29584214
Iteration 134, loss = 3.28546521
Iteration 135, loss = 3.27701038
Iteration 136, loss = 3.26648885
Iteration 137, loss = 3.25833044
Iteration 138, loss = 3.24864069
Iteration 139, loss = 3.23820188
Iteration 140, loss = 3.23005480
Iteration 141, loss = 3.21964112
Iteration 142, loss = 3.21135679
Iteration 143, loss = 3.20108081
Iteration 144, loss = 3.19219103
Iteration 145, loss = 3.18383539
Iteration 146, loss = 3.17417439
Iteration 147, loss = 3.16484109
Iteration 148, loss = 3.15675339
Iteration 149, loss = 3.14709003
Iteration 150, loss = 3.13917055
Iteration 151, loss = 3.13225989
Iteration 152, loss = 3.12351868
Iteration 153, loss = 3.11321508
Iteration 154, loss = 3.10419740
Iteration 155, loss = 3.09771904
Iteration 156, loss = 3.08874196
Iteration 157, loss = 3.07964200
Iteration 158, loss = 3.07109363
Iteration 159, loss = 3.06332180
Iteration 160, loss = 3.05570359
Iteration 161, loss = 3.04814211
Iteration 162, loss = 3.04018352
Iteration 163, loss = 3.03097864
Iteration 164, loss = 3.02300439
Iteration 165, loss = 3.01522432
Iteration 166, loss = 3.00784167
Iteration 167, loss = 2.99854265
Iteration 168, loss = 2.99217578
Iteration 169, loss = 2.98329880
Iteration 170, loss = 2.97685320
Iteration 171, loss = 2.96747663
Iteration 172, loss = 2.96106118
Iteration 173, loss = 2.95297247
Iteration 174, loss = 2.94493443
Iteration 175, loss = 2.93851985
Iteration 176, loss = 2.93066728
Iteration 177, loss = 2.92360913
Iteration 178, loss = 2.91489872
Iteration 179, loss = 2.90893054
Iteration 180, loss = 2.90092080
Iteration 181, loss = 2.89349844
Iteration 182, loss = 2.88651707
Iteration 183, loss = 2.88019389
Iteration 184, loss = 2.87254162
Iteration 185, loss = 2.86609655
Iteration 186, loss = 2.85824066
Iteration 187, loss = 2.85060114
Iteration 188, loss = 2.84427030
Iteration 189, loss = 2.83722316
Iteration 190, loss = 2.83051950
Iteration 191, loss = 2.82328681
Iteration 192, loss = 2.81638412
Iteration 193, loss = 2.80963061
Iteration 194, loss = 2.80314971
Iteration 195, loss = 2.79622747
Iteration 196, loss = 2.79025609
Iteration 197, loss = 2.78307585
Iteration 198, loss = 2.77671272
Iteration 199, loss = 2.76937005
Iteration 200, loss = 2.76352234
Iteration 201, loss = 2.75762733
Iteration 202, loss = 2.75134075
Iteration 203, loss = 2.74392293
Iteration 204, loss = 2.73632522
Iteration 205, loss = 2.73030822
Iteration 206, loss = 2.72544237
Iteration 207, loss = 2.71892482
Iteration 208, loss = 2.71213229
Iteration 209, loss = 2.70548502
Iteration 210, loss = 2.70048407
Iteration 211, loss = 2.69346980
Iteration 212, loss = 2.68817915
Iteration 213, loss = 2.68226639
Iteration 214, loss = 2.67544520
Iteration 215, loss = 2.66791463
Iteration 216, loss = 2.66252703
Iteration 217, loss = 2.65709428
Iteration 218, loss = 2.65153236
Iteration 219, loss = 2.64484550
Iteration 220, loss = 2.64003313
Iteration 221, loss = 2.63298216
Iteration 222, loss = 2.62820929
Iteration 223, loss = 2.62167832
Iteration 224, loss = 2.61656553
Iteration 225, loss = 2.60992926
Iteration 226, loss = 2.60518103
Iteration 227, loss = 2.59912732
Iteration 228, loss = 2.59327378
Iteration 229, loss = 2.58694420
Iteration 230, loss = 2.58087630
Iteration 231, loss = 2.57722859
Iteration 232, loss = 2.57075937
Iteration 233, loss = 2.56541829
Iteration 234, loss = 2.55866527
Iteration 235, loss = 2.55333453
Iteration 236, loss = 2.54932545
Iteration 237, loss = 2.54159217
Iteration 238, loss = 2.53709933
Iteration 239, loss = 2.53063443
Iteration 240, loss = 2.52621838
Iteration 241, loss = 2.52016513
Iteration 242, loss = 2.51481031
Iteration 243, loss = 2.51135153
Iteration 244, loss = 2.50601413
Iteration 245, loss = 2.49908720
Iteration 246, loss = 2.49561623
Iteration 247, loss = 2.49006935
Iteration 248, loss = 2.48470209
Iteration 249, loss = 2.47885059
Iteration 250, loss = 2.47475799
Iteration 251, loss = 2.46908969
Iteration 252, loss = 2.46392191
Iteration 253, loss = 2.45968338
Iteration 254, loss = 2.45322894
Iteration 255, loss = 2.44862339
Iteration 256, loss = 2.44356464
Iteration 257, loss = 2.43822377
Iteration 258, loss = 2.43372855
Iteration 259, loss = 2.42788999
Iteration 260, loss = 2.42458707
Iteration 261, loss = 2.41869463
Iteration 262, loss = 2.41377114
Iteration 263, loss = 2.40954703
Iteration 264, loss = 2.40516105
Iteration 265, loss = 2.39882807
Iteration 266, loss = 2.39510975
Iteration 267, loss = 2.39015679
Iteration 268, loss = 2.38428053
Iteration 269, loss = 2.37888072
Iteration 270, loss = 2.37510284
Iteration 271, loss = 2.36965414
Iteration 272, loss = 2.36497683
Iteration 273, loss = 2.36095930
Iteration 274, loss = 2.35732214
Iteration 275, loss = 2.35205330
Iteration 276, loss = 2.34698278
Iteration 277, loss = 2.34205555
Iteration 278, loss = 2.33806235
Iteration 279, loss = 2.33399769
Iteration 280, loss = 2.32800065
Iteration 281, loss = 2.32392839
Iteration 282, loss = 2.31985335
Iteration 283, loss = 2.31637336
Iteration 284, loss = 2.31067958
Iteration 285, loss = 2.30610927
Iteration 286, loss = 2.30216038
Iteration 287, loss = 2.29688608
Iteration 288, loss = 2.29519155
Iteration 289, loss = 2.28928921
Iteration 290, loss = 2.28402396
Iteration 291, loss = 2.27901371
Iteration 292, loss = 2.27635953
Iteration 293, loss = 2.27162093
Iteration 294, loss = 2.26774499
Iteration 295, loss = 2.26257664
Iteration 296, loss = 2.25876250
Iteration 297, loss = 2.25507030
Iteration 298, loss = 2.24988767
Iteration 299, loss = 2.24576027
Iteration 300, loss = 2.24176033
Iteration 301, loss = 2.23762382
Iteration 302, loss = 2.23350918
Iteration 303, loss = 2.22802009
Iteration 304, loss = 2.22513059
Iteration 305, loss = 2.22143381
Iteration 306, loss = 2.21762946
Iteration 307, loss = 2.21289745
Iteration 308, loss = 2.20869152
Iteration 309, loss = 2.20513428
Iteration 310, loss = 2.20139088
Iteration 311, loss = 2.19694373
Iteration 312, loss = 2.19256524
Iteration 313, loss = 2.18791892
Iteration 314, loss = 2.18386816
Iteration 315, loss = 2.17999824
Iteration 316, loss = 2.17640305
Iteration 317, loss = 2.17241942
Iteration 318, loss = 2.16768374
Iteration 319, loss = 2.16508189
Iteration 320, loss = 2.16086798
Iteration 321, loss = 2.15689734
Iteration 322, loss = 2.15293753
Iteration 323, loss = 2.14818002
Iteration 324, loss = 2.14529379
Iteration 325, loss = 2.14051404
Iteration 326, loss = 2.13814000
Iteration 327, loss = 2.13367283
Iteration 328, loss = 2.12954647
Iteration 329, loss = 2.12633940
Iteration 330, loss = 2.12297506
Iteration 331, loss = 2.11763183
Iteration 332, loss = 2.11450199
Iteration 333, loss = 2.11138731
Iteration 334, loss = 2.10601947
Iteration 335, loss = 2.10393364
Iteration 336, loss = 2.09872239
Iteration 337, loss = 2.09572673
Iteration 338, loss = 2.09315826
Iteration 339, loss = 2.08990262
Iteration 340, loss = 2.08614113
Iteration 341, loss = 2.08163634
Iteration 342, loss = 2.07779732
Iteration 343, loss = 2.07441035
Iteration 344, loss = 2.07075602
Iteration 345, loss = 2.06725785
Iteration 346, loss = 2.06346299
Iteration 347, loss = 2.06012289
Iteration 348, loss = 2.05578839
Iteration 349, loss = 2.05242724
Iteration 350, loss = 2.04810013
Iteration 351, loss = 2.04574391
Iteration 352, loss = 2.04212274
Iteration 353, loss = 2.03821122
Iteration 354, loss = 2.03530438
Iteration 355, loss = 2.03150321
Iteration 356, loss = 2.02826301
Iteration 357, loss = 2.02430796
Iteration 358, loss = 2.02044244
Iteration 359, loss = 2.01760576
Iteration 360, loss = 2.01527926
Iteration 361, loss = 2.00990340
Iteration 362, loss = 2.00833878
Iteration 363, loss = 2.00280486
Iteration 364, loss = 2.00168880
Iteration 365, loss = 1.99776277
Iteration 366, loss = 1.99425271
Iteration 367, loss = 1.99067362
Iteration 368, loss = 1.98785402
Iteration 369, loss = 1.98592486
Iteration 370, loss = 1.97982148
Iteration 371, loss = 1.97757189
Iteration 372, loss = 1.97431470
Iteration 373, loss = 1.97103528
Iteration 374, loss = 1.96787475
Iteration 375, loss = 1.96423153
Iteration 376, loss = 1.96114085
Iteration 377, loss = 1.95755560
Iteration 378, loss = 1.95645940
Iteration 379, loss = 1.95085671
Iteration 380, loss = 1.94745866
Iteration 381, loss = 1.94509876
Iteration 382, loss = 1.94175074
Iteration 383, loss = 1.93905583
Iteration 384, loss = 1.93562732
Iteration 385, loss = 1.93140205
Iteration 386, loss = 1.92837445
Iteration 387, loss = 1.92596710
Iteration 388, loss = 1.92299359
Iteration 389, loss = 1.91980171
Iteration 390, loss = 1.91721387
Iteration 391, loss = 1.91323914
Iteration 392, loss = 1.91118427
Iteration 393, loss = 1.90778148
Iteration 394, loss = 1.90367425
Iteration 395, loss = 1.90299847
Iteration 396, loss = 1.89732084
Iteration 397, loss = 1.89511189
Iteration 398, loss = 1.89215897
Iteration 399, loss = 1.88915491
Iteration 400, loss = 1.88565144
Iteration 401, loss = 1.88369084
Iteration 402, loss = 1.88010733
Iteration 403, loss = 1.87689245
Iteration 404, loss = 1.87454920
Iteration 405, loss = 1.87227196
Iteration 406, loss = 1.86879489
Iteration 407, loss = 1.86592687
Iteration 408, loss = 1.86269515
Iteration 409, loss = 1.85982141
Iteration 410, loss = 1.85695288
Iteration 411, loss = 1.85331565
Iteration 412, loss = 1.85041149
Iteration 413, loss = 1.84921785
Iteration 414, loss = 1.84566821
Iteration 415, loss = 1.84137477
Iteration 416, loss = 1.83884507
Iteration 417, loss = 1.83719263
Iteration 418, loss = 1.83330024
Iteration 419, loss = 1.83176055
Iteration 420, loss = 1.82691273
Iteration 421, loss = 1.82418727
Iteration 422, loss = 1.82163616
Iteration 423, loss = 1.81878801
Iteration 424, loss = 1.81581238
Iteration 425, loss = 1.81308434
Iteration 426, loss = 1.80994519
Iteration 427, loss = 1.80706421
Iteration 428, loss = 1.80596372
Iteration 429, loss = 1.80179461
Iteration 430, loss = 1.79833648
Iteration 431, loss = 1.79847331
Iteration 432, loss = 1.79338809
Iteration 433, loss = 1.78918471
Iteration 434, loss = 1.78749680
Iteration 435, loss = 1.78559060
Iteration 436, loss = 1.78175443
Iteration 437, loss = 1.77931200
Iteration 438, loss = 1.77671758
Iteration 439, loss = 1.77389891
Iteration 440, loss = 1.77138636
Iteration 441, loss = 1.76870980
Iteration 442, loss = 1.76562749
Iteration 443, loss = 1.76421797
Iteration 444, loss = 1.76136365
Iteration 445, loss = 1.75767249
Iteration 446, loss = 1.75647590
Iteration 447, loss = 1.75388908
Iteration 448, loss = 1.74940971
Iteration 449, loss = 1.74827912
Iteration 450, loss = 1.74502406
Iteration 451, loss = 1.74456110
Iteration 452, loss = 1.73961428
Iteration 453, loss = 1.73652372
Iteration 454, loss = 1.73495691
Iteration 455, loss = 1.73303668
Iteration 456, loss = 1.72934135
Iteration 457, loss = 1.72599109
Iteration 458, loss = 1.72353572
Iteration 459, loss = 1.72028938
Iteration 460, loss = 1.71972559
Iteration 461, loss = 1.71548324
Iteration 462, loss = 1.71348052
Iteration 463, loss = 1.71123429
Iteration 464, loss = 1.70881086
Iteration 465, loss = 1.70638042
Iteration 466, loss = 1.70336246
Iteration 467, loss = 1.70081472
Iteration 468, loss = 1.69921353
Iteration 469, loss = 1.69668532
Iteration 470, loss = 1.69523583
Iteration 471, loss = 1.69195518
Iteration 472, loss = 1.68839843
Iteration 473, loss = 1.68588610
Iteration 474, loss = 1.68441482
Iteration 475, loss = 1.68064781
Iteration 476, loss = 1.67959073
Iteration 477, loss = 1.67602150
Iteration 478, loss = 1.67313510
Iteration 479, loss = 1.67100704
Iteration 480, loss = 1.66975387
Iteration 481, loss = 1.66611250
Iteration 482, loss = 1.66329759
Iteration 483, loss = 1.66160493
Iteration 484, loss = 1.65985528
Iteration 485, loss = 1.65696475
Iteration 486, loss = 1.65483226
Iteration 487, loss = 1.65230710
Iteration 488, loss = 1.64922083
Iteration 489, loss = 1.64857744
Iteration 490, loss = 1.64384422
Iteration 491, loss = 1.64227107
Iteration 492, loss = 1.63942023
Iteration 493, loss = 1.63816613
Iteration 494, loss = 1.63490091
Iteration 495, loss = 1.63326090
Iteration 496, loss = 1.63067777
Iteration 497, loss = 1.62933294
Iteration 498, loss = 1.62674112
Iteration 499, loss = 1.62375558
Iteration 500, loss = 1.62148483
Iteration 501, loss = 1.61955685
Iteration 502, loss = 1.61685199
Iteration 503, loss = 1.61463787
Iteration 504, loss = 1.61209297
Iteration 505, loss = 1.61053465
Iteration 506, loss = 1.60862968
Iteration 507, loss = 1.60546527
Iteration 508, loss = 1.60178418
Iteration 509, loss = 1.60018702
Iteration 510, loss = 1.59816387
Iteration 511, loss = 1.59611719
Iteration 512, loss = 1.59334737
Iteration 513, loss = 1.59221536
Iteration 514, loss = 1.58829052
Iteration 515, loss = 1.58670756
Iteration 516, loss = 1.58545852
Iteration 517, loss = 1.58245900
Iteration 518, loss = 1.58021141
Iteration 519, loss = 1.57799096
Iteration 520, loss = 1.57557637
Iteration 521, loss = 1.57391979
Iteration 522, loss = 1.57025170
Iteration 523, loss = 1.56929838
Iteration 524, loss = 1.56730583
Iteration 525, loss = 1.56432519
Iteration 526, loss = 1.56219188
Iteration 527, loss = 1.56099971
Iteration 528, loss = 1.55906762
Iteration 529, loss = 1.55566381
Iteration 530, loss = 1.55410043
Iteration 531, loss = 1.55143464
Iteration 532, loss = 1.55026207
Iteration 533, loss = 1.54815285
Iteration 534, loss = 1.54442965
Iteration 535, loss = 1.54421673
Iteration 536, loss = 1.54131254
Iteration 537, loss = 1.53887903
Iteration 538, loss = 1.53603819
Iteration 539, loss = 1.53425111
Iteration 540, loss = 1.53215411
Iteration 541, loss = 1.53006725
Iteration 542, loss = 1.52824903
Iteration 543, loss = 1.52805942
Iteration 544, loss = 1.52367805
Iteration 545, loss = 1.52205948
Iteration 546, loss = 1.51977680
Iteration 547, loss = 1.51735985
Iteration 548, loss = 1.51747934
Iteration 549, loss = 1.51359381
Iteration 550, loss = 1.51149980
Iteration 551, loss = 1.51058204
Iteration 552, loss = 1.50860997
Iteration 553, loss = 1.50606319
Iteration 554, loss = 1.50383629
Iteration 555, loss = 1.50109141
Iteration 556, loss = 1.49937672
Iteration 557, loss = 1.49747517
Iteration 558, loss = 1.49535377
Iteration 559, loss = 1.49349311
Iteration 560, loss = 1.49127164
Iteration 561, loss = 1.48874513
Iteration 562, loss = 1.48738811
Iteration 563, loss = 1.48463134
Iteration 564, loss = 1.48390186
Iteration 565, loss = 1.47986627
Iteration 566, loss = 1.47900528
Iteration 567, loss = 1.47724078
Iteration 568, loss = 1.47494732
Iteration 569, loss = 1.47344496
Iteration 570, loss = 1.47070213
Iteration 571, loss = 1.46968627
Iteration 572, loss = 1.46774608
Iteration 573, loss = 1.46495047
Iteration 574, loss = 1.46322376
Iteration 575, loss = 1.46157586
Iteration 576, loss = 1.45872388
Iteration 577, loss = 1.45749299
Iteration 578, loss = 1.45507269
Iteration 579, loss = 1.45313678
Iteration 580, loss = 1.45154646
Iteration 581, loss = 1.44923348
Iteration 582, loss = 1.44742359
Iteration 583, loss = 1.44434731
Iteration 584, loss = 1.44380469
Iteration 585, loss = 1.44174602
Iteration 586, loss = 1.44133799
Iteration 587, loss = 1.43756085
Iteration 588, loss = 1.43663769
Iteration 589, loss = 1.43272020
Iteration 590, loss = 1.43228561
Iteration 591, loss = 1.42941791
Iteration 592, loss = 1.42883471
Iteration 593, loss = 1.42577800
Iteration 594, loss = 1.42396759
Iteration 595, loss = 1.42334678
Iteration 596, loss = 1.42104176
Iteration 597, loss = 1.41722052
Iteration 598, loss = 1.41638974
Iteration 599, loss = 1.41453867
Iteration 600, loss = 1.41218367
Iteration 601, loss = 1.40947757
Iteration 602, loss = 1.40848236
Iteration 603, loss = 1.40668801
Iteration 604, loss = 1.40620257
Iteration 605, loss = 1.40398794
Iteration 606, loss = 1.40147653
Iteration 607, loss = 1.39963546
Iteration 608, loss = 1.39722334
Iteration 609, loss = 1.39634103
Iteration 610, loss = 1.39454662
Iteration 611, loss = 1.39229476
Iteration 612, loss = 1.39112317
Iteration 613, loss = 1.38773195
Iteration 614, loss = 1.38613236
Iteration 615, loss = 1.38578267
Iteration 616, loss = 1.38337342
Iteration 617, loss = 1.38121875
Iteration 618, loss = 1.38033044
Iteration 619, loss = 1.37737870
Iteration 620, loss = 1.37573177
Iteration 621, loss = 1.37507588
Iteration 622, loss = 1.37261708
Iteration 623, loss = 1.37196853
Iteration 624, loss = 1.36875514
Iteration 625, loss = 1.36704852
Iteration 626, loss = 1.36619111
Iteration 627, loss = 1.36460954
Iteration 628, loss = 1.36098017
Iteration 629, loss = 1.36032962
Iteration 630, loss = 1.35873124
Iteration 631, loss = 1.35729085
Iteration 632, loss = 1.35646542
Iteration 633, loss = 1.35378763
Iteration 634, loss = 1.35084022
Iteration 635, loss = 1.34925809
Iteration 636, loss = 1.34828822
Iteration 637, loss = 1.34712947
Iteration 638, loss = 1.34559750
Iteration 639, loss = 1.34344285
Iteration 640, loss = 1.34085271
Iteration 641, loss = 1.33905183
Iteration 642, loss = 1.33767766
Iteration 643, loss = 1.33624881
Iteration 644, loss = 1.33408301
Iteration 645, loss = 1.33302012
Iteration 646, loss = 1.33102800
Iteration 647, loss = 1.32982998
Iteration 648, loss = 1.32788104
Iteration 649, loss = 1.32545124
Iteration 650, loss = 1.32440617
Iteration 651, loss = 1.32237689
Iteration 652, loss = 1.32027162
Iteration 653, loss = 1.32013776
Iteration 654, loss = 1.31679330
Iteration 655, loss = 1.31609376
Iteration 656, loss = 1.31461613
Iteration 657, loss = 1.31306667
Iteration 658, loss = 1.31217027
Iteration 659, loss = 1.30889936
Iteration 660, loss = 1.30782481
Iteration 661, loss = 1.30709504
Iteration 662, loss = 1.30490476
Iteration 663, loss = 1.30335060
Iteration 664, loss = 1.30191622
Iteration 665, loss = 1.29929157
Iteration 666, loss = 1.29750087
Iteration 667, loss = 1.29501563
Iteration 668, loss = 1.29447282
Iteration 669, loss = 1.29287170
Iteration 670, loss = 1.29122930
Iteration 671, loss = 1.28971182
Iteration 672, loss = 1.28776946
Iteration 673, loss = 1.28592033
Iteration 674, loss = 1.28508042
Iteration 675, loss = 1.28274551
Iteration 676, loss = 1.28120459
Iteration 677, loss = 1.27977183
Iteration 678, loss = 1.27872255
Iteration 679, loss = 1.27716220
Iteration 680, loss = 1.27526109
Iteration 681, loss = 1.27411633
Iteration 682, loss = 1.27148494
Iteration 683, loss = 1.26999475
Iteration 684, loss = 1.26798359
Iteration 685, loss = 1.26724424
Iteration 686, loss = 1.26649572
Iteration 687, loss = 1.26333858
Iteration 688, loss = 1.26194734
Iteration 689, loss = 1.26059191
Iteration 690, loss = 1.25943287
Iteration 691, loss = 1.25745705
Iteration 692, loss = 1.25664488
Iteration 693, loss = 1.25542093
Iteration 694, loss = 1.25285644
Iteration 695, loss = 1.25151780
Iteration 696, loss = 1.25085624
Iteration 697, loss = 1.24840613
Iteration 698, loss = 1.24655360
Iteration 699, loss = 1.24498275
Iteration 700, loss = 1.24474762
Iteration 701, loss = 1.24307466
Iteration 702, loss = 1.24105430
Iteration 703, loss = 1.23948307
Iteration 704, loss = 1.23768814
Iteration 705, loss = 1.23686321
Iteration 706, loss = 1.23549834
Iteration 707, loss = 1.23302893
Iteration 708, loss = 1.23156526
Iteration 709, loss = 1.23054875
Iteration 710, loss = 1.22907274
Iteration 711, loss = 1.22692051
Iteration 712, loss = 1.22547351
Iteration 713, loss = 1.22377744
Iteration 714, loss = 1.22231408
Iteration 715, loss = 1.22232346
Iteration 716, loss = 1.21960812
Iteration 717, loss = 1.21914640
Iteration 718, loss = 1.21693884
Iteration 719, loss = 1.21456655
Iteration 720, loss = 1.21441489
Iteration 721, loss = 1.21184611
Iteration 722, loss = 1.21107733
Iteration 723, loss = 1.20972913
Iteration 724, loss = 1.20767098
Iteration 725, loss = 1.20682686
Iteration 726, loss = 1.20488561
Iteration 727, loss = 1.20397963
Iteration 728, loss = 1.20344206
Iteration 729, loss = 1.20057051
Iteration 730, loss = 1.19949514
Iteration 731, loss = 1.19814689
Iteration 732, loss = 1.19648783
Iteration 733, loss = 1.19496445
Iteration 734, loss = 1.19441252
Iteration 735, loss = 1.19168950
Iteration 736, loss = 1.19062682
Iteration 737, loss = 1.18936475
Iteration 738, loss = 1.18754745
Iteration 739, loss = 1.18663755
Iteration 740, loss = 1.18434933
Iteration 741, loss = 1.18311055
Iteration 742, loss = 1.18218370
Iteration 743, loss = 1.18015602
Iteration 744, loss = 1.18026801
Iteration 745, loss = 1.17847673
Iteration 746, loss = 1.17667930
Iteration 747, loss = 1.17479080
Iteration 748, loss = 1.17367234
Iteration 749, loss = 1.17151135
Iteration 750, loss = 1.17123190
Iteration 751, loss = 1.16966895
Iteration 752, loss = 1.16746602
Iteration 753, loss = 1.16713112
Iteration 754, loss = 1.16541510
Iteration 755, loss = 1.16456047
Iteration 756, loss = 1.16289867
Iteration 757, loss = 1.16134906
Iteration 758, loss = 1.15994029
Iteration 759, loss = 1.15813209
Iteration 760, loss = 1.15694143
Iteration 761, loss = 1.15502003
Iteration 762, loss = 1.15414703
Iteration 763, loss = 1.15406167
Iteration 764, loss = 1.15293889
Iteration 765, loss = 1.14993184
Iteration 766, loss = 1.14932926
Iteration 767, loss = 1.14782116
Iteration 768, loss = 1.14604546
Iteration 769, loss = 1.14507613
Iteration 770, loss = 1.14343188
Iteration 771, loss = 1.14179108
Iteration 772, loss = 1.14161952
Iteration 773, loss = 1.14031152
Iteration 774, loss = 1.13864744
Iteration 775, loss = 1.13817729
Iteration 776, loss = 1.13655442
Iteration 777, loss = 1.13483567
Iteration 778, loss = 1.13394706
Iteration 779, loss = 1.13125498
Iteration 780, loss = 1.12921493
Iteration 781, loss = 1.12968877
Iteration 782, loss = 1.12794972
Iteration 783, loss = 1.12724365
Iteration 784, loss = 1.12591049
Iteration 785, loss = 1.12463180
Iteration 786, loss = 1.12244064
Iteration 787, loss = 1.12182613
Iteration 788, loss = 1.11948981
Iteration 789, loss = 1.11811398
Iteration 790, loss = 1.11702913
Iteration 791, loss = 1.11598011
Iteration 792, loss = 1.11484706
Iteration 793, loss = 1.11296640
Iteration 794, loss = 1.11229213
Iteration 795, loss = 1.11094753
Iteration 796, loss = 1.10971169
Iteration 797, loss = 1.10879859
Iteration 798, loss = 1.10673563
Iteration 799, loss = 1.10608606
Iteration 800, loss = 1.10563804
Iteration 801, loss = 1.10465449
Iteration 802, loss = 1.10243325
Iteration 803, loss = 1.10143519
Iteration 804, loss = 1.10115555
Iteration 805, loss = 1.09923459
Iteration 806, loss = 1.09764050
Iteration 807, loss = 1.09541685
Iteration 808, loss = 1.09495866
Iteration 809, loss = 1.09418008
Iteration 810, loss = 1.09221856
Iteration 811, loss = 1.09115708
Iteration 812, loss = 1.08945389
Iteration 813, loss = 1.08914739
Iteration 814, loss = 1.08833881
Iteration 815, loss = 1.08655731
Iteration 816, loss = 1.08515658
Iteration 817, loss = 1.08312867
Iteration 818, loss = 1.08181022
Iteration 819, loss = 1.08166199
Iteration 820, loss = 1.07984413
Iteration 821, loss = 1.07918495
Iteration 822, loss = 1.07720013
Iteration 823, loss = 1.07510746
Iteration 824, loss = 1.07530667
Iteration 825, loss = 1.07409036
Iteration 826, loss = 1.07258861
Iteration 827, loss = 1.07171410
Iteration 828, loss = 1.07040121
Iteration 829, loss = 1.06842301
Iteration 830, loss = 1.06720645
Iteration 831, loss = 1.06618746
Iteration 832, loss = 1.06466345
Iteration 833, loss = 1.06514755
Iteration 834, loss = 1.06292079
Iteration 835, loss = 1.06214471
Iteration 836, loss = 1.06091951
Iteration 837, loss = 1.06011938
Iteration 838, loss = 1.05932046
Iteration 839, loss = 1.05655478
Iteration 840, loss = 1.05466136
Iteration 841, loss = 1.05452258
Iteration 842, loss = 1.05425043
Iteration 843, loss = 1.05298715
Iteration 844, loss = 1.05077697
Iteration 845, loss = 1.04957873
Iteration 846, loss = 1.04856617
Iteration 847, loss = 1.04798759
Iteration 848, loss = 1.04634567
Iteration 849, loss = 1.04572377
Iteration 850, loss = 1.04394788
Iteration 851, loss = 1.04259784
Iteration 852, loss = 1.04166654
Iteration 853, loss = 1.03962368
Iteration 854, loss = 1.03982270
Iteration 855, loss = 1.03869172
Iteration 856, loss = 1.03763979
Iteration 857, loss = 1.03646932
Iteration 858, loss = 1.03534521
Iteration 859, loss = 1.03354914
Iteration 860, loss = 1.03274410
Iteration 861, loss = 1.03280718
Iteration 862, loss = 1.03053591
Iteration 863, loss = 1.02893672
Iteration 864, loss = 1.02818237
Iteration 865, loss = 1.02608664
Iteration 866, loss = 1.02543353
Iteration 867, loss = 1.02521633
Iteration 868, loss = 1.02400106
Iteration 869, loss = 1.02264289
Iteration 870, loss = 1.02221661
Iteration 871, loss = 1.01961050
Iteration 872, loss = 1.01807223
Iteration 873, loss = 1.01848785
Iteration 874, loss = 1.01636903
Iteration 875, loss = 1.01596518
Iteration 876, loss = 1.01440048
Iteration 877, loss = 1.01332706
Iteration 878, loss = 1.01240590
Iteration 879, loss = 1.01166798
Iteration 880, loss = 1.00972701
Iteration 881, loss = 1.00814852
Iteration 882, loss = 1.00782032
Iteration 883, loss = 1.00659500
Iteration 884, loss = 1.00561894
Iteration 885, loss = 1.00524341
Iteration 886, loss = 1.00299885
Iteration 887, loss = 1.00234507
Iteration 888, loss = 1.00131668
Iteration 889, loss = 1.00149842
Iteration 890, loss = 0.99993934
Iteration 891, loss = 0.99849988
Iteration 892, loss = 0.99727891
Iteration 893, loss = 0.99547734
Iteration 894, loss = 0.99457176
Iteration 895, loss = 0.99362664
Iteration 896, loss = 0.99221934
Iteration 897, loss = 0.99227511
Iteration 898, loss = 0.99136523
Iteration 899, loss = 0.98894374
Iteration 900, loss = 0.98891946
Iteration 901, loss = 0.98844737
Iteration 902, loss = 0.98630878
Iteration 903, loss = 0.98438883
Iteration 904, loss = 0.98379150
Iteration 905, loss = 0.98355694
Iteration 906, loss = 0.98203458
Iteration 907, loss = 0.98088302
Iteration 908, loss = 0.98009568
Iteration 909, loss = 0.97998001
Iteration 910, loss = 0.97834826
Iteration 911, loss = 0.97644394
Iteration 912, loss = 0.97620285
Iteration 913, loss = 0.97472310
Iteration 914, loss = 0.97414662
Iteration 915, loss = 0.97264742
Iteration 916, loss = 0.97179223
Iteration 917, loss = 0.97052944
Iteration 918, loss = 0.97043870
Iteration 919, loss = 0.96853652
Iteration 920, loss = 0.96800317
Iteration 921, loss = 0.96634291
Iteration 922, loss = 0.96548476
Iteration 923, loss = 0.96600514
Iteration 924, loss = 0.96347558
Iteration 925, loss = 0.96107676
Iteration 926, loss = 0.96110516
Iteration 927, loss = 0.95993275
Iteration 928, loss = 0.95910667
Iteration 929, loss = 0.95851713
Iteration 930, loss = 0.95718768
Iteration 931, loss = 0.95645763
Iteration 932, loss = 0.95407588
Iteration 933, loss = 0.95447777
Iteration 934, loss = 0.95276544
Iteration 935, loss = 0.95250303
Iteration 936, loss = 0.95138858
Iteration 937, loss = 0.95086257
Iteration 938, loss = 0.94850663
Iteration 939, loss = 0.94814120
Iteration 940, loss = 0.94655100
Iteration 941, loss = 0.94570597
Iteration 942, loss = 0.94494873
Iteration 943, loss = 0.94417690
Iteration 944, loss = 0.94313368
Iteration 945, loss = 0.94207273
Iteration 946, loss = 0.94193979
Iteration 947, loss = 0.94232610
Iteration 948, loss = 0.93994028
Iteration 949, loss = 0.93865035
Iteration 950, loss = 0.93720714
Iteration 951, loss = 0.93646364
Iteration 952, loss = 0.93602329
Iteration 953, loss = 0.93500733
Iteration 954, loss = 0.93378028
Iteration 955, loss = 0.93311272
Iteration 956, loss = 0.93102729
Iteration 957, loss = 0.93039749
Iteration 958, loss = 0.92946691
Iteration 959, loss = 0.92967065
Iteration 960, loss = 0.92814167
Iteration 961, loss = 0.92534458
Iteration 962, loss = 0.92624070
Iteration 963, loss = 0.92447522
Iteration 964, loss = 0.92344737
Iteration 965, loss = 0.92315428
Iteration 966, loss = 0.92232712
Iteration 967, loss = 0.92191001
Iteration 968, loss = 0.91992557
Iteration 969, loss = 0.91915080
Iteration 970, loss = 0.91854745
Iteration 971, loss = 0.91781904
Iteration 972, loss = 0.91653267
Iteration 973, loss = 0.91574382
Iteration 974, loss = 0.91502005
Iteration 975, loss = 0.91439123
Iteration 976, loss = 0.91301916
Iteration 977, loss = 0.91177948
Iteration 978, loss = 0.91132881
Iteration 979, loss = 0.90999931
Iteration 980, loss = 0.91050805
Iteration 981, loss = 0.90809272
Iteration 982, loss = 0.90672792
Iteration 983, loss = 0.90580602
Iteration 984, loss = 0.90503319
Iteration 985, loss = 0.90502791
Iteration 986, loss = 0.90371421
Iteration 987, loss = 0.90388012
Iteration 988, loss = 0.90188563
Iteration 989, loss = 0.90092892
Iteration 990, loss = 0.89953788
Iteration 991, loss = 0.89885131
Iteration 992, loss = 0.89862852
Iteration 993, loss = 0.89630327
Iteration 994, loss = 0.89449118
Iteration 995, loss = 0.89440853
Iteration 996, loss = 0.89354469
Iteration 997, loss = 0.89340774
Iteration 998, loss = 0.89310003
Iteration 999, loss = 0.89172255
Iteration 1000, loss = 0.88968207
Iteration 1001, loss = 0.88944838
Iteration 1002, loss = 0.88962389
Iteration 1003, loss = 0.88787548
Iteration 1004, loss = 0.88647372
Iteration 1005, loss = 0.88587920
Iteration 1006, loss = 0.88572370
Iteration 1007, loss = 0.88356645
Iteration 1008, loss = 0.88483577
Iteration 1009, loss = 0.88221323
Iteration 1010, loss = 0.88214820
Iteration 1011, loss = 0.88035195
Iteration 1012, loss = 0.87997971
Iteration 1013, loss = 0.87948923
Iteration 1014, loss = 0.87893464
Iteration 1015, loss = 0.87695157
Iteration 1016, loss = 0.87605444
Iteration 1017, loss = 0.87594343
Iteration 1018, loss = 0.87599020
Iteration 1019, loss = 0.87386464
Iteration 1020, loss = 0.87345088
Iteration 1021, loss = 0.87193012
Iteration 1022, loss = 0.87205148
Iteration 1023, loss = 0.87023482
Iteration 1024, loss = 0.86967179
Iteration 1025, loss = 0.86955025
Iteration 1026, loss = 0.86785618
Iteration 1027, loss = 0.86641354
Iteration 1028, loss = 0.86563115
Iteration 1029, loss = 0.86448675
Iteration 1030, loss = 0.86412415
Iteration 1031, loss = 0.86257337
Iteration 1032, loss = 0.86250886
Iteration 1033, loss = 0.86093497
Iteration 1034, loss = 0.86157353
Iteration 1035, loss = 0.85939223
Iteration 1036, loss = 0.85868819
Iteration 1037, loss = 0.85723421
Iteration 1038, loss = 0.85730856
Iteration 1039, loss = 0.85682168
Iteration 1040, loss = 0.85510891
Iteration 1041, loss = 0.85483867
Iteration 1042, loss = 0.85445527
Iteration 1043, loss = 0.85290710
Iteration 1044, loss = 0.85201259
Iteration 1045, loss = 0.85140356
Iteration 1046, loss = 0.85172730
Iteration 1047, loss = 0.84971242
Iteration 1048, loss = 0.84914838
Iteration 1049, loss = 0.84760916
Iteration 1050, loss = 0.84859813
Iteration 1051, loss = 0.84554310
Iteration 1052, loss = 0.84648641
Iteration 1053, loss = 0.84506136
Iteration 1054, loss = 0.84426192
Iteration 1055, loss = 0.84248538
Iteration 1056, loss = 0.84153133
Iteration 1057, loss = 0.84076561
Iteration 1058, loss = 0.84082815
Iteration 1059, loss = 0.83997496
Iteration 1060, loss = 0.84049693
Iteration 1061, loss = 0.83909985
Iteration 1062, loss = 0.83738910
Iteration 1063, loss = 0.83663486
Iteration 1064, loss = 0.83596084
Iteration 1065, loss = 0.83469915
Iteration 1066, loss = 0.83429159
Iteration 1067, loss = 0.83341924
Iteration 1068, loss = 0.83135037
Iteration 1069, loss = 0.83158667
Iteration 1070, loss = 0.83044161
Iteration 1071, loss = 0.82974809
Iteration 1072, loss = 0.82921235
Iteration 1073, loss = 0.82784269
Iteration 1074, loss = 0.82744236
Iteration 1075, loss = 0.82762743
Iteration 1076, loss = 0.82633973
Iteration 1077, loss = 0.82541955
Iteration 1078, loss = 0.82521753
Iteration 1079, loss = 0.82281773
Iteration 1080, loss = 0.82150966
Iteration 1081, loss = 0.82316788
Iteration 1082, loss = 0.82199394
Iteration 1083, loss = 0.82091457
Iteration 1084, loss = 0.81812119
Iteration 1085, loss = 0.81971782
Iteration 1086, loss = 0.81871868
Iteration 1087, loss = 0.81743259
Iteration 1088, loss = 0.81642795
Iteration 1089, loss = 0.81598528
Iteration 1090, loss = 0.81541201
Iteration 1091, loss = 0.81513938
Iteration 1092, loss = 0.81298447
Iteration 1093, loss = 0.81263988
Iteration 1094, loss = 0.81174970
Iteration 1095, loss = 0.81119645
Iteration 1096, loss = 0.80993336
Iteration 1097, loss = 0.80906939
Iteration 1098, loss = 0.80821779
Iteration 1099, loss = 0.80808630
Iteration 1100, loss = 0.80713944
Iteration 1101, loss = 0.80555500
Iteration 1102, loss = 0.80570056
Iteration 1103, loss = 0.80579038
Iteration 1104, loss = 0.80391522
Iteration 1105, loss = 0.80381335
Iteration 1106, loss = 0.80257342
Iteration 1107, loss = 0.80075089
Iteration 1108, loss = 0.80099927
Iteration 1109, loss = 0.79977550
Iteration 1110, loss = 0.79816180
Iteration 1111, loss = 0.79867794
Iteration 1112, loss = 0.79799370
Iteration 1113, loss = 0.79770891
Iteration 1114, loss = 0.79636887
Iteration 1115, loss = 0.79487709
Iteration 1116, loss = 0.79465224
Iteration 1117, loss = 0.79456438
Iteration 1118, loss = 0.79238780
Iteration 1119, loss = 0.79156100
Iteration 1120, loss = 0.79114674
Iteration 1121, loss = 0.79193965
Iteration 1122, loss = 0.79112919
Iteration 1123, loss = 0.78969507
Iteration 1124, loss = 0.79001048
Iteration 1125, loss = 0.78781235
Iteration 1126, loss = 0.78745909
Iteration 1127, loss = 0.78564864
Iteration 1128, loss = 0.78578395
Iteration 1129, loss = 0.78447631
Iteration 1130, loss = 0.78365904
Iteration 1131, loss = 0.78325414
Iteration 1132, loss = 0.78330833
Iteration 1133, loss = 0.78164251
Iteration 1134, loss = 0.78156688
Iteration 1135, loss = 0.78023744
Iteration 1136, loss = 0.77958177
Iteration 1137, loss = 0.77958909
Iteration 1138, loss = 0.77828448
Iteration 1139, loss = 0.77768446
Iteration 1140, loss = 0.77680013
Iteration 1141, loss = 0.77696072
Iteration 1142, loss = 0.77509505
Iteration 1143, loss = 0.77452981
Iteration 1144, loss = 0.77378705
Iteration 1145, loss = 0.77318903
Iteration 1146, loss = 0.77164443
Iteration 1147, loss = 0.77056766
Iteration 1148, loss = 0.77103049
Iteration 1149, loss = 0.77042415
Iteration 1150, loss = 0.76925826
Iteration 1151, loss = 0.76968186
Iteration 1152, loss = 0.76850412
Iteration 1153, loss = 0.76773818
Iteration 1154, loss = 0.76545506
Iteration 1155, loss = 0.76565920
Iteration 1156, loss = 0.76484111
Iteration 1157, loss = 0.76464698
Iteration 1158, loss = 0.76387840
Iteration 1159, loss = 0.76318649
Iteration 1160, loss = 0.76229215
Iteration 1161, loss = 0.76236510
Iteration 1162, loss = 0.76091214
Iteration 1163, loss = 0.75902301
Iteration 1164, loss = 0.75939132
Iteration 1165, loss = 0.75877062
Iteration 1166, loss = 0.75773257
Iteration 1167, loss = 0.75692022
Iteration 1168, loss = 0.75698118
Iteration 1169, loss = 0.75593134
Iteration 1170, loss = 0.75490037
Iteration 1171, loss = 0.75430286
Iteration 1172, loss = 0.75360268
Iteration 1173, loss = 0.75296503
Iteration 1174, loss = 0.75254046
Iteration 1175, loss = 0.75154327
Iteration 1176, loss = 0.75064269
Iteration 1177, loss = 0.74968193
Iteration 1178, loss = 0.74941520
Iteration 1179, loss = 0.74947629
Iteration 1180, loss = 0.74827696
Iteration 1181, loss = 0.74732587
Iteration 1182, loss = 0.74681733
Iteration 1183, loss = 0.74608138
Iteration 1184, loss = 0.74581053
Iteration 1185, loss = 0.74542307
Iteration 1186, loss = 0.74461724
Iteration 1187, loss = 0.74330766
Iteration 1188, loss = 0.74238312
Iteration 1189, loss = 0.74189668
Iteration 1190, loss = 0.74186826
Iteration 1191, loss = 0.74033540
Iteration 1192, loss = 0.73951001
Iteration 1193, loss = 0.73931902
Iteration 1194, loss = 0.73871317
Iteration 1195, loss = 0.73801563
Iteration 1196, loss = 0.73766445
Iteration 1197, loss = 0.73602335
Iteration 1198, loss = 0.73625402
Iteration 1199, loss = 0.73508226
Iteration 1200, loss = 0.73509055
Iteration 1201, loss = 0.73350448
Iteration 1202, loss = 0.73240846
Iteration 1203, loss = 0.73270737
Iteration 1204, loss = 0.73206588
Iteration 1205, loss = 0.73096760
Iteration 1206, loss = 0.73075109
Iteration 1207, loss = 0.73013494
Iteration 1208, loss = 0.72819278
Iteration 1209, loss = 0.72834043
Iteration 1210, loss = 0.72677239
Iteration 1211, loss = 0.72711681
Iteration 1212, loss = 0.72664065
Iteration 1213, loss = 0.72593265
Iteration 1214, loss = 0.72455040
Iteration 1215, loss = 0.72502211
Iteration 1216, loss = 0.72261617
Iteration 1217, loss = 0.72294645
Iteration 1218, loss = 0.72203249
Iteration 1219, loss = 0.72155584
Iteration 1220, loss = 0.72080889
Iteration 1221, loss = 0.72205392
Iteration 1222, loss = 0.72041914
Iteration 1223, loss = 0.71863024
Iteration 1224, loss = 0.71779769
Iteration 1225, loss = 0.71940991
Iteration 1226, loss = 0.71695131
Iteration 1227, loss = 0.71650301
Iteration 1228, loss = 0.71503015
Iteration 1229, loss = 0.71548912
Iteration 1230, loss = 0.71438586
Iteration 1231, loss = 0.71374328
Iteration 1232, loss = 0.71313994
Iteration 1233, loss = 0.71227383
Iteration 1234, loss = 0.71150824
Iteration 1235, loss = 0.71096204
Iteration 1236, loss = 0.71022777
Iteration 1237, loss = 0.71001429
Iteration 1238, loss = 0.70898163
Iteration 1239, loss = 0.70938455
Iteration 1240, loss = 0.70838814
Iteration 1241, loss = 0.70796982
Iteration 1242, loss = 0.70733275
Iteration 1243, loss = 0.70570383
Iteration 1244, loss = 0.70553494
Iteration 1245, loss = 0.70436354
Iteration 1246, loss = 0.70480860
Iteration 1247, loss = 0.70406878
Iteration 1248, loss = 0.70298680
Iteration 1249, loss = 0.70208318
Iteration 1250, loss = 0.70195486
Iteration 1251, loss = 0.70111306
Iteration 1252, loss = 0.69981166
Iteration 1253, loss = 0.69996272
Iteration 1254, loss = 0.70047734
Iteration 1255, loss = 0.69867714
Iteration 1256, loss = 0.69820354
Iteration 1257, loss = 0.69738245
Iteration 1258, loss = 0.69639968
Iteration 1259, loss = 0.69589383
Iteration 1260, loss = 0.69485959
Iteration 1261, loss = 0.69424446
Iteration 1262, loss = 0.69413004
Iteration 1263, loss = 0.69282967
Iteration 1264, loss = 0.69351299
Iteration 1265, loss = 0.69159145
Iteration 1266, loss = 0.69143246
Iteration 1267, loss = 0.69103947
Iteration 1268, loss = 0.69066942
Iteration 1269, loss = 0.68975169
Iteration 1270, loss = 0.68922571
Iteration 1271, loss = 0.68794571
Iteration 1272, loss = 0.68825534
Iteration 1273, loss = 0.68661712
Iteration 1274, loss = 0.68633451
Iteration 1275, loss = 0.68586816
Iteration 1276, loss = 0.68593106
Iteration 1277, loss = 0.68483477
Iteration 1278, loss = 0.68424920
Iteration 1279, loss = 0.68382952
Iteration 1280, loss = 0.68282823
Iteration 1281, loss = 0.68174268
Iteration 1282, loss = 0.68189307
Iteration 1283, loss = 0.68104655
Iteration 1284, loss = 0.68012283
Iteration 1285, loss = 0.67891487
Iteration 1286, loss = 0.67913959
Iteration 1287, loss = 0.67932781
Iteration 1288, loss = 0.67811505
Iteration 1289, loss = 0.67732450
Iteration 1290, loss = 0.67617266
Iteration 1291, loss = 0.67585133
Iteration 1292, loss = 0.67468009
Iteration 1293, loss = 0.67499935
Iteration 1294, loss = 0.67448602
Iteration 1295, loss = 0.67391126
Iteration 1296, loss = 0.67322950
Iteration 1297, loss = 0.67199992
Iteration 1298, loss = 0.67280342
Iteration 1299, loss = 0.67187318
Iteration 1300, loss = 0.67129956
Iteration 1301, loss = 0.66995097
Iteration 1302, loss = 0.66945676
Iteration 1303, loss = 0.66923450
Iteration 1304, loss = 0.66892675
Iteration 1305, loss = 0.66757821
Iteration 1306, loss = 0.66714187
Iteration 1307, loss = 0.66581636
Iteration 1308, loss = 0.66626377
Iteration 1309, loss = 0.66492349
Iteration 1310, loss = 0.66435853
Iteration 1311, loss = 0.66267759
Iteration 1312, loss = 0.66318291
Iteration 1313, loss = 0.66368391
Iteration 1314, loss = 0.66292668
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100), {'error': 1.0247964671849159, 'fit': 0.87987499999999996, 'time': 2321.05})
Iteration 1, loss = 6.24133250
Iteration 2, loss = 5.76526162
Iteration 3, loss = 5.63198202
Iteration 4, loss = 5.54397929
Iteration 5, loss = 5.47707264
Iteration 6, loss = 5.42074929
Iteration 7, loss = 5.37371872
Iteration 8, loss = 5.32953472
Iteration 9, loss = 5.28860154
Iteration 10, loss = 5.25128180
Iteration 11, loss = 5.21690308
Iteration 12, loss = 5.18044375
Iteration 13, loss = 5.14832809
Iteration 14, loss = 5.11628531
Iteration 15, loss = 5.08838767
Iteration 16, loss = 5.05623753
Iteration 17, loss = 5.02728019
Iteration 18, loss = 4.99970488
Iteration 19, loss = 4.97082372
Iteration 20, loss = 4.94398324
Iteration 21, loss = 4.91690349
Iteration 22, loss = 4.88728057
Iteration 23, loss = 4.86156426
Iteration 24, loss = 4.83525319
Iteration 25, loss = 4.80709230
Iteration 26, loss = 4.78217414
Iteration 27, loss = 4.75591752
Iteration 28, loss = 4.73086878
Iteration 29, loss = 4.70477281
Iteration 30, loss = 4.67672870
Iteration 31, loss = 4.64953799
Iteration 32, loss = 4.62289924
Iteration 33, loss = 4.59828604
Iteration 34, loss = 4.57046837
Iteration 35, loss = 4.54445538
Iteration 36, loss = 4.51802684
Iteration 37, loss = 4.49249473
Iteration 38, loss = 4.46719988
Iteration 39, loss = 4.44414843
Iteration 40, loss = 4.41874858
Iteration 41, loss = 4.39364243
Iteration 42, loss = 4.37073546
Iteration 43, loss = 4.34696195
Iteration 44, loss = 4.32330145
Iteration 45, loss = 4.29939878
Iteration 46, loss = 4.27837719
Iteration 47, loss = 4.25429797
Iteration 48, loss = 4.23004068
Iteration 49, loss = 4.20575063
Iteration 50, loss = 4.18456608
Iteration 51, loss = 4.16294882
Iteration 52, loss = 4.14185888
Iteration 53, loss = 4.11951286
Iteration 54, loss = 4.09658112
Iteration 55, loss = 4.07631719
Iteration 56, loss = 4.05459527
Iteration 57, loss = 4.03328967
Iteration 58, loss = 4.01304630
Iteration 59, loss = 3.99504480
Iteration 60, loss = 3.97253597
Iteration 61, loss = 3.95282738
Iteration 62, loss = 3.93315664
Iteration 63, loss = 3.91174501
Iteration 64, loss = 3.89312603
Iteration 65, loss = 3.87316128
Iteration 66, loss = 3.85442282
Iteration 67, loss = 3.83589687
Iteration 68, loss = 3.81814053
Iteration 69, loss = 3.80020774
Iteration 70, loss = 3.78223702
Iteration 71, loss = 3.76200023
Iteration 72, loss = 3.74181534
Iteration 73, loss = 3.72520164
Iteration 74, loss = 3.70829510
Iteration 75, loss = 3.68783283
Iteration 76, loss = 3.67288872
Iteration 77, loss = 3.65445906
Iteration 78, loss = 3.64016767
Iteration 79, loss = 3.62017397
Iteration 80, loss = 3.60348727
Iteration 81, loss = 3.58614179
Iteration 82, loss = 3.57033216
Iteration 83, loss = 3.55124518
Iteration 84, loss = 3.53730921
Iteration 85, loss = 3.51984107
Iteration 86, loss = 3.50338995
Iteration 87, loss = 3.48952020
Iteration 88, loss = 3.47188719
Iteration 89, loss = 3.45644172
Iteration 90, loss = 3.44125905
Iteration 91, loss = 3.42465674
Iteration 92, loss = 3.40731853
Iteration 93, loss = 3.39663463
Iteration 94, loss = 3.37801700
Iteration 95, loss = 3.36137818
Iteration 96, loss = 3.34865735
Iteration 97, loss = 3.33409704
Iteration 98, loss = 3.31840123
Iteration 99, loss = 3.30643578
Iteration 100, loss = 3.29045617
Iteration 101, loss = 3.27631291
Iteration 102, loss = 3.25925724
Iteration 103, loss = 3.24650445
Iteration 104, loss = 3.23161329
Iteration 105, loss = 3.21847022
Iteration 106, loss = 3.20482889
Iteration 107, loss = 3.18976920
Iteration 108, loss = 3.17743457
Iteration 109, loss = 3.16406428
Iteration 110, loss = 3.15081358
Iteration 111, loss = 3.13598480
Iteration 112, loss = 3.12227624
Iteration 113, loss = 3.11171667
Iteration 114, loss = 3.09871351
Iteration 115, loss = 3.08125414
Iteration 116, loss = 3.06925572
Iteration 117, loss = 3.05642120
Iteration 118, loss = 3.04323001
Iteration 119, loss = 3.03037284
Iteration 120, loss = 3.01712388
Iteration 121, loss = 3.00536139
Iteration 122, loss = 2.99230312
Iteration 123, loss = 2.97991819
Iteration 124, loss = 2.96920096
Iteration 125, loss = 2.95389213
Iteration 126, loss = 2.94299260
Iteration 127, loss = 2.92885262
Iteration 128, loss = 2.91878577
Iteration 129, loss = 2.90741471
Iteration 130, loss = 2.89463076
Iteration 131, loss = 2.88376813
Iteration 132, loss = 2.87072062
Iteration 133, loss = 2.85889186
Iteration 134, loss = 2.84904347
Iteration 135, loss = 2.83754178
Iteration 136, loss = 2.82388479
Iteration 137, loss = 2.81243779
Iteration 138, loss = 2.80042090
Iteration 139, loss = 2.79066417
Iteration 140, loss = 2.77968463
Iteration 141, loss = 2.76782063
Iteration 142, loss = 2.75678471
Iteration 143, loss = 2.74826447
Iteration 144, loss = 2.73617249
Iteration 145, loss = 2.72431694
Iteration 146, loss = 2.71492435
Iteration 147, loss = 2.70360915
Iteration 148, loss = 2.69184006
Iteration 149, loss = 2.68044371
Iteration 150, loss = 2.66921832
Iteration 151, loss = 2.66108448
Iteration 152, loss = 2.64869444
Iteration 153, loss = 2.63871535
Iteration 154, loss = 2.62983530
Iteration 155, loss = 2.61995409
Iteration 156, loss = 2.60827084
Iteration 157, loss = 2.59823328
Iteration 158, loss = 2.58748183
Iteration 159, loss = 2.58048765
Iteration 160, loss = 2.56793358
Iteration 161, loss = 2.55843778
Iteration 162, loss = 2.54913178
Iteration 163, loss = 2.53940556
Iteration 164, loss = 2.52924359
Iteration 165, loss = 2.52027277
Iteration 166, loss = 2.50895978
Iteration 167, loss = 2.50011997
Iteration 168, loss = 2.49195470
Iteration 169, loss = 2.48391524
Iteration 170, loss = 2.47120404
Iteration 171, loss = 2.46177312
Iteration 172, loss = 2.45340514
Iteration 173, loss = 2.44334653
Iteration 174, loss = 2.43582653
Iteration 175, loss = 2.42614422
Iteration 176, loss = 2.41500995
Iteration 177, loss = 2.40534975
Iteration 178, loss = 2.39715033
Iteration 179, loss = 2.38995445
Iteration 180, loss = 2.38225408
Iteration 181, loss = 2.37091934
Iteration 182, loss = 2.36176634
Iteration 183, loss = 2.35486265
Iteration 184, loss = 2.34496209
Iteration 185, loss = 2.33647529
Iteration 186, loss = 2.32636113
Iteration 187, loss = 2.31863799
Iteration 188, loss = 2.31171055
Iteration 189, loss = 2.30056160
Iteration 190, loss = 2.29161907
Iteration 191, loss = 2.28391764
Iteration 192, loss = 2.27564264
Iteration 193, loss = 2.26887412
Iteration 194, loss = 2.26015584
Iteration 195, loss = 2.25224043
Iteration 196, loss = 2.24259254
Iteration 197, loss = 2.23599640
Iteration 198, loss = 2.22721639
Iteration 199, loss = 2.21959771
Iteration 200, loss = 2.20852880
Iteration 201, loss = 2.20367397
Iteration 202, loss = 2.19494833
Iteration 203, loss = 2.18633809
Iteration 204, loss = 2.17848070
Iteration 205, loss = 2.16805063
Iteration 206, loss = 2.16214296
Iteration 207, loss = 2.15537621
Iteration 208, loss = 2.14848455
Iteration 209, loss = 2.13766358
Iteration 210, loss = 2.13269866
Iteration 211, loss = 2.12363388
Iteration 212, loss = 2.11632102
Iteration 213, loss = 2.10934915
Iteration 214, loss = 2.10392429
Iteration 215, loss = 2.09191526
Iteration 216, loss = 2.08543808
Iteration 217, loss = 2.08037111
Iteration 218, loss = 2.07162907
Iteration 219, loss = 2.06149392
Iteration 220, loss = 2.05649419
Iteration 221, loss = 2.04786522
Iteration 222, loss = 2.04150889
Iteration 223, loss = 2.03290511
Iteration 224, loss = 2.02906100
Iteration 225, loss = 2.01991600
Iteration 226, loss = 2.01095101
Iteration 227, loss = 2.00542833
Iteration 228, loss = 1.99831024
Iteration 229, loss = 1.99091998
Iteration 230, loss = 1.98563105
Iteration 231, loss = 1.98001901
Iteration 232, loss = 1.97087375
Iteration 233, loss = 1.96248797
Iteration 234, loss = 1.95649549
Iteration 235, loss = 1.95006254
Iteration 236, loss = 1.94047346
Iteration 237, loss = 1.93619896
Iteration 238, loss = 1.92770907
Iteration 239, loss = 1.92024141
Iteration 240, loss = 1.91652781
Iteration 241, loss = 1.91040904
Iteration 242, loss = 1.90070521
Iteration 243, loss = 1.89453866
Iteration 244, loss = 1.88893141
Iteration 245, loss = 1.88295298
Iteration 246, loss = 1.87689569
Iteration 247, loss = 1.86946382
Iteration 248, loss = 1.86224181
Iteration 249, loss = 1.85479806
Iteration 250, loss = 1.85019750
Iteration 251, loss = 1.84241944
Iteration 252, loss = 1.83680114
Iteration 253, loss = 1.83026305
Iteration 254, loss = 1.82426982
Iteration 255, loss = 1.81892388
Iteration 256, loss = 1.81074012
Iteration 257, loss = 1.80449118
Iteration 258, loss = 1.79801495
Iteration 259, loss = 1.79275517
Iteration 260, loss = 1.78627776
Iteration 261, loss = 1.78052531
Iteration 262, loss = 1.77436461
Iteration 263, loss = 1.76874915
Iteration 264, loss = 1.76110693
Iteration 265, loss = 1.75531564
Iteration 266, loss = 1.75062794
Iteration 267, loss = 1.74276546
Iteration 268, loss = 1.73865760
Iteration 269, loss = 1.73168470
Iteration 270, loss = 1.72679154
Iteration 271, loss = 1.71808765
Iteration 272, loss = 1.71563018
Iteration 273, loss = 1.70795280
Iteration 274, loss = 1.70295910
Iteration 275, loss = 1.69507236
Iteration 276, loss = 1.69066378
Iteration 277, loss = 1.68386020
Iteration 278, loss = 1.68149985
Iteration 279, loss = 1.67439257
Iteration 280, loss = 1.66566745
Iteration 281, loss = 1.66189438
Iteration 282, loss = 1.65572556
Iteration 283, loss = 1.65026856
Iteration 284, loss = 1.64357906
Iteration 285, loss = 1.63945457
Iteration 286, loss = 1.63330479
Iteration 287, loss = 1.62805790
Iteration 288, loss = 1.62383576
Iteration 289, loss = 1.61762259
Iteration 290, loss = 1.61197428
Iteration 291, loss = 1.60785468
Iteration 292, loss = 1.60139502
Iteration 293, loss = 1.59454917
Iteration 294, loss = 1.58795648
Iteration 295, loss = 1.58546807
Iteration 296, loss = 1.58066980
Iteration 297, loss = 1.57485028
Iteration 298, loss = 1.56872475
Iteration 299, loss = 1.56353248
Iteration 300, loss = 1.55827128
Iteration 301, loss = 1.55347793
Iteration 302, loss = 1.54774919
Iteration 303, loss = 1.54124678
Iteration 304, loss = 1.53538213
Iteration 305, loss = 1.53185103
Iteration 306, loss = 1.52518637
Iteration 307, loss = 1.52141052
Iteration 308, loss = 1.51732541
Iteration 309, loss = 1.50996360
Iteration 310, loss = 1.50543570
Iteration 311, loss = 1.49876189
Iteration 312, loss = 1.49488666
Iteration 313, loss = 1.49005965
Iteration 314, loss = 1.48606805
Iteration 315, loss = 1.48180644
Iteration 316, loss = 1.47742380
Iteration 317, loss = 1.47132958
Iteration 318, loss = 1.46714477
Iteration 319, loss = 1.46152601
Iteration 320, loss = 1.45622310
Iteration 321, loss = 1.45102563
Iteration 322, loss = 1.44574005
Iteration 323, loss = 1.44012053
Iteration 324, loss = 1.43611549
Iteration 325, loss = 1.42996683
Iteration 326, loss = 1.42655949
Iteration 327, loss = 1.42253473
Iteration 328, loss = 1.41739724
Iteration 329, loss = 1.41180715
Iteration 330, loss = 1.40677279
Iteration 331, loss = 1.40348651
Iteration 332, loss = 1.39729251
Iteration 333, loss = 1.39429814
Iteration 334, loss = 1.38860517
Iteration 335, loss = 1.38538768
Iteration 336, loss = 1.38047762
Iteration 337, loss = 1.37760880
Iteration 338, loss = 1.36940026
Iteration 339, loss = 1.36740164
Iteration 340, loss = 1.36060891
Iteration 341, loss = 1.35808298
Iteration 342, loss = 1.35277264
Iteration 343, loss = 1.34707915
Iteration 344, loss = 1.34292539
Iteration 345, loss = 1.33864563
Iteration 346, loss = 1.33451237
Iteration 347, loss = 1.32934854
Iteration 348, loss = 1.32586250
Iteration 349, loss = 1.32129823
Iteration 350, loss = 1.31950782
Iteration 351, loss = 1.31199239
Iteration 352, loss = 1.30756108
Iteration 353, loss = 1.30481048
Iteration 354, loss = 1.29829387
Iteration 355, loss = 1.29349771
Iteration 356, loss = 1.29034529
Iteration 357, loss = 1.28648912
Iteration 358, loss = 1.28081567
Iteration 359, loss = 1.27824560
Iteration 360, loss = 1.27234633
Iteration 361, loss = 1.26881852
Iteration 362, loss = 1.26639118
Iteration 363, loss = 1.25976290
Iteration 364, loss = 1.25637040
Iteration 365, loss = 1.25195626
Iteration 366, loss = 1.24867847
Iteration 367, loss = 1.24353637
Iteration 368, loss = 1.23924315
Iteration 369, loss = 1.23561880
Iteration 370, loss = 1.23035470
Iteration 371, loss = 1.22670959
Iteration 372, loss = 1.22563977
Iteration 373, loss = 1.22016013
Iteration 374, loss = 1.21412503
Iteration 375, loss = 1.21151424
Iteration 376, loss = 1.20711677
Iteration 377, loss = 1.20464300
Iteration 378, loss = 1.20021175
Iteration 379, loss = 1.19522066
Iteration 380, loss = 1.19192953
Iteration 381, loss = 1.18679872
Iteration 382, loss = 1.18508171
Iteration 383, loss = 1.17927694
Iteration 384, loss = 1.17628525
Iteration 385, loss = 1.17267974
Iteration 386, loss = 1.16955297
Iteration 387, loss = 1.16488109
Iteration 388, loss = 1.15910245
Iteration 389, loss = 1.15498339
Iteration 390, loss = 1.15239122
Iteration 391, loss = 1.14791191
Iteration 392, loss = 1.14463784
Iteration 393, loss = 1.14096931
Iteration 394, loss = 1.13563588
Iteration 395, loss = 1.13322910
Iteration 396, loss = 1.13022539
Iteration 397, loss = 1.12559946
Iteration 398, loss = 1.12473240
Iteration 399, loss = 1.11971705
Iteration 400, loss = 1.11581075
Iteration 401, loss = 1.11289895
Iteration 402, loss = 1.10815649
Iteration 403, loss = 1.10457676
Iteration 404, loss = 1.10108527
Iteration 405, loss = 1.09467982
Iteration 406, loss = 1.09111887
Iteration 407, loss = 1.09019445
Iteration 408, loss = 1.08481728
Iteration 409, loss = 1.08315412
Iteration 410, loss = 1.07723845
Iteration 411, loss = 1.07480032
Iteration 412, loss = 1.07256159
Iteration 413, loss = 1.06774634
Iteration 414, loss = 1.06389837
Iteration 415, loss = 1.06140113
Iteration 416, loss = 1.05911499
Iteration 417, loss = 1.05538486
Iteration 418, loss = 1.04922198
Iteration 419, loss = 1.04635504
Iteration 420, loss = 1.04356389
Iteration 421, loss = 1.03829014
Iteration 422, loss = 1.03745153
Iteration 423, loss = 1.03334182
Iteration 424, loss = 1.03059119
Iteration 425, loss = 1.02519943
Iteration 426, loss = 1.02188805
Iteration 427, loss = 1.02079358
Iteration 428, loss = 1.01590515
Iteration 429, loss = 1.01346078
Iteration 430, loss = 1.01139971
Iteration 431, loss = 1.00784005
Iteration 432, loss = 1.00068808
Iteration 433, loss = 1.00115403
Iteration 434, loss = 0.99740224
Iteration 435, loss = 0.99415059
Iteration 436, loss = 0.99102669
Iteration 437, loss = 0.98646905
Iteration 438, loss = 0.98132310
Iteration 439, loss = 0.97946962
Iteration 440, loss = 0.97661068
Iteration 441, loss = 0.97222036
Iteration 442, loss = 0.96886086
Iteration 443, loss = 0.96603982
Iteration 444, loss = 0.96337704
Iteration 445, loss = 0.96147997
Iteration 446, loss = 0.95591797
Iteration 447, loss = 0.95594217
Iteration 448, loss = 0.95053520
Iteration 449, loss = 0.94674487
Iteration 450, loss = 0.94456548
Iteration 451, loss = 0.94290328
Iteration 452, loss = 0.93745698
Iteration 453, loss = 0.93715962
Iteration 454, loss = 0.93135628
Iteration 455, loss = 0.92919174
Iteration 456, loss = 0.92581213
Iteration 457, loss = 0.92450455
Iteration 458, loss = 0.92069127
Iteration 459, loss = 0.91607433
Iteration 460, loss = 0.91432701
Iteration 461, loss = 0.91113329
Iteration 462, loss = 0.90710753
Iteration 463, loss = 0.90470191
Iteration 464, loss = 0.90103802
Iteration 465, loss = 0.89819177
Iteration 466, loss = 0.89676982
Iteration 467, loss = 0.89350337
Iteration 468, loss = 0.89040557
Iteration 469, loss = 0.88700816
Iteration 470, loss = 0.88545702
Iteration 471, loss = 0.88108132
Iteration 472, loss = 0.87980519
Iteration 473, loss = 0.87545995
Iteration 474, loss = 0.87244430
Iteration 475, loss = 0.86719212
Iteration 476, loss = 0.86723593
Iteration 477, loss = 0.86361373
Iteration 478, loss = 0.86243826
Iteration 479, loss = 0.85644753
Iteration 480, loss = 0.85446307
Iteration 481, loss = 0.85517160
Iteration 482, loss = 0.85114329
Iteration 483, loss = 0.84765295
Iteration 484, loss = 0.84489470
Iteration 485, loss = 0.84022217
Iteration 486, loss = 0.83961682
Iteration 487, loss = 0.83550137
Iteration 488, loss = 0.83309200
Iteration 489, loss = 0.83220658
Iteration 490, loss = 0.82677135
Iteration 491, loss = 0.82467111
Iteration 492, loss = 0.82368610
Iteration 493, loss = 0.81976002
Iteration 494, loss = 0.81721266
Iteration 495, loss = 0.81452215
Iteration 496, loss = 0.81183229
Iteration 497, loss = 0.80926837
Iteration 498, loss = 0.80747547
Iteration 499, loss = 0.80415840
Iteration 500, loss = 0.80256038
Iteration 501, loss = 0.79889199
Iteration 502, loss = 0.79481595
Iteration 503, loss = 0.79237400
Iteration 504, loss = 0.78991057
Iteration 505, loss = 0.78810394
Iteration 506, loss = 0.78406887
Iteration 507, loss = 0.78255896
Iteration 508, loss = 0.78144004
Iteration 509, loss = 0.77832182
Iteration 510, loss = 0.77585707
Iteration 511, loss = 0.77275671
Iteration 512, loss = 0.76969652
Iteration 513, loss = 0.76725277
Iteration 514, loss = 0.76617058
Iteration 515, loss = 0.76217573
Iteration 516, loss = 0.76064441
Iteration 517, loss = 0.75934093
Iteration 518, loss = 0.75908269
Iteration 519, loss = 0.75415398
Iteration 520, loss = 0.75065321
Iteration 521, loss = 0.75072360
Iteration 522, loss = 0.74523731
Iteration 523, loss = 0.74323201
Iteration 524, loss = 0.73991759
Iteration 525, loss = 0.73868195
Iteration 526, loss = 0.73520560
Iteration 527, loss = 0.73330310
Iteration 528, loss = 0.73001816
Iteration 529, loss = 0.72692573
Iteration 530, loss = 0.72751439
Iteration 531, loss = 0.72379677
Iteration 532, loss = 0.72069816
Iteration 533, loss = 0.71941511
Iteration 534, loss = 0.71418466
Iteration 535, loss = 0.71584330
Iteration 536, loss = 0.71053231
Iteration 537, loss = 0.70871947
Iteration 538, loss = 0.70724391
Iteration 539, loss = 0.70411018
Iteration 540, loss = 0.70093989
Iteration 541, loss = 0.70065725
Iteration 542, loss = 0.69586952
Iteration 543, loss = 0.69656430
Iteration 544, loss = 0.69278983
Iteration 545, loss = 0.69105531
Iteration 546, loss = 0.68874417
Iteration 547, loss = 0.68575234
Iteration 548, loss = 0.68431710
Iteration 549, loss = 0.68184779
Iteration 550, loss = 0.67774186
Iteration 551, loss = 0.67740264
Iteration 552, loss = 0.67542355
Iteration 553, loss = 0.67328095
Iteration 554, loss = 0.67081952
Iteration 555, loss = 0.66814054
Iteration 556, loss = 0.66518319
Iteration 557, loss = 0.66613188
Iteration 558, loss = 0.66094685
Iteration 559, loss = 0.65946887
Iteration 560, loss = 0.65775954
Iteration 561, loss = 0.65513596
Iteration 562, loss = 0.65369536
Iteration 563, loss = 0.65331906
Iteration 564, loss = 0.64999615
Iteration 565, loss = 0.64568903
Iteration 566, loss = 0.64413198
Iteration 567, loss = 0.64330246
Iteration 568, loss = 0.64079002
Iteration 569, loss = 0.63748403
Iteration 570, loss = 0.63600894
Iteration 571, loss = 0.63464570
Iteration 572, loss = 0.63333513
Iteration 573, loss = 0.63369260
Iteration 574, loss = 0.62794727
Iteration 575, loss = 0.62668342
Iteration 576, loss = 0.62379470
Iteration 577, loss = 0.62134322
Iteration 578, loss = 0.61962268
Iteration 579, loss = 0.61844686
Iteration 580, loss = 0.61675135
Iteration 581, loss = 0.61196898
Iteration 582, loss = 0.61062805
Iteration 583, loss = 0.60957728
Iteration 584, loss = 0.60760771
Iteration 585, loss = 0.60446087
Iteration 586, loss = 0.60288259
Iteration 587, loss = 0.60144817
Iteration 588, loss = 0.60077506
Iteration 589, loss = 0.59959912
Iteration 590, loss = 0.59545273
Iteration 591, loss = 0.59483487
Iteration 592, loss = 0.59161747
Iteration 593, loss = 0.59053659
Iteration 594, loss = 0.58851177
Iteration 595, loss = 0.58561563
Iteration 596, loss = 0.58306292
Iteration 597, loss = 0.58090085
Iteration 598, loss = 0.57979308
Iteration 599, loss = 0.57682004
Iteration 600, loss = 0.57586697
Iteration 601, loss = 0.57441480
Iteration 602, loss = 0.57184337
Iteration 603, loss = 0.57257591
Iteration 604, loss = 0.56648672
Iteration 605, loss = 0.56626497
Iteration 606, loss = 0.56489660
Iteration 607, loss = 0.56410390
Iteration 608, loss = 0.56130500
Iteration 609, loss = 0.55812638
Iteration 610, loss = 0.55662991
Iteration 611, loss = 0.55448094
Iteration 612, loss = 0.55390758
Iteration 613, loss = 0.55269503
Iteration 614, loss = 0.54856278
Iteration 615, loss = 0.54859496
Iteration 616, loss = 0.54691469
Iteration 617, loss = 0.54380840
Iteration 618, loss = 0.54147645
Iteration 619, loss = 0.54034492
Iteration 620, loss = 0.53726351
Iteration 621, loss = 0.53685997
Iteration 622, loss = 0.53654583
Iteration 623, loss = 0.53205797
Iteration 624, loss = 0.53176056
Iteration 625, loss = 0.52888452
Iteration 626, loss = 0.52874009
Iteration 627, loss = 0.52606367
Iteration 628, loss = 0.52432527
Iteration 629, loss = 0.52224337
Iteration 630, loss = 0.52078646
Iteration 631, loss = 0.52015707
Iteration 632, loss = 0.51693582
Iteration 633, loss = 0.51801619
Iteration 634, loss = 0.51487023
Iteration 635, loss = 0.51224459
Iteration 636, loss = 0.51175224
Iteration 637, loss = 0.50970020
Iteration 638, loss = 0.50817832
Iteration 639, loss = 0.50764541
Iteration 640, loss = 0.50593856
Iteration 641, loss = 0.50331696
Iteration 642, loss = 0.50257937
Iteration 643, loss = 0.49990144
Iteration 644, loss = 0.49749613
Iteration 645, loss = 0.49595465
Iteration 646, loss = 0.49483149
Iteration 647, loss = 0.49281011
Iteration 648, loss = 0.49044301
Iteration 649, loss = 0.48952058
Iteration 650, loss = 0.48820599
Iteration 651, loss = 0.48676971
Iteration 652, loss = 0.48606532
Iteration 653, loss = 0.48349751
Iteration 654, loss = 0.48217273
Iteration 655, loss = 0.48109377
Iteration 656, loss = 0.47928777
Iteration 657, loss = 0.47831771
Iteration 658, loss = 0.47674556
Iteration 659, loss = 0.47437067
Iteration 660, loss = 0.47476907
Iteration 661, loss = 0.46994365
Iteration 662, loss = 0.46965435
Iteration 663, loss = 0.46762594
Iteration 664, loss = 0.46671914
Iteration 665, loss = 0.46531609
Iteration 666, loss = 0.46407052
Iteration 667, loss = 0.46163832
Iteration 668, loss = 0.45967978
Iteration 669, loss = 0.45802005
Iteration 670, loss = 0.45520193
Iteration 671, loss = 0.45401248
Iteration 672, loss = 0.45386391
Iteration 673, loss = 0.45111459
Iteration 674, loss = 0.44955958
Iteration 675, loss = 0.44995622
Iteration 676, loss = 0.44758097
Iteration 677, loss = 0.44598556
Iteration 678, loss = 0.44476336
Iteration 679, loss = 0.44210614
Iteration 680, loss = 0.44105393
Iteration 681, loss = 0.43999141
Iteration 682, loss = 0.43776549
Iteration 683, loss = 0.43751689
Iteration 684, loss = 0.43575964
Iteration 685, loss = 0.43448829
Iteration 686, loss = 0.43405645
Iteration 687, loss = 0.43233117
Iteration 688, loss = 0.43177048
Iteration 689, loss = 0.42856674
Iteration 690, loss = 0.42709394
Iteration 691, loss = 0.42722298
Iteration 692, loss = 0.42473111
Iteration 693, loss = 0.42379021
Iteration 694, loss = 0.42060166
Iteration 695, loss = 0.41949837
Iteration 696, loss = 0.41925807
Iteration 697, loss = 0.41919430
Iteration 698, loss = 0.41753423
Iteration 699, loss = 0.41475953
Iteration 700, loss = 0.41260345
Iteration 701, loss = 0.41119713
Iteration 702, loss = 0.41114583
Iteration 703, loss = 0.40845330
Iteration 704, loss = 0.40759049
Iteration 705, loss = 0.40671037
Iteration 706, loss = 0.40562343
Iteration 707, loss = 0.40408135
Iteration 708, loss = 0.40199185
Iteration 709, loss = 0.40120631
Iteration 710, loss = 0.40036464
Iteration 711, loss = 0.39862425
Iteration 712, loss = 0.39696285
Iteration 713, loss = 0.39611645
Iteration 714, loss = 0.39420109
Iteration 715, loss = 0.39329416
Iteration 716, loss = 0.39157457
Iteration 717, loss = 0.39361935
Iteration 718, loss = 0.39011249
Iteration 719, loss = 0.38909792
Iteration 720, loss = 0.38596909
Iteration 721, loss = 0.38598151
Iteration 722, loss = 0.38440804
Iteration 723, loss = 0.38193291
Iteration 724, loss = 0.38295505
Iteration 725, loss = 0.38167326
Iteration 726, loss = 0.37995031
Iteration 727, loss = 0.37886433
Iteration 728, loss = 0.37694614
Iteration 729, loss = 0.37513287
Iteration 730, loss = 0.37440620
Iteration 731, loss = 0.37247938
Iteration 732, loss = 0.37349780
Iteration 733, loss = 0.37025410
Iteration 734, loss = 0.36918848
Iteration 735, loss = 0.36867744
Iteration 736, loss = 0.36642763
Iteration 737, loss = 0.36623467
Iteration 738, loss = 0.36345452
Iteration 739, loss = 0.36606619
Iteration 740, loss = 0.36093928
Iteration 741, loss = 0.36193913
Iteration 742, loss = 0.36018936
Iteration 743, loss = 0.35916268
Iteration 744, loss = 0.35728765
Iteration 745, loss = 0.35669202
Iteration 746, loss = 0.35591397
Iteration 747, loss = 0.35441301
Iteration 748, loss = 0.35278861
Iteration 749, loss = 0.35315263
Iteration 750, loss = 0.35091544
Iteration 751, loss = 0.35047676
Iteration 752, loss = 0.35015411
Iteration 753, loss = 0.34820657
Iteration 754, loss = 0.34662945
Iteration 755, loss = 0.34700037
Iteration 756, loss = 0.34544746
Iteration 757, loss = 0.34382149
Iteration 758, loss = 0.34073201
Iteration 759, loss = 0.34009404
Iteration 760, loss = 0.33927728
Iteration 761, loss = 0.33768034
Iteration 762, loss = 0.33767778
Iteration 763, loss = 0.33567562
Iteration 764, loss = 0.33371112
Iteration 765, loss = 0.33411657
Iteration 766, loss = 0.33396087
Iteration 767, loss = 0.33126514
Iteration 768, loss = 0.33134590
Iteration 769, loss = 0.32820908
Iteration 770, loss = 0.33001982
Iteration 771, loss = 0.32686056
Iteration 772, loss = 0.32534026
Iteration 773, loss = 0.32670342
Iteration 774, loss = 0.32509349
Iteration 775, loss = 0.32320882
Iteration 776, loss = 0.32139432
Iteration 777, loss = 0.32102926
Iteration 778, loss = 0.32009349
Iteration 779, loss = 0.31974448
Iteration 780, loss = 0.31794901
Iteration 781, loss = 0.31611519
Iteration 782, loss = 0.31655274
Iteration 783, loss = 0.31569307
Iteration 784, loss = 0.31357807
Iteration 785, loss = 0.31323722
Iteration 786, loss = 0.31199338
Iteration 787, loss = 0.31093094
Iteration 788, loss = 0.30947130
Iteration 789, loss = 0.30866739
Iteration 790, loss = 0.30739931
Iteration 791, loss = 0.30793729
Iteration 792, loss = 0.30659821
Iteration 793, loss = 0.30497002
Iteration 794, loss = 0.30325369
Iteration 795, loss = 0.30300362
Iteration 796, loss = 0.30118077
Iteration 797, loss = 0.30127923
Iteration 798, loss = 0.30056866
Iteration 799, loss = 0.29828960
Iteration 800, loss = 0.29893866
Iteration 801, loss = 0.29567076
Iteration 802, loss = 0.29571028
Iteration 803, loss = 0.29405723
Iteration 804, loss = 0.29458322
Iteration 805, loss = 0.29184718
Iteration 806, loss = 0.29181559
Iteration 807, loss = 0.29090669
Iteration 808, loss = 0.29065037
Iteration 809, loss = 0.28966894
Iteration 810, loss = 0.28792510
Iteration 811, loss = 0.28724387
Iteration 812, loss = 0.28596481
Iteration 813, loss = 0.28441118
Iteration 814, loss = 0.28478230
Iteration 815, loss = 0.28241024
Iteration 816, loss = 0.28192486
Iteration 817, loss = 0.28201126
Iteration 818, loss = 0.28170110
Iteration 819, loss = 0.28098157
Iteration 820, loss = 0.27960112
Iteration 821, loss = 0.27666053
Iteration 822, loss = 0.27709464
Iteration 823, loss = 0.27689940
Iteration 824, loss = 0.27557070
Iteration 825, loss = 0.27474700
Iteration 826, loss = 0.27391502
Iteration 827, loss = 0.27246218
Iteration 828, loss = 0.27173306
Iteration 829, loss = 0.27094849
Iteration 830, loss = 0.26963331
Iteration 831, loss = 0.26874281
Iteration 832, loss = 0.26772940
Iteration 833, loss = 0.26673357
Iteration 834, loss = 0.26627702
Iteration 835, loss = 0.26633629
Iteration 836, loss = 0.26484618
Iteration 837, loss = 0.26462023
Iteration 838, loss = 0.26238435
Iteration 839, loss = 0.26133823
Iteration 840, loss = 0.26034925
Iteration 841, loss = 0.26025397
Iteration 842, loss = 0.25903642
Iteration 843, loss = 0.25709746
Iteration 844, loss = 0.25971687
Iteration 845, loss = 0.25804828
Iteration 846, loss = 0.25729773
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100), {'error': 0.81147753078482365, 'fit': 0.98075000000000001, 'time': 1613.9799999999996})
Iteration 1, loss = 6.20563085
Iteration 2, loss = 5.76045752
Iteration 3, loss = 5.60477844
Iteration 4, loss = 5.50137228
Iteration 5, loss = 5.43139153
Iteration 6, loss = 5.38088054
Iteration 7, loss = 5.33197859
Iteration 8, loss = 5.28851494
Iteration 9, loss = 5.24461780
Iteration 10, loss = 5.20319692
Iteration 11, loss = 5.16267207
Iteration 12, loss = 5.12163615
Iteration 13, loss = 5.08555053
Iteration 14, loss = 5.04790463
Iteration 15, loss = 5.00811035
Iteration 16, loss = 4.96626333
Iteration 17, loss = 4.92686479
Iteration 18, loss = 4.88482212
Iteration 19, loss = 4.84613779
Iteration 20, loss = 4.81178845
Iteration 21, loss = 4.76965766
Iteration 22, loss = 4.73709489
Iteration 23, loss = 4.70326677
Iteration 24, loss = 4.66934777
Iteration 25, loss = 4.63719728
Iteration 26, loss = 4.60608973
Iteration 27, loss = 4.58213843
Iteration 28, loss = 4.54938250
Iteration 29, loss = 4.52249364
Iteration 30, loss = 4.49759946
Iteration 31, loss = 4.47002857
Iteration 32, loss = 4.44368535
Iteration 33, loss = 4.41871790
Iteration 34, loss = 4.39421818
Iteration 35, loss = 4.37694068
Iteration 36, loss = 4.34705907
Iteration 37, loss = 4.32030520
Iteration 38, loss = 4.30060222
Iteration 39, loss = 4.27695471
Iteration 40, loss = 4.25237751
Iteration 41, loss = 4.22901317
Iteration 42, loss = 4.20696793
Iteration 43, loss = 4.18917859
Iteration 44, loss = 4.16279976
Iteration 45, loss = 4.14331074
Iteration 46, loss = 4.12535164
Iteration 47, loss = 4.09789680
Iteration 48, loss = 4.07958337
Iteration 49, loss = 4.05922721
Iteration 50, loss = 4.03638399
Iteration 51, loss = 4.01438651
Iteration 52, loss = 3.99688918
Iteration 53, loss = 3.97772635
Iteration 54, loss = 3.95291488
Iteration 55, loss = 3.93665809
Iteration 56, loss = 3.91562339
Iteration 57, loss = 3.89923217
Iteration 58, loss = 3.87604689
Iteration 59, loss = 3.85556252
Iteration 60, loss = 3.83852899
Iteration 61, loss = 3.82488170
Iteration 62, loss = 3.79985636
Iteration 63, loss = 3.78008694
Iteration 64, loss = 3.75926914
Iteration 65, loss = 3.74216116
Iteration 66, loss = 3.72452163
Iteration 67, loss = 3.70219893
Iteration 68, loss = 3.68268751
Iteration 69, loss = 3.66595245
Iteration 70, loss = 3.65035730
Iteration 71, loss = 3.63492221
Iteration 72, loss = 3.61457215
Iteration 73, loss = 3.59441034
Iteration 74, loss = 3.57635455
Iteration 75, loss = 3.55851307
Iteration 76, loss = 3.54296693
Iteration 77, loss = 3.52570624
Iteration 78, loss = 3.50697882
Iteration 79, loss = 3.49177255
Iteration 80, loss = 3.47410985
Iteration 81, loss = 3.45877632
Iteration 82, loss = 3.44500883
Iteration 83, loss = 3.42331551
Iteration 84, loss = 3.40258959
Iteration 85, loss = 3.39046835
Iteration 86, loss = 3.37267490
Iteration 87, loss = 3.35451949
Iteration 88, loss = 3.33796319
Iteration 89, loss = 3.31915198
Iteration 90, loss = 3.30376247
Iteration 91, loss = 3.29014526
Iteration 92, loss = 3.27273772
Iteration 93, loss = 3.25750564
Iteration 94, loss = 3.24134598
Iteration 95, loss = 3.22570833
Iteration 96, loss = 3.20743258
Iteration 97, loss = 3.19261471
Iteration 98, loss = 3.17835330
Iteration 99, loss = 3.16221597
Iteration 100, loss = 3.14778404
Iteration 101, loss = 3.12972426
Iteration 102, loss = 3.11645775
Iteration 103, loss = 3.09935080
Iteration 104, loss = 3.08541693
Iteration 105, loss = 3.06989130
Iteration 106, loss = 3.05586311
Iteration 107, loss = 3.03860358
Iteration 108, loss = 3.02102008
Iteration 109, loss = 3.00646551
Iteration 110, loss = 2.99482642
Iteration 111, loss = 2.98176721
Iteration 112, loss = 2.96790540
Iteration 113, loss = 2.95269647
Iteration 114, loss = 2.93973909
Iteration 115, loss = 2.92046582
Iteration 116, loss = 2.90827586
Iteration 117, loss = 2.89786213
Iteration 118, loss = 2.87900797
Iteration 119, loss = 2.86520027
Iteration 120, loss = 2.85428396
Iteration 121, loss = 2.83943875
Iteration 122, loss = 2.82149894
Iteration 123, loss = 2.80849267
Iteration 124, loss = 2.79330477
Iteration 125, loss = 2.78465092
Iteration 126, loss = 2.76864870
Iteration 127, loss = 2.75442801
Iteration 128, loss = 2.74110238
Iteration 129, loss = 2.72839641
Iteration 130, loss = 2.71586213
Iteration 131, loss = 2.69956911
Iteration 132, loss = 2.69012129
Iteration 133, loss = 2.67287829
Iteration 134, loss = 2.65913159
Iteration 135, loss = 2.64694912
Iteration 136, loss = 2.63815363
Iteration 137, loss = 2.61766527
Iteration 138, loss = 2.61043458
Iteration 139, loss = 2.59914489
Iteration 140, loss = 2.58350883
Iteration 141, loss = 2.57057330
Iteration 142, loss = 2.55570310
Iteration 143, loss = 2.54330065
Iteration 144, loss = 2.53209699
Iteration 145, loss = 2.52094241
Iteration 146, loss = 2.50610967
Iteration 147, loss = 2.49629543
Iteration 148, loss = 2.48369512
Iteration 149, loss = 2.46993139
Iteration 150, loss = 2.46255399
Iteration 151, loss = 2.44836691
Iteration 152, loss = 2.43551035
Iteration 153, loss = 2.42712289
Iteration 154, loss = 2.41380863
Iteration 155, loss = 2.40211718
Iteration 156, loss = 2.39379953
Iteration 157, loss = 2.37904485
Iteration 158, loss = 2.36446767
Iteration 159, loss = 2.35853948
Iteration 160, loss = 2.34444894
Iteration 161, loss = 2.33257202
Iteration 162, loss = 2.32103330
Iteration 163, loss = 2.30970503
Iteration 164, loss = 2.29859463
Iteration 165, loss = 2.28709783
Iteration 166, loss = 2.27519253
Iteration 167, loss = 2.26812892
Iteration 168, loss = 2.25417714
Iteration 169, loss = 2.24339682
Iteration 170, loss = 2.23054997
Iteration 171, loss = 2.22149702
Iteration 172, loss = 2.20975296
Iteration 173, loss = 2.20196485
Iteration 174, loss = 2.19075880
Iteration 175, loss = 2.17799940
Iteration 176, loss = 2.16868726
Iteration 177, loss = 2.15806539
Iteration 178, loss = 2.14782765
Iteration 179, loss = 2.13674411
Iteration 180, loss = 2.13062642
Iteration 181, loss = 2.11725461
Iteration 182, loss = 2.10769351
Iteration 183, loss = 2.09157169
Iteration 184, loss = 2.08254180
Iteration 185, loss = 2.07485315
Iteration 186, loss = 2.06163816
Iteration 187, loss = 2.05337169
Iteration 188, loss = 2.04301875
Iteration 189, loss = 2.03410776
Iteration 190, loss = 2.02496765
Iteration 191, loss = 2.01679060
Iteration 192, loss = 2.00376910
Iteration 193, loss = 1.99394789
Iteration 194, loss = 1.98555960
Iteration 195, loss = 1.97372842
Iteration 196, loss = 1.97083836
Iteration 197, loss = 1.95863370
Iteration 198, loss = 1.94731274
Iteration 199, loss = 1.93750838
Iteration 200, loss = 1.92571196
Iteration 201, loss = 1.92205483
Iteration 202, loss = 1.91180722
Iteration 203, loss = 1.89987096
Iteration 204, loss = 1.89135440
Iteration 205, loss = 1.88196143
Iteration 206, loss = 1.87562476
Iteration 207, loss = 1.86794484
Iteration 208, loss = 1.85777121
Iteration 209, loss = 1.84528411
Iteration 210, loss = 1.84185626
Iteration 211, loss = 1.83016662
Iteration 212, loss = 1.82107052
Iteration 213, loss = 1.80923084
Iteration 214, loss = 1.80344503
Iteration 215, loss = 1.78925435
Iteration 216, loss = 1.78208259
Iteration 217, loss = 1.77386658
Iteration 218, loss = 1.77187430
Iteration 219, loss = 1.76026990
Iteration 220, loss = 1.74744510
Iteration 221, loss = 1.74675686
Iteration 222, loss = 1.73589874
Iteration 223, loss = 1.72185885
Iteration 224, loss = 1.71554190
Iteration 225, loss = 1.70626795
Iteration 226, loss = 1.70050034
Iteration 227, loss = 1.68954735
Iteration 228, loss = 1.68689759
Iteration 229, loss = 1.67792522
Iteration 230, loss = 1.66866526
Iteration 231, loss = 1.66421337
Iteration 232, loss = 1.65135495
Iteration 233, loss = 1.64404691
Iteration 234, loss = 1.63421685
Iteration 235, loss = 1.62460321
Iteration 236, loss = 1.61793258
Iteration 237, loss = 1.61197831
Iteration 238, loss = 1.60173025
Iteration 239, loss = 1.59474880
Iteration 240, loss = 1.58729831
Iteration 241, loss = 1.58219619
Iteration 242, loss = 1.57432214
Iteration 243, loss = 1.56692437
Iteration 244, loss = 1.55372008
Iteration 245, loss = 1.54591948
Iteration 246, loss = 1.54389976
Iteration 247, loss = 1.53177032
Iteration 248, loss = 1.52781839
Iteration 249, loss = 1.51721682
Iteration 250, loss = 1.50934161
Iteration 251, loss = 1.50495998
Iteration 252, loss = 1.49923528
Iteration 253, loss = 1.49252738
Iteration 254, loss = 1.48694418
Iteration 255, loss = 1.47260432
Iteration 256, loss = 1.46724294
Iteration 257, loss = 1.46127951
Iteration 258, loss = 1.45666110
Iteration 259, loss = 1.44788293
Iteration 260, loss = 1.44174385
Iteration 261, loss = 1.43025588
Iteration 262, loss = 1.42279211
Iteration 263, loss = 1.42158492
Iteration 264, loss = 1.41315825
Iteration 265, loss = 1.40490784
Iteration 266, loss = 1.39336106
Iteration 267, loss = 1.38692745
Iteration 268, loss = 1.37990591
Iteration 269, loss = 1.37348452
Iteration 270, loss = 1.37205349
Iteration 271, loss = 1.36793841
Iteration 272, loss = 1.35557922
Iteration 273, loss = 1.34796805
Iteration 274, loss = 1.34608820
Iteration 275, loss = 1.33644433
Iteration 276, loss = 1.33494569
Iteration 277, loss = 1.32385042
Iteration 278, loss = 1.31956091
Iteration 279, loss = 1.31442222
Iteration 280, loss = 1.30588782
Iteration 281, loss = 1.29725639
Iteration 282, loss = 1.29015523
Iteration 283, loss = 1.28650111
Iteration 284, loss = 1.27562209
Iteration 285, loss = 1.26905212
Iteration 286, loss = 1.26570417
Iteration 287, loss = 1.26131270
Iteration 288, loss = 1.25148269
Iteration 289, loss = 1.24904532
Iteration 290, loss = 1.23857437
Iteration 291, loss = 1.23658605
Iteration 292, loss = 1.23248859
Iteration 293, loss = 1.22211364
Iteration 294, loss = 1.21665732
Iteration 295, loss = 1.21275904
Iteration 296, loss = 1.20308981
Iteration 297, loss = 1.20349131
Iteration 298, loss = 1.19217270
Iteration 299, loss = 1.18904965
Iteration 300, loss = 1.18392050
Iteration 301, loss = 1.17663280
Iteration 302, loss = 1.16935471
Iteration 303, loss = 1.16561900
Iteration 304, loss = 1.16458371
Iteration 305, loss = 1.15486224
Iteration 306, loss = 1.14925857
Iteration 307, loss = 1.14400535
Iteration 308, loss = 1.13732051
Iteration 309, loss = 1.13497813
Iteration 310, loss = 1.12584484
Iteration 311, loss = 1.11675615
Iteration 312, loss = 1.11123480
Iteration 313, loss = 1.10745374
Iteration 314, loss = 1.10077493
Iteration 315, loss = 1.09960481
Iteration 316, loss = 1.09080050
Iteration 317, loss = 1.08827331
Iteration 318, loss = 1.08511078
Iteration 319, loss = 1.07948415
Iteration 320, loss = 1.07928399
Iteration 321, loss = 1.06794203
Iteration 322, loss = 1.05976081
Iteration 323, loss = 1.05283326
Iteration 324, loss = 1.04821217
Iteration 325, loss = 1.04230237
Iteration 326, loss = 1.04217426
Iteration 327, loss = 1.03438322
Iteration 328, loss = 1.02922402
Iteration 329, loss = 1.02470370
Iteration 330, loss = 1.01839271
Iteration 331, loss = 1.01300159
Iteration 332, loss = 1.00710632
Iteration 333, loss = 1.00296008
Iteration 334, loss = 0.99768999
Iteration 335, loss = 0.99477339
Iteration 336, loss = 0.99024981
Iteration 337, loss = 0.98374283
Iteration 338, loss = 0.97527257
Iteration 339, loss = 0.97131235
Iteration 340, loss = 0.96737665
Iteration 341, loss = 0.96858470
Iteration 342, loss = 0.95842068
Iteration 343, loss = 0.95167971
Iteration 344, loss = 0.94983924
Iteration 345, loss = 0.94426777
Iteration 346, loss = 0.94027450
Iteration 347, loss = 0.93449081
Iteration 348, loss = 0.93169315
Iteration 349, loss = 0.92902898
Iteration 350, loss = 0.91886893
Iteration 351, loss = 0.91593932
Iteration 352, loss = 0.91003358
Iteration 353, loss = 0.90705872
Iteration 354, loss = 0.90287217
Iteration 355, loss = 0.89541290
Iteration 356, loss = 0.89602086
Iteration 357, loss = 0.88968765
Iteration 358, loss = 0.88294598
Iteration 359, loss = 0.87940014
Iteration 360, loss = 0.88103612
Iteration 361, loss = 0.87030436
Iteration 362, loss = 0.86544056
Iteration 363, loss = 0.86035543
Iteration 364, loss = 0.85904100
Iteration 365, loss = 0.85227918
Iteration 366, loss = 0.85379136
Iteration 367, loss = 0.84431327
Iteration 368, loss = 0.83756278
Iteration 369, loss = 0.83497651
Iteration 370, loss = 0.83256759
Iteration 371, loss = 0.83126645
Iteration 372, loss = 0.82483156
Iteration 373, loss = 0.81987420
Iteration 374, loss = 0.81381346
Iteration 375, loss = 0.80910993
Iteration 376, loss = 0.80768029
Iteration 377, loss = 0.80271905
Iteration 378, loss = 0.80067006
Iteration 379, loss = 0.79393316
Iteration 380, loss = 0.79088808
Iteration 381, loss = 0.78367164
Iteration 382, loss = 0.78202865
Iteration 383, loss = 0.78256545
Iteration 384, loss = 0.77332853
Iteration 385, loss = 0.77141418
Iteration 386, loss = 0.76942899
Iteration 387, loss = 0.76538477
Iteration 388, loss = 0.76205233
Iteration 389, loss = 0.75221561
Iteration 390, loss = 0.74836069
Iteration 391, loss = 0.74714556
Iteration 392, loss = 0.74307385
Iteration 393, loss = 0.74165749
Iteration 394, loss = 0.73528254
Iteration 395, loss = 0.73650067
Iteration 396, loss = 0.72830090
Iteration 397, loss = 0.72567958
Iteration 398, loss = 0.72665069
Iteration 399, loss = 0.71575436
Iteration 400, loss = 0.71204847
Iteration 401, loss = 0.70793899
Iteration 402, loss = 0.70417280
Iteration 403, loss = 0.70191927
Iteration 404, loss = 0.69706110
Iteration 405, loss = 0.69335876
Iteration 406, loss = 0.69172102
Iteration 407, loss = 0.68559111
Iteration 408, loss = 0.68289088
Iteration 409, loss = 0.67860728
Iteration 410, loss = 0.67523759
Iteration 411, loss = 0.67320755
Iteration 412, loss = 0.67179195
Iteration 413, loss = 0.66792285
Iteration 414, loss = 0.66247233
Iteration 415, loss = 0.65955198
Iteration 416, loss = 0.65502702
Iteration 417, loss = 0.64907666
Iteration 418, loss = 0.64815348
Iteration 419, loss = 0.64668868
Iteration 420, loss = 0.64209863
Iteration 421, loss = 0.64356488
Iteration 422, loss = 0.63978654
Iteration 423, loss = 0.63551741
Iteration 424, loss = 0.63095597
Iteration 425, loss = 0.62563480
Iteration 426, loss = 0.62156247
Iteration 427, loss = 0.61943336
Iteration 428, loss = 0.61573899
Iteration 429, loss = 0.61280237
Iteration 430, loss = 0.60963986
Iteration 431, loss = 0.61115834
Iteration 432, loss = 0.60223187
Iteration 433, loss = 0.60140903
Iteration 434, loss = 0.59843529
Iteration 435, loss = 0.59356481
Iteration 436, loss = 0.58988675
Iteration 437, loss = 0.58820159
Iteration 438, loss = 0.58840314
Iteration 439, loss = 0.58241135
Iteration 440, loss = 0.58090959
Iteration 441, loss = 0.57787269
Iteration 442, loss = 0.57246886
Iteration 443, loss = 0.57108220
Iteration 444, loss = 0.56669307
Iteration 445, loss = 0.56435689
Iteration 446, loss = 0.55928238
Iteration 447, loss = 0.55563682
Iteration 448, loss = 0.56008474
Iteration 449, loss = 0.55596646
Iteration 450, loss = 0.54844576
Iteration 451, loss = 0.54569607
Iteration 452, loss = 0.54386580
Iteration 453, loss = 0.54306989
Iteration 454, loss = 0.54096402
Iteration 455, loss = 0.53650893
Iteration 456, loss = 0.52918325
Iteration 457, loss = 0.52748691
Iteration 458, loss = 0.52579523
Iteration 459, loss = 0.52241077
Iteration 460, loss = 0.52025846
Iteration 461, loss = 0.51468624
Iteration 462, loss = 0.51457661
Iteration 463, loss = 0.51264406
Iteration 464, loss = 0.51031435
Iteration 465, loss = 0.51053366
Iteration 466, loss = 0.50504510
Iteration 467, loss = 0.50253014
Iteration 468, loss = 0.50035247
Iteration 469, loss = 0.49588285
Iteration 470, loss = 0.49550831
Iteration 471, loss = 0.49436719
Iteration 472, loss = 0.49195717
Iteration 473, loss = 0.48651352
Iteration 474, loss = 0.48430253
Iteration 475, loss = 0.48249919
Iteration 476, loss = 0.48107680
Iteration 477, loss = 0.47351092
Iteration 478, loss = 0.46817408
Iteration 479, loss = 0.47155130
Iteration 480, loss = 0.46782271
Iteration 481, loss = 0.46504070
Iteration 482, loss = 0.46452561
Iteration 483, loss = 0.46349857
Iteration 484, loss = 0.46142174
Iteration 485, loss = 0.46406984
Iteration 486, loss = 0.45850844
Iteration 487, loss = 0.45503069
Iteration 488, loss = 0.45159671
Iteration 489, loss = 0.44466934
Iteration 490, loss = 0.44624955
Iteration 491, loss = 0.44200112
Iteration 492, loss = 0.43849527
Iteration 493, loss = 0.43604577
Iteration 494, loss = 0.43110583
Iteration 495, loss = 0.42861831
Iteration 496, loss = 0.42896043
Iteration 497, loss = 0.42519655
Iteration 498, loss = 0.42584748
Iteration 499, loss = 0.42313205
Iteration 500, loss = 0.41964680
Iteration 501, loss = 0.42288019
Iteration 502, loss = 0.41852143
Iteration 503, loss = 0.41366804
Iteration 504, loss = 0.40881878
Iteration 505, loss = 0.40975874
Iteration 506, loss = 0.40862685
Iteration 507, loss = 0.40547757
Iteration 508, loss = 0.40187235
Iteration 509, loss = 0.40435158
Iteration 510, loss = 0.40703515
Iteration 511, loss = 0.40107898
Iteration 512, loss = 0.39646899
Iteration 513, loss = 0.39422574
Iteration 514, loss = 0.38821972
Iteration 515, loss = 0.38731916
Iteration 516, loss = 0.38662186
Iteration 517, loss = 0.38650263
Iteration 518, loss = 0.37965998
Iteration 519, loss = 0.37691572
Iteration 520, loss = 0.37880552
Iteration 521, loss = 0.37555345
Iteration 522, loss = 0.37207736
Iteration 523, loss = 0.36915922
Iteration 524, loss = 0.36672175
Iteration 525, loss = 0.36983865
Iteration 526, loss = 0.36330454
Iteration 527, loss = 0.36318006
Iteration 528, loss = 0.36032192
Iteration 529, loss = 0.35980668
Iteration 530, loss = 0.35882866
Iteration 531, loss = 0.36010072
Iteration 532, loss = 0.35345205
Iteration 533, loss = 0.35296379
Iteration 534, loss = 0.34739602
Iteration 535, loss = 0.34730585
Iteration 536, loss = 0.34576647
Iteration 537, loss = 0.34454742
Iteration 538, loss = 0.34156480
Iteration 539, loss = 0.33879430
Iteration 540, loss = 0.33977447
Iteration 541, loss = 0.33641326
Iteration 542, loss = 0.33627118
Iteration 543, loss = 0.33627018
Iteration 544, loss = 0.34313525
Iteration 545, loss = 0.33314543
Iteration 546, loss = 0.33030693
Iteration 547, loss = 0.32813530
Iteration 548, loss = 0.32559206
Iteration 549, loss = 0.32036096
Iteration 550, loss = 0.31987862
Iteration 551, loss = 0.31751085
Iteration 552, loss = 0.31500632
Iteration 553, loss = 0.31412803
Iteration 554, loss = 0.31571125
Iteration 555, loss = 0.31296579
Iteration 556, loss = 0.31175888
Iteration 557, loss = 0.30590432
Iteration 558, loss = 0.30746136
Iteration 559, loss = 0.31709571
Iteration 560, loss = 0.30731976
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100), {'error': 0.78811819726127197, 'fit': 0.97375, 'time': 1153.88})
Iteration 1, loss = 6.19560309
Iteration 2, loss = 5.74276348
Iteration 3, loss = 5.56863090
Iteration 4, loss = 5.48112068
Iteration 5, loss = 5.42738838
Iteration 6, loss = 5.36853197
Iteration 7, loss = 5.31681993
Iteration 8, loss = 5.26370387
Iteration 9, loss = 5.20623053
Iteration 10, loss = 5.15544338
Iteration 11, loss = 5.09715642
Iteration 12, loss = 5.04096757
Iteration 13, loss = 4.98684626
Iteration 14, loss = 4.94708728
Iteration 15, loss = 4.90238307
Iteration 16, loss = 4.85098431
Iteration 17, loss = 4.82081214
Iteration 18, loss = 4.78390887
Iteration 19, loss = 4.74117521
Iteration 20, loss = 4.71019813
Iteration 21, loss = 4.67767538
Iteration 22, loss = 4.64647781
Iteration 23, loss = 4.61551206
Iteration 24, loss = 4.58312010
Iteration 25, loss = 4.56520644
Iteration 26, loss = 4.53415500
Iteration 27, loss = 4.50642718
Iteration 28, loss = 4.47932871
Iteration 29, loss = 4.45440548
Iteration 30, loss = 4.43146043
Iteration 31, loss = 4.41131424
Iteration 32, loss = 4.38329654
Iteration 33, loss = 4.36072151
Iteration 34, loss = 4.33979357
Iteration 35, loss = 4.31866740
Iteration 36, loss = 4.29295003
Iteration 37, loss = 4.26941941
Iteration 38, loss = 4.24396686
Iteration 39, loss = 4.22431022
Iteration 40, loss = 4.20075927
Iteration 41, loss = 4.18785710
Iteration 42, loss = 4.16260744
Iteration 43, loss = 4.14419122
Iteration 44, loss = 4.11819402
Iteration 45, loss = 4.10737686
Iteration 46, loss = 4.08197192
Iteration 47, loss = 4.05438456
Iteration 48, loss = 4.03895238
Iteration 49, loss = 4.01892236
Iteration 50, loss = 3.99577931
Iteration 51, loss = 3.98457870
Iteration 52, loss = 3.96053391
Iteration 53, loss = 3.94355157
Iteration 54, loss = 3.93615044
Iteration 55, loss = 3.90414696
Iteration 56, loss = 3.88565334
Iteration 57, loss = 3.87164797
Iteration 58, loss = 3.85011184
Iteration 59, loss = 3.83284627
Iteration 60, loss = 3.81510771
Iteration 61, loss = 3.79642181
Iteration 62, loss = 3.78378456
Iteration 63, loss = 3.75707217
Iteration 64, loss = 3.74295543
Iteration 65, loss = 3.72130265
Iteration 66, loss = 3.70054952
Iteration 67, loss = 3.69625489
Iteration 68, loss = 3.67415029
Iteration 69, loss = 3.65217025
Iteration 70, loss = 3.63921323
Iteration 71, loss = 3.61665239
Iteration 72, loss = 3.59867678
Iteration 73, loss = 3.58316807
Iteration 74, loss = 3.56723186
Iteration 75, loss = 3.54870960
Iteration 76, loss = 3.53716911
Iteration 77, loss = 3.52849293
Iteration 78, loss = 3.50965284
Iteration 79, loss = 3.49540685
Iteration 80, loss = 3.46741890
Iteration 81, loss = 3.44879455
Iteration 82, loss = 3.43451499
Iteration 83, loss = 3.42618135
Iteration 84, loss = 3.39804093
Iteration 85, loss = 3.38471197
Iteration 86, loss = 3.36323861
Iteration 87, loss = 3.35451102
Iteration 88, loss = 3.33550182
Iteration 89, loss = 3.32071685
Iteration 90, loss = 3.30764120
Iteration 91, loss = 3.28945417
Iteration 92, loss = 3.28151777
Iteration 93, loss = 3.26619872
Iteration 94, loss = 3.24535862
Iteration 95, loss = 3.22663617
Iteration 96, loss = 3.21026630
Iteration 97, loss = 3.19580281
Iteration 98, loss = 3.17823965
Iteration 99, loss = 3.16096781
Iteration 100, loss = 3.15780533
Iteration 101, loss = 3.13346664
Iteration 102, loss = 3.11974352
Iteration 103, loss = 3.11277311
Iteration 104, loss = 3.09295516
Iteration 105, loss = 3.07824574
Iteration 106, loss = 3.06303132
Iteration 107, loss = 3.04493925
Iteration 108, loss = 3.03321127
Iteration 109, loss = 3.01976958
Iteration 110, loss = 3.01005879
Iteration 111, loss = 2.98886349
Iteration 112, loss = 2.96667717
Iteration 113, loss = 2.95030291
Iteration 114, loss = 2.93959182
Iteration 115, loss = 2.92855212
Iteration 116, loss = 2.91077414
Iteration 117, loss = 2.90643855
Iteration 118, loss = 2.88102412
Iteration 119, loss = 2.88197907
Iteration 120, loss = 2.86357795
Iteration 121, loss = 2.84488804
Iteration 122, loss = 2.83593526
Iteration 123, loss = 2.81310783
Iteration 124, loss = 2.81794859
Iteration 125, loss = 2.80252767
Iteration 126, loss = 2.77959397
Iteration 127, loss = 2.77109015
Iteration 128, loss = 2.75246498
Iteration 129, loss = 2.73721134
Iteration 130, loss = 2.73039276
Iteration 131, loss = 2.70828313
Iteration 132, loss = 2.70105721
Iteration 133, loss = 2.68379749
Iteration 134, loss = 2.67003700
Iteration 135, loss = 2.68393790
Iteration 136, loss = 2.65492650
Iteration 137, loss = 2.63736495
Iteration 138, loss = 2.62320715
Iteration 139, loss = 2.60583444
Iteration 140, loss = 2.59606340
Iteration 141, loss = 2.57773384
Iteration 142, loss = 2.56858986
Iteration 143, loss = 2.55773203
Iteration 144, loss = 2.54088416
Iteration 145, loss = 2.52888754
Iteration 146, loss = 2.51040051
Iteration 147, loss = 2.51757122
Iteration 148, loss = 2.50140057
Iteration 149, loss = 2.47970414
Iteration 150, loss = 2.46785478
Iteration 151, loss = 2.45653063
Iteration 152, loss = 2.44313473
Iteration 153, loss = 2.43527246
Iteration 154, loss = 2.41670149
Iteration 155, loss = 2.40962968
Iteration 156, loss = 2.39783362
Iteration 157, loss = 2.39083298
Iteration 158, loss = 2.38225129
Iteration 159, loss = 2.36457084
Iteration 160, loss = 2.35378910
Iteration 161, loss = 2.33666843
Iteration 162, loss = 2.33329018
Iteration 163, loss = 2.32299061
Iteration 164, loss = 2.30641414
Iteration 165, loss = 2.28949486
Iteration 166, loss = 2.28032923
Iteration 167, loss = 2.26764299
Iteration 168, loss = 2.25840728
Iteration 169, loss = 2.24862075
Iteration 170, loss = 2.23513884
Iteration 171, loss = 2.23620068
Iteration 172, loss = 2.21638669
Iteration 173, loss = 2.20345910
Iteration 174, loss = 2.19091803
Iteration 175, loss = 2.18160260
Iteration 176, loss = 2.17762694
Iteration 177, loss = 2.15768746
Iteration 178, loss = 2.15272185
Iteration 179, loss = 2.13628678
Iteration 180, loss = 2.13293525
Iteration 181, loss = 2.13025343
Iteration 182, loss = 2.10776408
Iteration 183, loss = 2.09739027
Iteration 184, loss = 2.09004095
Iteration 185, loss = 2.07758995
Iteration 186, loss = 2.07203525
Iteration 187, loss = 2.06239883
Iteration 188, loss = 2.05071949
Iteration 189, loss = 2.04305352
Iteration 190, loss = 2.02797463
Iteration 191, loss = 2.01581852
Iteration 192, loss = 2.01226846
Iteration 193, loss = 2.00383688
Iteration 194, loss = 1.98857941
Iteration 195, loss = 1.97468415
Iteration 196, loss = 1.96881403
Iteration 197, loss = 1.94678703
Iteration 198, loss = 1.95243674
Iteration 199, loss = 1.94032930
Iteration 200, loss = 1.92936024
Iteration 201, loss = 1.91453739
Iteration 202, loss = 1.90938528
Iteration 203, loss = 1.89519231
Iteration 204, loss = 1.88912548
Iteration 205, loss = 1.87191749
Iteration 206, loss = 1.87566049
Iteration 207, loss = 1.86123970
Iteration 208, loss = 1.85674371
Iteration 209, loss = 1.84458331
Iteration 210, loss = 1.83667364
Iteration 211, loss = 1.84286021
Iteration 212, loss = 1.81564934
Iteration 213, loss = 1.80183662
Iteration 214, loss = 1.79328291
Iteration 215, loss = 1.79366032
Iteration 216, loss = 1.77251113
Iteration 217, loss = 1.76782870
Iteration 218, loss = 1.76219485
Iteration 219, loss = 1.75304160
Iteration 220, loss = 1.74613233
Iteration 221, loss = 1.73992197
Iteration 222, loss = 1.72096454
Iteration 223, loss = 1.71541341
Iteration 224, loss = 1.69893337
Iteration 225, loss = 1.69171335
Iteration 226, loss = 1.69446433
Iteration 227, loss = 1.68578115
Iteration 228, loss = 1.67988346
Iteration 229, loss = 1.67133914
Iteration 230, loss = 1.66637281
Iteration 231, loss = 1.63963243
Iteration 232, loss = 1.64071599
Iteration 233, loss = 1.63207401
Iteration 234, loss = 1.61736197
Iteration 235, loss = 1.62148961
Iteration 236, loss = 1.62207374
Iteration 237, loss = 1.59994200
Iteration 238, loss = 1.59084381
Iteration 239, loss = 1.58334598
Iteration 240, loss = 1.57749268
Iteration 241, loss = 1.57213205
Iteration 242, loss = 1.56204457
Iteration 243, loss = 1.55505398
Iteration 244, loss = 1.54773969
Iteration 245, loss = 1.53745346
Iteration 246, loss = 1.53399627
Iteration 247, loss = 1.52957524
Iteration 248, loss = 1.52359067
Iteration 249, loss = 1.50086782
Iteration 250, loss = 1.49440708
Iteration 251, loss = 1.49058823
Iteration 252, loss = 1.48466467
Iteration 253, loss = 1.47966292
Iteration 254, loss = 1.46105904
Iteration 255, loss = 1.45836652
Iteration 256, loss = 1.44969855
Iteration 257, loss = 1.44471164
Iteration 258, loss = 1.43583149
Iteration 259, loss = 1.44434488
Iteration 260, loss = 1.42697123
Iteration 261, loss = 1.41137767
Iteration 262, loss = 1.40743058
Iteration 263, loss = 1.39841648
Iteration 264, loss = 1.40295662
Iteration 265, loss = 1.40156626
Iteration 266, loss = 1.39592021
Iteration 267, loss = 1.38676585
Iteration 268, loss = 1.36551568
Iteration 269, loss = 1.35913647
Iteration 270, loss = 1.36364631
Iteration 271, loss = 1.35204156
Iteration 272, loss = 1.33567009
Iteration 273, loss = 1.33425609
Iteration 274, loss = 1.33062846
Iteration 275, loss = 1.31840871
Iteration 276, loss = 1.31172506
Iteration 277, loss = 1.31005391
Iteration 278, loss = 1.30522689
Iteration 279, loss = 1.29509748
Iteration 280, loss = 1.29019712
Iteration 281, loss = 1.28281879
Iteration 282, loss = 1.26916078
Iteration 283, loss = 1.27322406
Iteration 284, loss = 1.28143486
Iteration 285, loss = 1.25962094
Iteration 286, loss = 1.24276445
Iteration 287, loss = 1.23899378
Iteration 288, loss = 1.23071973
Iteration 289, loss = 1.22723155
Iteration 290, loss = 1.21951891
Iteration 291, loss = 1.22724631
Iteration 292, loss = 1.22182853
Iteration 293, loss = 1.21089825
Iteration 294, loss = 1.19434839
Iteration 295, loss = 1.18370352
Iteration 296, loss = 1.18493978
Iteration 297, loss = 1.17226014
Iteration 298, loss = 1.16585523
Iteration 299, loss = 1.16450274
Iteration 300, loss = 1.15759316
Iteration 301, loss = 1.16373164
Iteration 302, loss = 1.14884513
Iteration 303, loss = 1.15276468
Iteration 304, loss = 1.16475993
Iteration 305, loss = 1.13013248
Iteration 306, loss = 1.12666632
Iteration 307, loss = 1.12586916
Iteration 308, loss = 1.10961719
Iteration 309, loss = 1.10344513
Iteration 310, loss = 1.09746037
Iteration 311, loss = 1.08962562
Iteration 312, loss = 1.09731183
Iteration 313, loss = 1.08355167
Iteration 314, loss = 1.08128768
Iteration 315, loss = 1.07359064
Iteration 316, loss = 1.06566338
Iteration 317, loss = 1.06352898
Iteration 318, loss = 1.05392774
Iteration 319, loss = 1.04768653
Iteration 320, loss = 1.04789259
Iteration 321, loss = 1.04622478
Iteration 322, loss = 1.03352568
Iteration 323, loss = 1.02757060
Iteration 324, loss = 1.03731503
Iteration 325, loss = 1.02052433
Iteration 326, loss = 1.01386433
Iteration 327, loss = 1.01176013
Iteration 328, loss = 1.00646475
Iteration 329, loss = 1.00105448
Iteration 330, loss = 1.00291471
Iteration 331, loss = 0.99261285
Iteration 332, loss = 0.99032947
Iteration 333, loss = 0.99327254
Iteration 334, loss = 0.98687013
Iteration 335, loss = 1.00605952
Iteration 336, loss = 0.99961021
Iteration 337, loss = 0.96487886
Iteration 338, loss = 0.95550314
Iteration 339, loss = 0.94738512
Iteration 340, loss = 0.94615999
Iteration 341, loss = 0.93984009
Iteration 342, loss = 0.92765566
Iteration 343, loss = 0.93225469
Iteration 344, loss = 0.92228955
Iteration 345, loss = 0.91453302
Iteration 346, loss = 0.91411298
Iteration 347, loss = 0.91097063
Iteration 348, loss = 0.90508175
Iteration 349, loss = 0.89885758
Iteration 350, loss = 0.89612944
Iteration 351, loss = 0.89301568
Iteration 352, loss = 0.90016333
Iteration 353, loss = 0.89173820
Iteration 354, loss = 0.88297530
Iteration 355, loss = 0.87532746
Iteration 356, loss = 0.86611066
Iteration 357, loss = 0.86470944
Iteration 358, loss = 0.86696240
Iteration 359, loss = 0.86279661
Iteration 360, loss = 0.85965395
Iteration 361, loss = 0.84448092
Iteration 362, loss = 0.83759529
Iteration 363, loss = 0.83745515
Iteration 364, loss = 0.83410786
Iteration 365, loss = 0.82598946
Iteration 366, loss = 0.82312782
Iteration 367, loss = 0.82504137
Iteration 368, loss = 0.82048477
Iteration 369, loss = 0.82265255
Iteration 370, loss = 0.80653008
Iteration 371, loss = 0.81196299
Iteration 372, loss = 0.80242883
Iteration 373, loss = 0.80884540
Iteration 374, loss = 0.79747276
Iteration 375, loss = 0.79245284
Iteration 376, loss = 0.78418269
Iteration 377, loss = 0.77256816
Iteration 378, loss = 0.77089639
Iteration 379, loss = 0.77161811
Iteration 380, loss = 0.77153875
Iteration 381, loss = 0.76297863
Iteration 382, loss = 0.75324087
Iteration 383, loss = 0.75385599
Iteration 384, loss = 0.75674370
Iteration 385, loss = 0.74475542
Iteration 386, loss = 0.74377887
Iteration 387, loss = 0.73518843
Iteration 388, loss = 0.72872484
Iteration 389, loss = 0.72536440
Iteration 390, loss = 0.74173909
Iteration 391, loss = 0.74791538
Iteration 392, loss = 0.72760496
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100), {'error': 0.74555960659783149, 'fit': 0.88187499999999996, 'time': 871.7300000000005})
Iteration 1, loss = 6.18582918
Iteration 2, loss = 5.75988216
Iteration 3, loss = 5.56374274
Iteration 4, loss = 5.46034035
Iteration 5, loss = 5.38691541
Iteration 6, loss = 5.32708345
Iteration 7, loss = 5.27032467
Iteration 8, loss = 5.20267227
Iteration 9, loss = 5.14437682
Iteration 10, loss = 5.08145668
Iteration 11, loss = 5.03205777
Iteration 12, loss = 4.96450621
Iteration 13, loss = 4.91517195
Iteration 14, loss = 4.87835143
Iteration 15, loss = 4.82522532
Iteration 16, loss = 4.79020044
Iteration 17, loss = 4.75621652
Iteration 18, loss = 4.71498024
Iteration 19, loss = 4.68695861
Iteration 20, loss = 4.65326325
Iteration 21, loss = 4.62081263
Iteration 22, loss = 4.61806127
Iteration 23, loss = 4.57171010
Iteration 24, loss = 4.54260635
Iteration 25, loss = 4.52613168
Iteration 26, loss = 4.48906593
Iteration 27, loss = 4.46325089
Iteration 28, loss = 4.44074144
Iteration 29, loss = 4.42494636
Iteration 30, loss = 4.39154342
Iteration 31, loss = 4.37339959
Iteration 32, loss = 4.34609987
Iteration 33, loss = 4.31909789
Iteration 34, loss = 4.30057402
Iteration 35, loss = 4.27448084
Iteration 36, loss = 4.25853385
Iteration 37, loss = 4.23703354
Iteration 38, loss = 4.21234271
Iteration 39, loss = 4.18948549
Iteration 40, loss = 4.18022131
Iteration 41, loss = 4.16683741
Iteration 42, loss = 4.13551654
Iteration 43, loss = 4.11439795
Iteration 44, loss = 4.09419519
Iteration 45, loss = 4.05624651
Iteration 46, loss = 4.04921654
Iteration 47, loss = 4.02293792
Iteration 48, loss = 3.99658545
Iteration 49, loss = 3.98808005
Iteration 50, loss = 3.96768169
Iteration 51, loss = 3.95418595
Iteration 52, loss = 3.94284628
Iteration 53, loss = 3.90749902
Iteration 54, loss = 3.88787908
Iteration 55, loss = 3.88129081
Iteration 56, loss = 3.85971554
Iteration 57, loss = 3.82718800
Iteration 58, loss = 3.81072696
Iteration 59, loss = 3.78525047
Iteration 60, loss = 3.77863320
Iteration 61, loss = 3.74787498
Iteration 62, loss = 3.72805576
Iteration 63, loss = 3.72597189
Iteration 64, loss = 3.69635266
Iteration 65, loss = 3.67957844
Iteration 66, loss = 3.67069075
Iteration 67, loss = 3.65288570
Iteration 68, loss = 3.62706427
Iteration 69, loss = 3.59702044
Iteration 70, loss = 3.57983652
Iteration 71, loss = 3.56904792
Iteration 72, loss = 3.54567148
Iteration 73, loss = 3.52922085
Iteration 74, loss = 3.52356319
Iteration 75, loss = 3.50523165
Iteration 76, loss = 3.48261266
Iteration 77, loss = 3.47038579
Iteration 78, loss = 3.46780739
Iteration 79, loss = 3.43325514
Iteration 80, loss = 3.40582103
Iteration 81, loss = 3.40041091
Iteration 82, loss = 3.38573066
Iteration 83, loss = 3.34669920
Iteration 84, loss = 3.33084876
Iteration 85, loss = 3.31974884
Iteration 86, loss = 3.31081517
Iteration 87, loss = 3.29096601
Iteration 88, loss = 3.27025016
Iteration 89, loss = 3.25881755
Iteration 90, loss = 3.24739394
Iteration 91, loss = 3.22441247
Iteration 92, loss = 3.20803300
Iteration 93, loss = 3.19556691
Iteration 94, loss = 3.17681080
Iteration 95, loss = 3.15310717
Iteration 96, loss = 3.14042799
Iteration 97, loss = 3.11890722
Iteration 98, loss = 3.10420045
Iteration 99, loss = 3.10496162
Iteration 100, loss = 3.08627113
Iteration 101, loss = 3.06185223
Iteration 102, loss = 3.05189495
Iteration 103, loss = 3.03053432
Iteration 104, loss = 3.00944940
Iteration 105, loss = 2.99400683
Iteration 106, loss = 3.00210893
Iteration 107, loss = 2.97491887
Iteration 108, loss = 2.95575977
Iteration 109, loss = 2.93938685
Iteration 110, loss = 2.93339033
Iteration 111, loss = 2.91012273
Iteration 112, loss = 2.88997316
Iteration 113, loss = 2.87252287
Iteration 114, loss = 2.88088730
Iteration 115, loss = 2.84939874
Iteration 116, loss = 2.82031693
Iteration 117, loss = 2.81409724
Iteration 118, loss = 2.80325074
Iteration 119, loss = 2.79393064
Iteration 120, loss = 2.76974014
Iteration 121, loss = 2.75383648
Iteration 122, loss = 2.75445219
Iteration 123, loss = 2.72815492
Iteration 124, loss = 2.71070953
Iteration 125, loss = 2.70986799
Iteration 126, loss = 2.68145704
Iteration 127, loss = 2.65806070
Iteration 128, loss = 2.65503065
Iteration 129, loss = 2.64060648
Iteration 130, loss = 2.61792126
Iteration 131, loss = 2.61443149
Iteration 132, loss = 2.60144719
Iteration 133, loss = 2.58681539
Iteration 134, loss = 2.57324259
Iteration 135, loss = 2.55228253
Iteration 136, loss = 2.54402620
Iteration 137, loss = 2.53077350
Iteration 138, loss = 2.50919960
Iteration 139, loss = 2.50774231
Iteration 140, loss = 2.49128188
Iteration 141, loss = 2.47198727
Iteration 142, loss = 2.46361090
Iteration 143, loss = 2.44950493
Iteration 144, loss = 2.43704808
Iteration 145, loss = 2.42514826
Iteration 146, loss = 2.40199780
Iteration 147, loss = 2.39059691
Iteration 148, loss = 2.38736441
Iteration 149, loss = 2.36993127
Iteration 150, loss = 2.35322338
Iteration 151, loss = 2.33591300
Iteration 152, loss = 2.32879657
Iteration 153, loss = 2.31730300
Iteration 154, loss = 2.29410786
Iteration 155, loss = 2.27879873
Iteration 156, loss = 2.27929912
Iteration 157, loss = 2.26600117
Iteration 158, loss = 2.24739679
Iteration 159, loss = 2.24063860
Iteration 160, loss = 2.21928548
Iteration 161, loss = 2.20413927
Iteration 162, loss = 2.20031330
Iteration 163, loss = 2.19152155
Iteration 164, loss = 2.18030652
Iteration 165, loss = 2.15917121
Iteration 166, loss = 2.14912203
Iteration 167, loss = 2.14424215
Iteration 168, loss = 2.13014024
Iteration 169, loss = 2.11606291
Iteration 170, loss = 2.10991663
Iteration 171, loss = 2.09009242
Iteration 172, loss = 2.09209534
Iteration 173, loss = 2.05998466
Iteration 174, loss = 2.07113086
Iteration 175, loss = 2.04951422
Iteration 176, loss = 2.04369781
Iteration 177, loss = 2.02228633
Iteration 178, loss = 2.00695986
Iteration 179, loss = 1.99816522
Iteration 180, loss = 1.98695754
Iteration 181, loss = 1.98396103
Iteration 182, loss = 1.95885063
Iteration 183, loss = 1.95052243
Iteration 184, loss = 1.94015906
Iteration 185, loss = 1.92786829
Iteration 186, loss = 1.92160673
Iteration 187, loss = 1.91104485
Iteration 188, loss = 1.90290960
Iteration 189, loss = 1.89173234
Iteration 190, loss = 1.88777792
Iteration 191, loss = 1.85783352
Iteration 192, loss = 1.85321565
Iteration 193, loss = 1.83676564
Iteration 194, loss = 1.82648404
Iteration 195, loss = 1.82422032
Iteration 196, loss = 1.80585222
Iteration 197, loss = 1.82211929
Iteration 198, loss = 1.79109200
Iteration 199, loss = 1.78532183
Iteration 200, loss = 1.76208937
Iteration 201, loss = 1.76685028
Iteration 202, loss = 1.75289062
Iteration 203, loss = 1.75644905
Iteration 204, loss = 1.73201978
Iteration 205, loss = 1.72306122
Iteration 206, loss = 1.70428646
Iteration 207, loss = 1.69218313
Iteration 208, loss = 1.68641673
Iteration 209, loss = 1.68257291
Iteration 210, loss = 1.66766062
Iteration 211, loss = 1.65904643
Iteration 212, loss = 1.65744077
Iteration 213, loss = 1.64728464
Iteration 214, loss = 1.62039798
Iteration 215, loss = 1.61341797
Iteration 216, loss = 1.63077196
Iteration 217, loss = 1.62337123
Iteration 218, loss = 1.59262600
Iteration 219, loss = 1.57979049
Iteration 220, loss = 1.57246249
Iteration 221, loss = 1.57495230
Iteration 222, loss = 1.55265849
Iteration 223, loss = 1.57106882
Iteration 224, loss = 1.54803690
Iteration 225, loss = 1.52802696
Iteration 226, loss = 1.52499622
Iteration 227, loss = 1.50775351
Iteration 228, loss = 1.49459021
Iteration 229, loss = 1.50014430
Iteration 230, loss = 1.49907159
Iteration 231, loss = 1.48655277
Iteration 232, loss = 1.46530398
Iteration 233, loss = 1.45619162
Iteration 234, loss = 1.44103334
Iteration 235, loss = 1.44353974
Iteration 236, loss = 1.43086759
Iteration 237, loss = 1.42205886
Iteration 238, loss = 1.42635253
Iteration 239, loss = 1.40515110
Iteration 240, loss = 1.38514153
Iteration 241, loss = 1.38260440
Iteration 242, loss = 1.39113025
Iteration 243, loss = 1.37441005
Iteration 244, loss = 1.36376922
Iteration 245, loss = 1.36414373
Iteration 246, loss = 1.34304583
Iteration 247, loss = 1.34518295
Iteration 248, loss = 1.36107475
Iteration 249, loss = 1.33033949
Iteration 250, loss = 1.31433521
Iteration 251, loss = 1.31360948
Iteration 252, loss = 1.29175680
Iteration 253, loss = 1.27896298
Iteration 254, loss = 1.27678068
Iteration 255, loss = 1.28550429
Iteration 256, loss = 1.27456170
Iteration 257, loss = 1.26294688
Iteration 258, loss = 1.24755900
Iteration 259, loss = 1.24758624
Iteration 260, loss = 1.23778266
Iteration 261, loss = 1.22885792
Iteration 262, loss = 1.22039000
Iteration 263, loss = 1.20840372
Iteration 264, loss = 1.21563049
Iteration 265, loss = 1.19938995
Iteration 266, loss = 1.19623512
Iteration 267, loss = 1.18902118
Iteration 268, loss = 1.19042237
Iteration 269, loss = 1.17538347
Iteration 270, loss = 1.18215815
Iteration 271, loss = 1.15836049
Iteration 272, loss = 1.14665510
Iteration 273, loss = 1.14120175
Iteration 274, loss = 1.12964750
Iteration 275, loss = 1.13553802
Iteration 276, loss = 1.12776373
Iteration 277, loss = 1.10785269
Iteration 278, loss = 1.11432678
Iteration 279, loss = 1.10479402
Iteration 280, loss = 1.09112943
Iteration 281, loss = 1.08768018
Iteration 282, loss = 1.07942581
Iteration 283, loss = 1.07131359
Iteration 284, loss = 1.06579850
Iteration 285, loss = 1.05066306
Iteration 286, loss = 1.04092312
Iteration 287, loss = 1.04340266
Iteration 288, loss = 1.03606387
Iteration 289, loss = 1.03576000
Iteration 290, loss = 1.03660664
Iteration 291, loss = 1.02170109
Iteration 292, loss = 1.02405628
Iteration 293, loss = 1.02596310
Iteration 294, loss = 1.00757195
Iteration 295, loss = 0.99337216
Iteration 296, loss = 0.99472507
Iteration 297, loss = 0.98592231
Iteration 298, loss = 0.98684577
Iteration 299, loss = 0.98567505
Iteration 300, loss = 0.96980973
Iteration 301, loss = 0.95562124
Iteration 302, loss = 0.95505807
Iteration 303, loss = 0.94620669
Iteration 304, loss = 0.95407175
Iteration 305, loss = 0.93926166
Iteration 306, loss = 0.92982679
Iteration 307, loss = 0.92894181
Iteration 308, loss = 0.92600604
Iteration 309, loss = 0.91908103
Iteration 310, loss = 0.90930241
Iteration 311, loss = 0.90114313
Iteration 312, loss = 0.89212206
Iteration 313, loss = 0.91555056
Iteration 314, loss = 0.90063902
Iteration 315, loss = 0.87750249
Iteration 316, loss = 0.86877341
Iteration 317, loss = 0.87688150
Iteration 318, loss = 0.87647751
Iteration 319, loss = 0.87113887
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100), {'error': 0.75759795251776396, 'fit': 0.85187500000000005, 'time': 762.0699999999997})
Iteration 1, loss = 6.19029570
Iteration 2, loss = 5.72536894
Iteration 3, loss = 5.53644218
Iteration 4, loss = 5.46181636
Iteration 5, loss = 5.39578101
Iteration 6, loss = 5.33404857
Iteration 7, loss = 5.26842915
Iteration 8, loss = 5.20535380
Iteration 9, loss = 5.14397444
Iteration 10, loss = 5.08192410
Iteration 11, loss = 5.02164207
Iteration 12, loss = 4.96094260
Iteration 13, loss = 4.91470786
Iteration 14, loss = 4.88523695
Iteration 15, loss = 4.83469699
Iteration 16, loss = 4.78840925
Iteration 17, loss = 4.75977736
Iteration 18, loss = 4.72971557
Iteration 19, loss = 4.70823629
Iteration 20, loss = 4.68500059
Iteration 21, loss = 4.65647359
Iteration 22, loss = 4.63279440
Iteration 23, loss = 4.61077676
Iteration 24, loss = 4.58008539
Iteration 25, loss = 4.55801329
Iteration 26, loss = 4.52899515
Iteration 27, loss = 4.50467426
Iteration 28, loss = 4.49575485
Iteration 29, loss = 4.46084946
Iteration 30, loss = 4.44887934
Iteration 31, loss = 4.41612654
Iteration 32, loss = 4.39932597
Iteration 33, loss = 4.39192465
Iteration 34, loss = 4.36541113
Iteration 35, loss = 4.33980929
Iteration 36, loss = 4.33347589
Iteration 37, loss = 4.31478403
Iteration 38, loss = 4.30066097
Iteration 39, loss = 4.27847313
Iteration 40, loss = 4.24210441
Iteration 41, loss = 4.23092633
Iteration 42, loss = 4.20781736
Iteration 43, loss = 4.19315795
Iteration 44, loss = 4.16594162
Iteration 45, loss = 4.14189216
Iteration 46, loss = 4.13141086
Iteration 47, loss = 4.14138179
Iteration 48, loss = 4.09483414
Iteration 49, loss = 4.08032942
Iteration 50, loss = 4.05720090
Iteration 51, loss = 4.04276318
Iteration 52, loss = 4.01334741
Iteration 53, loss = 4.00893424
Iteration 54, loss = 3.99792624
Iteration 55, loss = 3.96927511
Iteration 56, loss = 3.94607491
Iteration 57, loss = 3.94715931
Iteration 58, loss = 3.93516464
Iteration 59, loss = 3.89147006
Iteration 60, loss = 3.87382393
Iteration 61, loss = 3.85840132
Iteration 62, loss = 3.83532469
Iteration 63, loss = 3.82397112
Iteration 64, loss = 3.80189433
Iteration 65, loss = 3.79099076
Iteration 66, loss = 3.78147698
Iteration 67, loss = 3.76992099
Iteration 68, loss = 3.74244679
Iteration 69, loss = 3.71825961
Iteration 70, loss = 3.69886731
Iteration 71, loss = 3.68579676
Iteration 72, loss = 3.66482847
Iteration 73, loss = 3.64170404
Iteration 74, loss = 3.64019965
Iteration 75, loss = 3.62464601
Iteration 76, loss = 3.58687621
Iteration 77, loss = 3.58621735
Iteration 78, loss = 3.57820381
Iteration 79, loss = 3.55529994
Iteration 80, loss = 3.54431198
Iteration 81, loss = 3.51924458
Iteration 82, loss = 3.50052093
Iteration 83, loss = 3.49098403
Iteration 84, loss = 3.46673945
Iteration 85, loss = 3.44351458
Iteration 86, loss = 3.43917382
Iteration 87, loss = 3.43340597
Iteration 88, loss = 3.39306054
Iteration 89, loss = 3.38360800
Iteration 90, loss = 3.38062965
Iteration 91, loss = 3.35286679
Iteration 92, loss = 3.34874068
Iteration 93, loss = 3.32139649
Iteration 94, loss = 3.30673190
Iteration 95, loss = 3.30730705
Iteration 96, loss = 3.28417578
Iteration 97, loss = 3.25622333
Iteration 98, loss = 3.24069902
Iteration 99, loss = 3.23292713
Iteration 100, loss = 3.23100698
Iteration 101, loss = 3.19739651
Iteration 102, loss = 3.18089473
Iteration 103, loss = 3.16193637
Iteration 104, loss = 3.14746762
Iteration 105, loss = 3.15489072
Iteration 106, loss = 3.12092616
Iteration 107, loss = 3.10170898
Iteration 108, loss = 3.07679917
Iteration 109, loss = 3.06684470
Iteration 110, loss = 3.04780679
Iteration 111, loss = 3.04266807
Iteration 112, loss = 3.02615493
Iteration 113, loss = 3.01531749
Iteration 114, loss = 2.99263180
Iteration 115, loss = 2.97681334
Iteration 116, loss = 2.96079009
Iteration 117, loss = 2.95704121
Iteration 118, loss = 2.94057297
Iteration 119, loss = 2.91668220
Iteration 120, loss = 2.91330406
Iteration 121, loss = 2.88729112
Iteration 122, loss = 2.88098551
Iteration 123, loss = 2.85943127
Iteration 124, loss = 2.85174532
Iteration 125, loss = 2.84890165
Iteration 126, loss = 2.80588292
Iteration 127, loss = 2.79846210
Iteration 128, loss = 2.78951403
Iteration 129, loss = 2.76818814
Iteration 130, loss = 2.75387921
Iteration 131, loss = 2.74012545
Iteration 132, loss = 2.72787531
Iteration 133, loss = 2.71584219
Iteration 134, loss = 2.69944854
Iteration 135, loss = 2.67784942
Iteration 136, loss = 2.66235271
Iteration 137, loss = 2.66221122
Iteration 138, loss = 2.65288522
Iteration 139, loss = 2.62648515
Iteration 140, loss = 2.61491150
Iteration 141, loss = 2.60513126
Iteration 142, loss = 2.58704255
Iteration 143, loss = 2.57225435
Iteration 144, loss = 2.57607251
Iteration 145, loss = 2.53638775
Iteration 146, loss = 2.52967192
Iteration 147, loss = 2.52963145
Iteration 148, loss = 2.52201579
Iteration 149, loss = 2.49461893
Iteration 150, loss = 2.47706145
Iteration 151, loss = 2.48593775
Iteration 152, loss = 2.44680503
Iteration 153, loss = 2.42383272
Iteration 154, loss = 2.41759270
Iteration 155, loss = 2.41877920
Iteration 156, loss = 2.38778929
Iteration 157, loss = 2.36880356
Iteration 158, loss = 2.37444260
Iteration 159, loss = 2.35196698
Iteration 160, loss = 2.34472199
Iteration 161, loss = 2.33965427
Iteration 162, loss = 2.30735103
Iteration 163, loss = 2.30727340
Iteration 164, loss = 2.27566195
Iteration 165, loss = 2.27962897
Iteration 166, loss = 2.26871105
Iteration 167, loss = 2.25174578
Iteration 168, loss = 2.22011886
Iteration 169, loss = 2.22509841
Iteration 170, loss = 2.19824179
Iteration 171, loss = 2.19321754
Iteration 172, loss = 2.17482833
Iteration 173, loss = 2.17808623
Iteration 174, loss = 2.14576261
Iteration 175, loss = 2.14258658
Iteration 176, loss = 2.13385658
Iteration 177, loss = 2.12455188
Iteration 178, loss = 2.11219853
Iteration 179, loss = 2.10080460
Iteration 180, loss = 2.09185142
Iteration 181, loss = 2.08126238
Iteration 182, loss = 2.04885290
Iteration 183, loss = 2.04121717
Iteration 184, loss = 2.03801029
Iteration 185, loss = 2.03872649
Iteration 186, loss = 2.01314378
Iteration 187, loss = 2.01250059
Iteration 188, loss = 2.00589721
Iteration 189, loss = 1.98277430
Iteration 190, loss = 1.96180242
Iteration 191, loss = 1.95062242
Iteration 192, loss = 1.93558626
Iteration 193, loss = 1.93217019
Iteration 194, loss = 1.92531818
Iteration 195, loss = 1.91624977
Iteration 196, loss = 1.91344809
Iteration 197, loss = 1.89408811
Iteration 198, loss = 1.88286163
Iteration 199, loss = 1.85640589
Iteration 200, loss = 1.85841578
Iteration 201, loss = 1.84653793
Iteration 202, loss = 1.82508524
Iteration 203, loss = 1.80037017
Iteration 204, loss = 1.79447603
Iteration 205, loss = 1.80544245
Iteration 206, loss = 1.78236200
Iteration 207, loss = 1.77215788
Iteration 208, loss = 1.78669438
Iteration 209, loss = 1.74548603
Iteration 210, loss = 1.73456555
Iteration 211, loss = 1.72849419
Iteration 212, loss = 1.69956891
Iteration 213, loss = 1.69362453
Iteration 214, loss = 1.69830745
Iteration 215, loss = 1.70861881
Iteration 216, loss = 1.67717285
Iteration 217, loss = 1.68070881
Iteration 218, loss = 1.67070145
Iteration 219, loss = 1.66423361
Iteration 220, loss = 1.66766708
Iteration 221, loss = 1.64628723
Iteration 222, loss = 1.61064865
Iteration 223, loss = 1.63309158
Iteration 224, loss = 1.60617573
Iteration 225, loss = 1.60342600
Iteration 226, loss = 1.56814557
Iteration 227, loss = 1.55845562
Iteration 228, loss = 1.53573774
Iteration 229, loss = 1.53739170
Iteration 230, loss = 1.54742129
Iteration 231, loss = 1.56273346
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100), {'error': 0.74013958411335012, 'fit': 0.67825000000000002, 'time': 587.9099999999999})
Iteration 1, loss = 6.17891706
Iteration 2, loss = 5.69446824
Iteration 3, loss = 5.52020721
Iteration 4, loss = 5.42882736
Iteration 5, loss = 5.37194263
Iteration 6, loss = 5.28847390
Iteration 7, loss = 5.22592468
Iteration 8, loss = 5.14098652
Iteration 9, loss = 5.07519973
Iteration 10, loss = 5.01911204
Iteration 11, loss = 4.97693788
Iteration 12, loss = 4.92336152
Iteration 13, loss = 4.89136548
Iteration 14, loss = 4.86395012
Iteration 15, loss = 4.82811608
Iteration 16, loss = 4.80374536
Iteration 17, loss = 4.77727927
Iteration 18, loss = 4.75970498
Iteration 19, loss = 4.74990972
Iteration 20, loss = 4.71816561
Iteration 21, loss = 4.69583755
Iteration 22, loss = 4.68818858
Iteration 23, loss = 4.65592589
Iteration 24, loss = 4.63121733
Iteration 25, loss = 4.61797834
Iteration 26, loss = 4.58776759
Iteration 27, loss = 4.57274308
Iteration 28, loss = 4.55245935
Iteration 29, loss = 4.53825990
Iteration 30, loss = 4.54790111
Iteration 31, loss = 4.49870480
Iteration 32, loss = 4.49207810
Iteration 33, loss = 4.48738135
Iteration 34, loss = 4.45438822
Iteration 35, loss = 4.43773047
Iteration 36, loss = 4.44811488
Iteration 37, loss = 4.41479875
Iteration 38, loss = 4.39929506
Iteration 39, loss = 4.36426651
Iteration 40, loss = 4.36453848
Iteration 41, loss = 4.36073048
Iteration 42, loss = 4.33280373
Iteration 43, loss = 4.31347681
Iteration 44, loss = 4.30066284
Iteration 45, loss = 4.28829869
Iteration 46, loss = 4.27877336
Iteration 47, loss = 4.26251624
Iteration 48, loss = 4.24582344
Iteration 49, loss = 4.22712909
Iteration 50, loss = 4.21221717
Iteration 51, loss = 4.18837811
Iteration 52, loss = 4.19132211
Iteration 53, loss = 4.16988077
Iteration 54, loss = 4.14959899
Iteration 55, loss = 4.13268910
Iteration 56, loss = 4.10898595
Iteration 57, loss = 4.09811624
Iteration 58, loss = 4.07983049
Iteration 59, loss = 4.07665498
Iteration 60, loss = 4.04849648
Iteration 61, loss = 4.04309463
Iteration 62, loss = 4.03433462
Iteration 63, loss = 4.02997157
Iteration 64, loss = 4.00545903
Iteration 65, loss = 3.99683423
Iteration 66, loss = 3.98178626
Iteration 67, loss = 3.97479531
Iteration 68, loss = 3.94765886
Iteration 69, loss = 3.93894887
Iteration 70, loss = 3.93344295
Iteration 71, loss = 3.89979810
Iteration 72, loss = 3.90648328
Iteration 73, loss = 3.89431489
Iteration 74, loss = 3.86180896
Iteration 75, loss = 3.85483153
Iteration 76, loss = 3.82613660
Iteration 77, loss = 3.82598079
Iteration 78, loss = 3.81825737
Iteration 79, loss = 3.79062209
Iteration 80, loss = 3.79153553
Iteration 81, loss = 3.76741358
Iteration 82, loss = 3.76226380
Iteration 83, loss = 3.77123411
Iteration 84, loss = 3.72501965
Iteration 85, loss = 3.70950984
Iteration 86, loss = 3.70504001
Iteration 87, loss = 3.69454289
Iteration 88, loss = 3.67165086
Iteration 89, loss = 3.66273015
Iteration 90, loss = 3.64789451
Iteration 91, loss = 3.65293463
Iteration 92, loss = 3.64778038
Iteration 93, loss = 3.62405914
Iteration 94, loss = 3.60913240
Iteration 95, loss = 3.59340219
Iteration 96, loss = 3.58710385
Iteration 97, loss = 3.57574177
Iteration 98, loss = 3.55781640
Iteration 99, loss = 3.53686874
Iteration 100, loss = 3.52342639
Iteration 101, loss = 3.51690735
Iteration 102, loss = 3.49033309
Iteration 103, loss = 3.48172290
Iteration 104, loss = 3.46757347
Iteration 105, loss = 3.45457684
Iteration 106, loss = 3.44081725
Iteration 107, loss = 3.43592339
Iteration 108, loss = 3.41599487
Iteration 109, loss = 3.41990653
Iteration 110, loss = 3.39330088
Iteration 111, loss = 3.37261398
Iteration 112, loss = 3.35468418
Iteration 113, loss = 3.34519177
Iteration 114, loss = 3.33819848
Iteration 115, loss = 3.33605223
Iteration 116, loss = 3.32185637
Iteration 117, loss = 3.31557657
Iteration 118, loss = 3.28599539
Iteration 119, loss = 3.29585059
Iteration 120, loss = 3.26821630
Iteration 121, loss = 3.26939349
Iteration 122, loss = 3.25639926
Iteration 123, loss = 3.22943310
Iteration 124, loss = 3.21214889
Iteration 125, loss = 3.20066366
Iteration 126, loss = 3.19632345
Iteration 127, loss = 3.18859823
Iteration 128, loss = 3.18920855
Iteration 129, loss = 3.15139947
Iteration 130, loss = 3.13800898
Iteration 131, loss = 3.15191671
Iteration 132, loss = 3.11521635
Iteration 133, loss = 3.11140788
Iteration 134, loss = 3.08908892
Iteration 135, loss = 3.10477518
Iteration 136, loss = 3.12636779
Iteration 137, loss = 3.07703065
Iteration 138, loss = 3.08718756
Iteration 139, loss = 3.06663199
Iteration 140, loss = 3.02334959
Iteration 141, loss = 3.00474219
Iteration 142, loss = 2.99873990
Iteration 143, loss = 2.97640999
Iteration 144, loss = 2.97507435
Iteration 145, loss = 2.95555200
Iteration 146, loss = 2.97675463
Iteration 147, loss = 2.93820417
Iteration 148, loss = 2.91873687
Iteration 149, loss = 2.92931336
Iteration 150, loss = 2.92768237
Iteration 151, loss = 2.89094845
Iteration 152, loss = 2.89226222
Iteration 153, loss = 2.90110696
Iteration 154, loss = 2.85779998
Iteration 155, loss = 2.86593899
Iteration 156, loss = 2.83351775
Iteration 157, loss = 2.83374955
Iteration 158, loss = 2.82465531
Iteration 159, loss = 2.80690113
Iteration 160, loss = 2.79658148
Iteration 161, loss = 2.77350661
Iteration 162, loss = 2.79646454
Iteration 163, loss = 2.76097901
Iteration 164, loss = 2.74170013
Iteration 165, loss = 2.73004228
Iteration 166, loss = 2.71801797
Iteration 167, loss = 2.73557587
Iteration 168, loss = 2.71703705
Iteration 169, loss = 2.69044733
Iteration 170, loss = 2.68910185
Iteration 171, loss = 2.68334940
Iteration 172, loss = 2.67280735
Iteration 173, loss = 2.64270456
Iteration 174, loss = 2.63783998
Iteration 175, loss = 2.62932890
Iteration 176, loss = 2.60976593
Iteration 177, loss = 2.59383800
Iteration 178, loss = 2.59012230
Iteration 179, loss = 2.58019743
Iteration 180, loss = 2.57203503
Iteration 181, loss = 2.59602226
Iteration 182, loss = 2.59156580
Iteration 183, loss = 2.57275350
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.73222104281851619, 'fit': 0.37774999999999997, 'time': 495.72999999999956})
Iteration 1, loss = 6.17142163
Iteration 2, loss = 5.67658578
Iteration 3, loss = 5.50875332
Iteration 4, loss = 5.43765062
Iteration 5, loss = 5.35689220
Iteration 6, loss = 5.29089819
Iteration 7, loss = 5.22625163
Iteration 8, loss = 5.14806714
Iteration 9, loss = 5.08065605
Iteration 10, loss = 5.03397996
Iteration 11, loss = 4.99086922
Iteration 12, loss = 4.97043410
Iteration 13, loss = 4.93666658
Iteration 14, loss = 4.90147122
Iteration 15, loss = 4.85991535
Iteration 16, loss = 4.83931029
Iteration 17, loss = 4.84437531
Iteration 18, loss = 4.82480872
Iteration 19, loss = 4.78791099
Iteration 20, loss = 4.76578214
Iteration 21, loss = 4.76085898
Iteration 22, loss = 4.72716984
Iteration 23, loss = 4.73183136
Iteration 24, loss = 4.70805665
Iteration 25, loss = 4.68429739
Iteration 26, loss = 4.66379558
Iteration 27, loss = 4.64668326
Iteration 28, loss = 4.62972329
Iteration 29, loss = 4.61407087
Iteration 30, loss = 4.61321687
Iteration 31, loss = 4.58410832
Iteration 32, loss = 4.57644321
Iteration 33, loss = 4.56096974
Iteration 34, loss = 4.54371652
Iteration 35, loss = 4.55265128
Iteration 36, loss = 4.51015648
Iteration 37, loss = 4.50577109
Iteration 38, loss = 4.49165778
Iteration 39, loss = 4.45317298
Iteration 40, loss = 4.46299830
Iteration 41, loss = 4.46289987
Iteration 42, loss = 4.43849130
Iteration 43, loss = 4.40371205
Iteration 44, loss = 4.40265162
Iteration 45, loss = 4.39126077
Iteration 46, loss = 4.36984266
Iteration 47, loss = 4.37118686
Iteration 48, loss = 4.33799182
Iteration 49, loss = 4.33652854
Iteration 50, loss = 4.32233751
Iteration 51, loss = 4.31167778
Iteration 52, loss = 4.31177808
Iteration 53, loss = 4.27577702
Iteration 54, loss = 4.28737795
Iteration 55, loss = 4.27729253
Iteration 56, loss = 4.24929591
Iteration 57, loss = 4.24078081
Iteration 58, loss = 4.24365380
Iteration 59, loss = 4.20463473
Iteration 60, loss = 4.20127500
Iteration 61, loss = 4.19555279
Iteration 62, loss = 4.19126248
Iteration 63, loss = 4.17018887
Iteration 64, loss = 4.15012623
Iteration 65, loss = 4.14888843
Iteration 66, loss = 4.13056770
Iteration 67, loss = 4.10654209
Iteration 68, loss = 4.12016747
Iteration 69, loss = 4.10247619
Iteration 70, loss = 4.07272976
Iteration 71, loss = 4.06580251
Iteration 72, loss = 4.06357549
Iteration 73, loss = 4.04825066
Iteration 74, loss = 4.02567441
Iteration 75, loss = 4.04275082
Iteration 76, loss = 4.04514614
Iteration 77, loss = 4.06884658
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.74240039314689277, 'fit': 0.091124999999999998, 'time': 220.6900000000005})
Iteration 1, loss = 6.18717183
Iteration 2, loss = 5.68745709
Iteration 3, loss = 5.51511219
Iteration 4, loss = 5.43045114
Iteration 5, loss = 5.34423535
Iteration 6, loss = 5.25838426
Iteration 7, loss = 5.17637915
Iteration 8, loss = 5.11630542
Iteration 9, loss = 5.06133226
Iteration 10, loss = 5.00291013
Iteration 11, loss = 4.96626141
Iteration 12, loss = 4.95923644
Iteration 13, loss = 4.90353624
Iteration 14, loss = 4.89368315
Iteration 15, loss = 4.84560986
Iteration 16, loss = 4.85126532
Iteration 17, loss = 4.83511467
Iteration 18, loss = 4.81099086
Iteration 19, loss = 4.76678354
Iteration 20, loss = 4.73740587
Iteration 21, loss = 4.73454458
Iteration 22, loss = 4.73318188
Iteration 23, loss = 4.68872798
Iteration 24, loss = 4.67780334
Iteration 25, loss = 4.68003329
Iteration 26, loss = 4.67049622
Iteration 27, loss = 4.62781839
Iteration 28, loss = 4.60143920
Iteration 29, loss = 4.62698830
Iteration 30, loss = 4.60165527
Iteration 31, loss = 4.57474633
Iteration 32, loss = 4.57777476
Iteration 33, loss = 4.56325678
Iteration 34, loss = 4.54336819
Iteration 35, loss = 4.50838128
Iteration 36, loss = 4.49042614
Iteration 37, loss = 4.49109278
Iteration 38, loss = 4.48248681
Iteration 39, loss = 4.47802371
Iteration 40, loss = 4.45899101
Iteration 41, loss = 4.42604969
Iteration 42, loss = 4.41881163
Iteration 43, loss = 4.44421897
Iteration 44, loss = 4.39673518
Iteration 45, loss = 4.38504622
Iteration 46, loss = 4.39787896
Iteration 47, loss = 4.37437413
Iteration 48, loss = 4.37664902
Iteration 49, loss = 4.34266483
Iteration 50, loss = 4.32162145
Iteration 51, loss = 4.33151427
Iteration 52, loss = 4.30004305
Iteration 53, loss = 4.27192573
Iteration 54, loss = 4.26937990
Iteration 55, loss = 4.26357475
Iteration 56, loss = 4.26584352
Iteration 57, loss = 4.24218926
Iteration 58, loss = 4.21729673
Iteration 59, loss = 4.24789790
Iteration 60, loss = 4.23503850
Iteration 61, loss = 4.19482821
Iteration 62, loss = 4.17576211
Iteration 63, loss = 4.16406766
Iteration 64, loss = 4.16115979
Iteration 65, loss = 4.16388021
Iteration 66, loss = 4.14214705
Iteration 67, loss = 4.11477229
Iteration 68, loss = 4.12459537
Iteration 69, loss = 4.11115599
Iteration 70, loss = 4.11931375
Iteration 71, loss = 4.08567458
Iteration 72, loss = 4.08069317
Iteration 73, loss = 4.05254223
Iteration 74, loss = 4.04060822
Iteration 75, loss = 4.04901587
Iteration 76, loss = 4.02327213
Iteration 77, loss = 4.00295725
Iteration 78, loss = 4.01883087
Iteration 79, loss = 3.99505218
Iteration 80, loss = 3.99090656
Iteration 81, loss = 3.96425469
Iteration 82, loss = 3.94986935
Iteration 83, loss = 3.93035889
Iteration 84, loss = 3.93393207
Iteration 85, loss = 3.92703341
Iteration 86, loss = 3.90276925
Iteration 87, loss = 3.92185597
Iteration 88, loss = 3.90030971
Iteration 89, loss = 3.87711091
Iteration 90, loss = 3.86716231
Iteration 91, loss = 3.91607076
Iteration 92, loss = 3.89040707
Iteration 93, loss = 3.85545700
Iteration 94, loss = 3.82531470
Iteration 95, loss = 3.82517704
Iteration 96, loss = 3.80862486
Iteration 97, loss = 3.82396426
Iteration 98, loss = 3.84887682
Iteration 99, loss = 3.80743439
Iteration 100, loss = 3.76403443
Iteration 101, loss = 3.75063522
Iteration 102, loss = 3.73672791
Iteration 103, loss = 3.71924436
Iteration 104, loss = 3.75508166
Iteration 105, loss = 3.72774578
Iteration 106, loss = 3.70623703
Iteration 107, loss = 3.68779480
Iteration 108, loss = 3.66640435
Iteration 109, loss = 3.67781969
Iteration 110, loss = 3.69566865
Iteration 111, loss = 3.67645450
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.70524627408304674, 'fit': 0.14124999999999999, 'time': 335.8299999999999})
Iteration 1, loss = 6.16848141
Iteration 2, loss = 5.71853925
Iteration 3, loss = 5.53959570
Iteration 4, loss = 5.44830542
Iteration 5, loss = 5.38413564
Iteration 6, loss = 5.28855533
Iteration 7, loss = 5.21756201
Iteration 8, loss = 5.16207232
Iteration 9, loss = 5.07753183
Iteration 10, loss = 5.03490034
Iteration 11, loss = 4.98959772
Iteration 12, loss = 4.94782564
Iteration 13, loss = 4.92941657
Iteration 14, loss = 4.90892986
Iteration 15, loss = 4.87801810
Iteration 16, loss = 4.85502871
Iteration 17, loss = 4.82997417
Iteration 18, loss = 4.80241722
Iteration 19, loss = 4.80216534
Iteration 20, loss = 4.75893172
Iteration 21, loss = 4.74469869
Iteration 22, loss = 4.74348355
Iteration 23, loss = 4.72835507
Iteration 24, loss = 4.68457885
Iteration 25, loss = 4.68650856
Iteration 26, loss = 4.66226100
Iteration 27, loss = 4.62596355
Iteration 28, loss = 4.62011351
Iteration 29, loss = 4.61737213
Iteration 30, loss = 4.58971415
Iteration 31, loss = 4.57346573
Iteration 32, loss = 4.54576043
Iteration 33, loss = 4.54707710
Iteration 34, loss = 4.52383241
Iteration 35, loss = 4.52784605
Iteration 36, loss = 4.48370837
Iteration 37, loss = 4.47002838
Iteration 38, loss = 4.45305334
Iteration 39, loss = 4.43995057
Iteration 40, loss = 4.45062509
Iteration 41, loss = 4.44518037
Iteration 42, loss = 4.43311436
Iteration 43, loss = 4.39637334
Iteration 44, loss = 4.37983508
Iteration 45, loss = 4.40219680
Iteration 46, loss = 4.34351431
Iteration 47, loss = 4.32931254
Iteration 48, loss = 4.32326707
Iteration 49, loss = 4.30280208
Iteration 50, loss = 4.29927958
Iteration 51, loss = 4.28601219
Iteration 52, loss = 4.25707423
Iteration 53, loss = 4.24774029
Iteration 54, loss = 4.23941054
Iteration 55, loss = 4.22282917
Iteration 56, loss = 4.23212447
Iteration 57, loss = 4.22653393
Iteration 58, loss = 4.18893780
Iteration 59, loss = 4.18523140
Iteration 60, loss = 4.17244117
Iteration 61, loss = 4.15459891
Iteration 62, loss = 4.13068822
Iteration 63, loss = 4.12673444
Iteration 64, loss = 4.10631687
Iteration 65, loss = 4.08385039
Iteration 66, loss = 4.08161821
Iteration 67, loss = 4.11417906
Iteration 68, loss = 4.06527433
Iteration 69, loss = 4.05181074
Iteration 70, loss = 4.04622754
Iteration 71, loss = 4.04171182
Iteration 72, loss = 4.05486359
Iteration 73, loss = 3.99919460
Iteration 74, loss = 3.97581482
Iteration 75, loss = 3.96653394
Iteration 76, loss = 3.95831487
Iteration 77, loss = 3.98253177
Iteration 78, loss = 3.92951945
Iteration 79, loss = 3.89861832
Iteration 80, loss = 3.90234301
Iteration 81, loss = 3.90070197
Iteration 82, loss = 3.88440379
Iteration 83, loss = 3.87429788
Iteration 84, loss = 3.87681801
Iteration 85, loss = 3.84949403
Iteration 86, loss = 3.83676867
Iteration 87, loss = 3.83234191
Iteration 88, loss = 3.82417211
Iteration 89, loss = 3.77433345
Iteration 90, loss = 3.79245978
Iteration 91, loss = 3.76088179
Iteration 92, loss = 3.74985863
Iteration 93, loss = 3.73588862
Iteration 94, loss = 3.74172159
Iteration 95, loss = 3.72382682
Iteration 96, loss = 3.70117585
Iteration 97, loss = 3.68455852
Iteration 98, loss = 3.68245932
Iteration 99, loss = 3.68396543
Iteration 100, loss = 3.67872433
Iteration 101, loss = 3.67001118
Iteration 102, loss = 3.64791168
Iteration 103, loss = 3.61911173
Iteration 104, loss = 3.61301740
Iteration 105, loss = 3.60373654
Iteration 106, loss = 3.58170703
Iteration 107, loss = 3.56227252
Iteration 108, loss = 3.56241171
Iteration 109, loss = 3.57106003
Iteration 110, loss = 3.56072975
Iteration 111, loss = 3.51313422
Iteration 112, loss = 3.50492456
Iteration 113, loss = 3.50670541
Iteration 114, loss = 3.52443108
Iteration 115, loss = 3.50051846
Iteration 116, loss = 3.48562871
Iteration 117, loss = 3.46984374
Iteration 118, loss = 3.45004526
Iteration 119, loss = 3.43038364
Iteration 120, loss = 3.43902159
Iteration 121, loss = 3.44946266
Iteration 122, loss = 3.38959843
Iteration 123, loss = 3.41705772
Iteration 124, loss = 3.38154888
Iteration 125, loss = 3.36781624
Iteration 126, loss = 3.35535449
Iteration 127, loss = 3.33615332
Iteration 128, loss = 3.33093609
Iteration 129, loss = 3.31294136
Iteration 130, loss = 3.30533712
Iteration 131, loss = 3.29824340
Iteration 132, loss = 3.27824308
Iteration 133, loss = 3.25430655
Iteration 134, loss = 3.25503298
Iteration 135, loss = 3.29796532
Iteration 136, loss = 3.23033914
Iteration 137, loss = 3.21489187
Iteration 138, loss = 3.20214325
Iteration 139, loss = 3.22314994
Iteration 140, loss = 3.22880676
Iteration 141, loss = 3.18417913
Iteration 142, loss = 3.15370515
Iteration 143, loss = 3.14702955
Iteration 144, loss = 3.14656391
Iteration 145, loss = 3.13699242
Iteration 146, loss = 3.12954493
Iteration 147, loss = 3.11387846
Iteration 148, loss = 3.09463404
Iteration 149, loss = 3.10466557
Iteration 150, loss = 3.07116565
Iteration 151, loss = 3.07294941
Iteration 152, loss = 3.06982397
Iteration 153, loss = 3.03827539
Iteration 154, loss = 3.04964384
Iteration 155, loss = 3.06507568
Iteration 156, loss = 3.03914915
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.6906037683518319, 'fit': 0.28499999999999998, 'time': 495.27000000000044})
Iteration 1, loss = 6.15408347
Iteration 2, loss = 5.66434764
Iteration 3, loss = 5.51047054
Iteration 4, loss = 5.40393789
Iteration 5, loss = 5.31288881
Iteration 6, loss = 5.24403857
Iteration 7, loss = 5.15899914
Iteration 8, loss = 5.09166531
Iteration 9, loss = 5.03770815
Iteration 10, loss = 5.01528081
Iteration 11, loss = 4.96328248
Iteration 12, loss = 4.97206372
Iteration 13, loss = 4.92208747
Iteration 14, loss = 4.88138273
Iteration 15, loss = 4.87249208
Iteration 16, loss = 4.83049363
Iteration 17, loss = 4.81567711
Iteration 18, loss = 4.80096314
Iteration 19, loss = 4.79884316
Iteration 20, loss = 4.76608144
Iteration 21, loss = 4.74607455
Iteration 22, loss = 4.74900118
Iteration 23, loss = 4.72076682
Iteration 24, loss = 4.70092575
Iteration 25, loss = 4.71100854
Iteration 26, loss = 4.72316579
Iteration 27, loss = 4.67537278
Iteration 28, loss = 4.64951444
Iteration 29, loss = 4.62647763
Iteration 30, loss = 4.63566310
Iteration 31, loss = 4.60992542
Iteration 32, loss = 4.59893191
Iteration 33, loss = 4.56408658
Iteration 34, loss = 4.58012210
Iteration 35, loss = 4.56019496
Iteration 36, loss = 4.56270029
Iteration 37, loss = 4.52561766
Iteration 38, loss = 4.50777304
Iteration 39, loss = 4.50722797
Iteration 40, loss = 4.49996802
Iteration 41, loss = 4.47973282
Iteration 42, loss = 4.48007721
Iteration 43, loss = 4.44967456
Iteration 44, loss = 4.43567502
Iteration 45, loss = 4.43750743
Iteration 46, loss = 4.44328389
Iteration 47, loss = 4.41818688
Iteration 48, loss = 4.42489665
Iteration 49, loss = 4.40487038
Iteration 50, loss = 4.39311878
Iteration 51, loss = 4.39147774
Iteration 52, loss = 4.35095276
Iteration 53, loss = 4.38795066
Iteration 54, loss = 4.35066391
Iteration 55, loss = 4.31989223
Iteration 56, loss = 4.31891924
Iteration 57, loss = 4.29479355
Iteration 58, loss = 4.31758080
Iteration 59, loss = 4.28061104
Iteration 60, loss = 4.27614445
Iteration 61, loss = 4.28408025
Iteration 62, loss = 4.25901061
Iteration 63, loss = 4.23743270
Iteration 64, loss = 4.24964631
Iteration 65, loss = 4.21574621
Iteration 66, loss = 4.22342230
Iteration 67, loss = 4.21000977
Iteration 68, loss = 4.20249967
Iteration 69, loss = 4.19162703
Iteration 70, loss = 4.17932215
Iteration 71, loss = 4.17112556
Iteration 72, loss = 4.13513017
Iteration 73, loss = 4.16726634
Iteration 74, loss = 4.14120954
Iteration 75, loss = 4.11771832
Iteration 76, loss = 4.12744265
Iteration 77, loss = 4.11242634
Iteration 78, loss = 4.09624902
Iteration 79, loss = 4.10029362
Iteration 80, loss = 4.13172165
Iteration 81, loss = 4.07637199
Iteration 82, loss = 4.06980189
Iteration 83, loss = 4.04524088
Iteration 84, loss = 4.04557046
Iteration 85, loss = 4.05244766
Iteration 86, loss = 4.02805421
Iteration 87, loss = 4.02284037
Iteration 88, loss = 4.01227630
Iteration 89, loss = 4.00146260
Iteration 90, loss = 3.96417526
Iteration 91, loss = 3.99219939
Iteration 92, loss = 3.97973689
Iteration 93, loss = 3.94771049
Iteration 94, loss = 3.93551416
Iteration 95, loss = 3.93776685
Iteration 96, loss = 3.95065038
Iteration 97, loss = 3.93847712
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.70885742340172242, 'fit': 0.105875, 'time': 322.9400000000005})
Iteration 1, loss = 6.19087897
Iteration 2, loss = 5.73778445
Iteration 3, loss = 5.55985495
Iteration 4, loss = 5.46701152
Iteration 5, loss = 5.38691377
Iteration 6, loss = 5.30918078
Iteration 7, loss = 5.22881061
Iteration 8, loss = 5.18494427
Iteration 9, loss = 5.13547494
Iteration 10, loss = 5.06637589
Iteration 11, loss = 5.02449288
Iteration 12, loss = 4.98795117
Iteration 13, loss = 4.95919101
Iteration 14, loss = 4.92958309
Iteration 15, loss = 4.91868225
Iteration 16, loss = 4.90357538
Iteration 17, loss = 4.88963514
Iteration 18, loss = 4.85116621
Iteration 19, loss = 4.82395806
Iteration 20, loss = 4.81013559
Iteration 21, loss = 4.78832227
Iteration 22, loss = 4.78526813
Iteration 23, loss = 4.75368241
Iteration 24, loss = 4.74707689
Iteration 25, loss = 4.75198331
Iteration 26, loss = 4.73820006
Iteration 27, loss = 4.71758743
Iteration 28, loss = 4.67779991
Iteration 29, loss = 4.66601050
Iteration 30, loss = 4.67237711
Iteration 31, loss = 4.65654777
Iteration 32, loss = 4.70428148
Iteration 33, loss = 4.62945593
Iteration 34, loss = 4.61012108
Iteration 35, loss = 4.59160810
Iteration 36, loss = 4.58480852
Iteration 37, loss = 4.58153157
Iteration 38, loss = 4.57707870
Iteration 39, loss = 4.56621104
Iteration 40, loss = 4.54613823
Iteration 41, loss = 4.55827853
Iteration 42, loss = 4.61427624
Iteration 43, loss = 4.52837887
Iteration 44, loss = 4.50936089
Iteration 45, loss = 4.49727105
Iteration 46, loss = 4.48240429
Iteration 47, loss = 4.47336515
Iteration 48, loss = 4.49287198
Iteration 49, loss = 4.48756057
Iteration 50, loss = 4.45060309
Iteration 51, loss = 4.43020771
Iteration 52, loss = 4.43037741
Iteration 53, loss = 4.40211007
Iteration 54, loss = 4.43566324
Iteration 55, loss = 4.39419964
Iteration 56, loss = 4.36833092
Iteration 57, loss = 4.37429945
Iteration 58, loss = 4.37454319
Iteration 59, loss = 4.34990849
Iteration 60, loss = 4.34846116
Iteration 61, loss = 4.34713641
Iteration 62, loss = 4.35368889
Iteration 63, loss = 4.32211945
Iteration 64, loss = 4.29896177
Iteration 65, loss = 4.30102455
Iteration 66, loss = 4.30936894
Iteration 67, loss = 4.27433287
Iteration 68, loss = 4.29724754
Iteration 69, loss = 4.27662966
Iteration 70, loss = 4.25471257
Iteration 71, loss = 4.22504449
Iteration 72, loss = 4.23816762
Iteration 73, loss = 4.21359738
Iteration 74, loss = 4.21899517
Iteration 75, loss = 4.20390649
Iteration 76, loss = 4.20432512
Iteration 77, loss = 4.19590860
Iteration 78, loss = 4.18620499
Iteration 79, loss = 4.17937158
Iteration 80, loss = 4.20464823
Iteration 81, loss = 4.18038316
Iteration 82, loss = 4.14924110
Iteration 83, loss = 4.13449712
Iteration 84, loss = 4.13170189
Iteration 85, loss = 4.10924215
Iteration 86, loss = 4.10812813
Iteration 87, loss = 4.08256238
Iteration 88, loss = 4.09424227
Iteration 89, loss = 4.08179803
Iteration 90, loss = 4.06977381
Iteration 91, loss = 4.06963630
Iteration 92, loss = 4.06296927
Iteration 93, loss = 4.04257982
Iteration 94, loss = 4.02440156
Iteration 95, loss = 4.03120041
Iteration 96, loss = 4.04706231
Iteration 97, loss = 4.01593213
Iteration 98, loss = 3.99192562
Iteration 99, loss = 4.00051217
Iteration 100, loss = 3.96475643
Iteration 101, loss = 3.99776094
Iteration 102, loss = 3.96800070
Iteration 103, loss = 3.95729152
Iteration 104, loss = 3.97542441
Iteration 105, loss = 3.96502309
Iteration 106, loss = 3.96532939
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.73830196651088997, 'fit': 0.088499999999999995, 'time': 371.85000000000036})
Iteration 1, loss = 6.15149195
Iteration 2, loss = 5.66295351
Iteration 3, loss = 5.52680131
Iteration 4, loss = 5.43550368
Iteration 5, loss = 5.35242484
Iteration 6, loss = 5.25465833
Iteration 7, loss = 5.18769992
Iteration 8, loss = 5.12338973
Iteration 9, loss = 5.05779802
Iteration 10, loss = 5.02260236
Iteration 11, loss = 5.00055505
Iteration 12, loss = 4.97387667
Iteration 13, loss = 4.97350680
Iteration 14, loss = 4.93851732
Iteration 15, loss = 4.91320348
Iteration 16, loss = 4.87851093
Iteration 17, loss = 4.87258118
Iteration 18, loss = 4.86965728
Iteration 19, loss = 4.84206664
Iteration 20, loss = 4.85735020
Iteration 21, loss = 4.82004146
Iteration 22, loss = 4.81352294
Iteration 23, loss = 4.80732032
Iteration 24, loss = 4.79270112
Iteration 25, loss = 4.78781125
Iteration 26, loss = 4.75245306
Iteration 27, loss = 4.73634704
Iteration 28, loss = 4.73831598
Iteration 29, loss = 4.71007295
Iteration 30, loss = 4.70328683
Iteration 31, loss = 4.67877509
Iteration 32, loss = 4.69696239
Iteration 33, loss = 4.67889740
Iteration 34, loss = 4.66980762
Iteration 35, loss = 4.65217229
Iteration 36, loss = 4.63185845
Iteration 37, loss = 4.65404255
Iteration 38, loss = 4.63427111
Iteration 39, loss = 4.62563865
Iteration 40, loss = 4.60631265
Iteration 41, loss = 4.58559389
Iteration 42, loss = 4.58557367
Iteration 43, loss = 4.57573975
Iteration 44, loss = 4.56285131
Iteration 45, loss = 4.57709243
Iteration 46, loss = 4.57423130
Iteration 47, loss = 4.54903227
Iteration 48, loss = 4.54039796
Iteration 49, loss = 4.52409587
Iteration 50, loss = 4.49928790
Iteration 51, loss = 4.52043107
Iteration 52, loss = 4.56493419
Iteration 53, loss = 4.52581014
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.73772389951512851, 'fit': 0.050999999999999997, 'time': 193.0})
Iteration 1, loss = 6.17279893
Iteration 2, loss = 5.66835258
Iteration 3, loss = 5.51833497
Iteration 4, loss = 5.43264580
Iteration 5, loss = 5.36241506
Iteration 6, loss = 5.29113799
Iteration 7, loss = 5.20539914
Iteration 8, loss = 5.19761573
Iteration 9, loss = 5.09125734
Iteration 10, loss = 5.05378867
Iteration 11, loss = 5.03169098
Iteration 12, loss = 4.99594828
Iteration 13, loss = 4.97636714
Iteration 14, loss = 4.95803451
Iteration 15, loss = 4.94119372
Iteration 16, loss = 4.91442303
Iteration 17, loss = 4.87401295
Iteration 18, loss = 4.88304538
Iteration 19, loss = 4.87607735
Iteration 20, loss = 4.85382999
Iteration 21, loss = 4.85839290
Iteration 22, loss = 4.83679415
Iteration 23, loss = 4.83091008
Iteration 24, loss = 4.80565097
Iteration 25, loss = 4.80641598
Iteration 26, loss = 4.79556840
Iteration 27, loss = 4.76975162
Iteration 28, loss = 4.76013412
Iteration 29, loss = 4.76117331
Iteration 30, loss = 4.74628403
Iteration 31, loss = 4.73105016
Iteration 32, loss = 4.71816018
Iteration 33, loss = 4.70604555
Iteration 34, loss = 4.69645303
Iteration 35, loss = 4.68197049
Iteration 36, loss = 4.66492253
Iteration 37, loss = 4.65891159
Iteration 38, loss = 4.64636627
Iteration 39, loss = 4.62836085
Iteration 40, loss = 4.71919559
Iteration 41, loss = 4.64571194
Iteration 42, loss = 4.61655258
Iteration 43, loss = 4.65037442
Iteration 44, loss = 4.60579942
Iteration 45, loss = 4.57421441
Iteration 46, loss = 4.58480881
Iteration 47, loss = 4.59890556
Iteration 48, loss = 4.56324231
Iteration 49, loss = 4.55438817
Iteration 50, loss = 4.53026657
Iteration 51, loss = 4.50825502
Iteration 52, loss = 4.51949187
Iteration 53, loss = 4.53099495
Iteration 54, loss = 4.51809649
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.72780587513070072, 'fit': 0.055375000000000001, 'time': 204.8199999999997})
Iteration 1, loss = 6.18095889
Iteration 2, loss = 5.67083605
Iteration 3, loss = 5.51691195
Iteration 4, loss = 5.43605620
Iteration 5, loss = 5.37165450
Iteration 6, loss = 5.31114063
Iteration 7, loss = 5.22000717
Iteration 8, loss = 5.13513835
Iteration 9, loss = 5.09039652
Iteration 10, loss = 5.06513731
Iteration 11, loss = 5.03530984
Iteration 12, loss = 4.98301632
Iteration 13, loss = 4.96603123
Iteration 14, loss = 4.92457745
Iteration 15, loss = 4.90052895
Iteration 16, loss = 4.91386847
Iteration 17, loss = 4.91691968
Iteration 18, loss = 4.90464385
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.80062706147341012, 'fit': 0.041875000000000002, 'time': 70.64999999999964})
Iteration 1, loss = 6.16175230
Iteration 2, loss = 5.67260800
Iteration 3, loss = 5.52700312
Iteration 4, loss = 5.45310688
Iteration 5, loss = 5.34900079
Iteration 6, loss = 5.26239686
Iteration 7, loss = 5.20554421
Iteration 8, loss = 5.15422080
Iteration 9, loss = 5.11735004
Iteration 10, loss = 5.08701991
Iteration 11, loss = 5.03391653
Iteration 12, loss = 5.00675959
Iteration 13, loss = 5.01429676
Iteration 14, loss = 4.97382120
Iteration 15, loss = 4.95065862
Iteration 16, loss = 4.92329432
Iteration 17, loss = 4.90719930
Iteration 18, loss = 4.88798573
Iteration 19, loss = 4.86504475
Iteration 20, loss = 4.90515716
Iteration 21, loss = 4.90654695
Iteration 22, loss = 4.86605749
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.78615954472215122, 'fit': 0.041000000000000002, 'time': 89.95000000000073})
Iteration 1, loss = 6.16023258
Iteration 2, loss = 5.68184172
Iteration 3, loss = 5.53333722
Iteration 4, loss = 5.42432974
Iteration 5, loss = 5.35756927
Iteration 6, loss = 5.28975217
Iteration 7, loss = 5.21978950
Iteration 8, loss = 5.17254005
Iteration 9, loss = 5.11530545
Iteration 10, loss = 5.07599566
Iteration 11, loss = 5.04689191
Iteration 12, loss = 5.00632662
Iteration 13, loss = 4.95792087
Iteration 14, loss = 4.93655438
Iteration 15, loss = 4.90979718
Iteration 16, loss = 4.95553257
Iteration 17, loss = 4.88488712
Iteration 18, loss = 4.87036748
Iteration 19, loss = 4.88598684
Iteration 20, loss = 4.88851716
Iteration 21, loss = 4.83530346
Iteration 22, loss = 4.81457883
Iteration 23, loss = 4.78730765
Iteration 24, loss = 4.82982434
Iteration 25, loss = 4.81450995
Iteration 26, loss = 4.74214582
Iteration 27, loss = 4.74058122
Iteration 28, loss = 4.75443159
Iteration 29, loss = 4.73452419
Iteration 30, loss = 4.72978842
Iteration 31, loss = 4.73140724
Iteration 32, loss = 4.68054751
Iteration 33, loss = 4.70869394
Iteration 34, loss = 4.69279107
Iteration 35, loss = 4.66182505
Iteration 36, loss = 4.68070202
Iteration 37, loss = 4.70258400
Iteration 38, loss = 4.62642070
Iteration 39, loss = 4.61455154
Iteration 40, loss = 4.63156018
Iteration 41, loss = 4.62213527
Iteration 42, loss = 4.59362457
Iteration 43, loss = 4.56550596
Iteration 44, loss = 4.59027897
Iteration 45, loss = 4.60298888
Iteration 46, loss = 4.61836890
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.74423622883769458, 'fit': 0.049250000000000002, 'time': 196.51000000000022})
Iteration 1, loss = 6.15237330
Iteration 2, loss = 5.66102783
Iteration 3, loss = 5.55894594
Iteration 4, loss = 5.47215086
Iteration 5, loss = 5.44735332
Iteration 6, loss = 5.36624949
Iteration 7, loss = 5.29949735
Iteration 8, loss = 5.23802343
Iteration 9, loss = 5.22306120
Iteration 10, loss = 5.14160194
Iteration 11, loss = 5.09942878
Iteration 12, loss = 5.10332992
Iteration 13, loss = 5.03442274
Iteration 14, loss = 5.03250869
Iteration 15, loss = 4.96024321
Iteration 16, loss = 4.98471881
Iteration 17, loss = 4.92680554
Iteration 18, loss = 4.91971640
Iteration 19, loss = 4.92001324
Iteration 20, loss = 4.89546832
Iteration 21, loss = 4.91098626
Iteration 22, loss = 4.91667212
Iteration 23, loss = 4.86659190
Iteration 24, loss = 4.83782284
Iteration 25, loss = 4.83062637
Iteration 26, loss = 4.85130409
Iteration 27, loss = 4.81917496
Iteration 28, loss = 4.78910219
Iteration 29, loss = 4.83623524
Iteration 30, loss = 4.81328608
Iteration 31, loss = 4.80474368
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.72117331303404331, 'fit': 0.035374999999999997, 'time': 136.36999999999898})
Iteration 1, loss = 6.19097130
Iteration 2, loss = 5.76077222
Iteration 3, loss = 5.57581396
Iteration 4, loss = 5.47347571
Iteration 5, loss = 5.40381936
Iteration 6, loss = 5.34224955
Iteration 7, loss = 5.30833823
Iteration 8, loss = 5.27297835
Iteration 9, loss = 5.21408903
Iteration 10, loss = 5.21174620
Iteration 11, loss = 5.17473253
Iteration 12, loss = 5.11701598
Iteration 13, loss = 5.09895282
Iteration 14, loss = 5.13097272
Iteration 15, loss = 5.08989481
Iteration 16, loss = 5.04502507
Iteration 17, loss = 5.06055815
Iteration 18, loss = 5.04823286
Iteration 19, loss = 5.02188041
Iteration 20, loss = 4.97145485
Iteration 21, loss = 4.96331333
Iteration 22, loss = 4.95104653
Iteration 23, loss = 4.91409139
Iteration 24, loss = 4.86916692
Iteration 25, loss = 4.87576021
Iteration 26, loss = 4.85229048
Iteration 27, loss = 4.85429785
Iteration 28, loss = 4.82827594
Iteration 29, loss = 4.77045560
Iteration 30, loss = 4.78275258
Iteration 31, loss = 4.78410828
Iteration 32, loss = 4.74865171
Iteration 33, loss = 4.71396032
Iteration 34, loss = 4.72184817
Iteration 35, loss = 4.67756801
Iteration 36, loss = 4.68200760
Iteration 37, loss = 4.73969916
Iteration 38, loss = 4.69038032
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.79296624968661389, 'fit': 0.040000000000000001, 'time': 173.3700000000008})
Iteration 1, loss = 6.15850400
Iteration 2, loss = 5.69017712
Iteration 3, loss = 5.52539901
Iteration 4, loss = 5.43400698
Iteration 5, loss = 5.36895611
Iteration 6, loss = 5.31627102
Iteration 7, loss = 5.28108240
Iteration 8, loss = 5.21534139
Iteration 9, loss = 5.19343942
Iteration 10, loss = 5.14455002
Iteration 11, loss = 5.09767749
Iteration 12, loss = 5.07513480
Iteration 13, loss = 5.02480970
Iteration 14, loss = 4.99328514
Iteration 15, loss = 5.00762608
Iteration 16, loss = 4.99860895
Iteration 17, loss = 4.97355963
Iteration 18, loss = 4.94170847
Iteration 19, loss = 4.92801452
Iteration 20, loss = 4.91974246
Iteration 21, loss = 4.88999622
Iteration 22, loss = 4.87641082
Iteration 23, loss = 4.86899553
Iteration 24, loss = 4.83249365
Iteration 25, loss = 4.86697229
Iteration 26, loss = 4.86019259
Iteration 27, loss = 4.87357398
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.74017758280303159, 'fit': 0.03875, 'time': 126.89000000000124})
Iteration 1, loss = 6.18720764
Iteration 2, loss = 5.74169198
Iteration 3, loss = 5.63392018
Iteration 4, loss = 5.56411877
Iteration 5, loss = 5.50172966
Iteration 6, loss = 5.41013043
Iteration 7, loss = 5.33675860
Iteration 8, loss = 5.30564562
Iteration 9, loss = 5.23851526
Iteration 10, loss = 5.16062484
Iteration 11, loss = 5.14991602
Iteration 12, loss = 5.10178113
Iteration 13, loss = 5.04840271
Iteration 14, loss = 5.02437818
Iteration 15, loss = 5.04383652
Iteration 16, loss = 4.99743642
Iteration 17, loss = 5.00870300
Iteration 18, loss = 4.97454261
Iteration 19, loss = 4.96037836
Iteration 20, loss = 4.97820354
Iteration 21, loss = 4.97652078
Iteration 22, loss = 4.93138172
Iteration 23, loss = 4.95820450
Iteration 24, loss = 4.92377551
Iteration 25, loss = 4.87534676
Iteration 26, loss = 4.87969729
Iteration 27, loss = 4.87802612
Iteration 28, loss = 4.93210992
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.60013272932747641, 'fit': 0.034375000000000003, 'time': 136.04000000000087})
Iteration 1, loss = 6.18506217
Iteration 2, loss = 5.72655832
Iteration 3, loss = 5.56987645
Iteration 4, loss = 5.47673874
Iteration 5, loss = 5.43134279
Iteration 6, loss = 5.38440195
Iteration 7, loss = 5.33650242
Iteration 8, loss = 5.28086574
Iteration 9, loss = 5.23994800
Iteration 10, loss = 5.21897089
Iteration 11, loss = 5.17862059
Iteration 12, loss = 5.16170543
Iteration 13, loss = 5.13941347
Iteration 14, loss = 5.13204252
Iteration 15, loss = 5.09197928
Iteration 16, loss = 5.06346341
Iteration 17, loss = 5.00041559
Iteration 18, loss = 5.00911881
Iteration 19, loss = 4.97008785
Iteration 20, loss = 4.97350664
Iteration 21, loss = 4.95878505
Iteration 22, loss = 4.95415902
Iteration 23, loss = 4.92016500
Iteration 24, loss = 4.91655814
Iteration 25, loss = 4.91517790
Iteration 26, loss = 4.87110922
Iteration 27, loss = 4.90617557
Iteration 28, loss = 4.87720370
Iteration 29, loss = 4.84898984
Iteration 30, loss = 4.85698432
Iteration 31, loss = 4.84711965
Iteration 32, loss = 4.84625301
Iteration 33, loss = 4.81797526
Iteration 34, loss = 4.87654486
Iteration 35, loss = 4.80705815
Iteration 36, loss = 4.81278562
Iteration 37, loss = 4.81280532
Iteration 38, loss = 4.80714901
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.69295600987191341, 'fit': 0.038374999999999999, 'time': 190.6900000000005})
Iteration 1, loss = 6.18832596
Iteration 2, loss = 5.78506157
Iteration 3, loss = 5.62806611
Iteration 4, loss = 5.58270517
Iteration 5, loss = 5.50918336
Iteration 6, loss = 5.50090303
Iteration 7, loss = 5.51908761
Iteration 8, loss = 5.46572532
Iteration 9, loss = 5.37354650
Iteration 10, loss = 5.32068239
Iteration 11, loss = 5.28807975
Iteration 12, loss = 5.28587276
Iteration 13, loss = 5.26600418
Iteration 14, loss = 5.21938819
Iteration 15, loss = 5.23002929
Iteration 16, loss = 5.21546859
Iteration 17, loss = 5.15783424
Iteration 18, loss = 5.13618256
Iteration 19, loss = 5.12346533
Iteration 20, loss = 5.08812575
Iteration 21, loss = 5.08456328
Iteration 22, loss = 5.04446883
Iteration 23, loss = 5.05227757
Iteration 24, loss = 5.01608562
Iteration 25, loss = 5.03123834
Iteration 26, loss = 5.03587840
Iteration 27, loss = 4.99586054
Iteration 28, loss = 4.94364542
Iteration 29, loss = 4.96269344
Iteration 30, loss = 4.93730484
Iteration 31, loss = 4.93748134
Iteration 32, loss = 4.94053774
Iteration 33, loss = 4.91715192
Iteration 34, loss = 4.89857497
Iteration 35, loss = 4.91971058
Iteration 36, loss = 4.97210762
Iteration 37, loss = 4.93275745
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.90610679899324698, 'fit': 0.029000000000000001, 'time': 192.14999999999964})
Iteration 1, loss = 6.16925003
Iteration 2, loss = 5.68504586
Iteration 3, loss = 5.56272332
Iteration 4, loss = 5.50486816
Iteration 5, loss = 5.42631279
Iteration 6, loss = 5.42516307
Iteration 7, loss = 5.44316861
Iteration 8, loss = 5.42089371
Iteration 9, loss = 5.33615158
Iteration 10, loss = 5.28284159
Iteration 11, loss = 5.24580342
Iteration 12, loss = 5.24245808
Iteration 13, loss = 5.19269993
Iteration 14, loss = 5.15812463
Iteration 15, loss = 5.15223589
Iteration 16, loss = 5.13560724
Iteration 17, loss = 5.13671177
Iteration 18, loss = 5.13489476
Iteration 19, loss = 5.10809225
Iteration 20, loss = 5.11807763
Iteration 21, loss = 5.09542590
Iteration 22, loss = 5.08549736
Iteration 23, loss = 5.05274964
Iteration 24, loss = 5.03535111
Iteration 25, loss = 5.02594811
Iteration 26, loss = 4.97907543
Iteration 27, loss = 5.00651182
Iteration 28, loss = 4.99104141
Iteration 29, loss = 4.99085259
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.97375302987116963, 'fit': 0.028375000000000001, 'time': 153.84000000000015})
Iteration 1, loss = 6.16865421
Iteration 2, loss = 5.73913787
Iteration 3, loss = 5.58380996
Iteration 4, loss = 5.53809601
Iteration 5, loss = 5.54109471
Iteration 6, loss = 5.48173769
Iteration 7, loss = 5.42340607
Iteration 8, loss = 5.40033793
Iteration 9, loss = 5.32111597
Iteration 10, loss = 5.30057030
Iteration 11, loss = 5.29417864
Iteration 12, loss = 5.24907835
Iteration 13, loss = 5.20112438
Iteration 14, loss = 5.21324442
Iteration 15, loss = 5.16785806
Iteration 16, loss = 5.17080017
Iteration 17, loss = 5.12954977
Iteration 18, loss = 5.14825334
Iteration 19, loss = 5.11306944
Iteration 20, loss = 5.10347475
Iteration 21, loss = 5.10271034
Iteration 22, loss = 5.03965876
Iteration 23, loss = 5.04608160
Iteration 24, loss = 5.04953846
Iteration 25, loss = 5.01985594
Iteration 26, loss = 5.01680106
Iteration 27, loss = 4.98413757
Iteration 28, loss = 4.97684719
Iteration 29, loss = 4.97926354
Iteration 30, loss = 4.94278852
Iteration 31, loss = 4.93590959
Iteration 32, loss = 5.02462719
Iteration 33, loss = 4.99370101
Iteration 34, loss = 4.93714216
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.82411109809336403, 'fit': 0.0315, 'time': 186.4399999999987})
Iteration 1, loss = 6.16122309
Iteration 2, loss = 5.78736347
Iteration 3, loss = 5.72237037
Iteration 4, loss = 5.69201081
Iteration 5, loss = 5.62858956
Iteration 6, loss = 5.55963393
Iteration 7, loss = 5.55424296
Iteration 8, loss = 5.52558141
Iteration 9, loss = 5.50906482
Iteration 10, loss = 5.47625613
Iteration 11, loss = 5.45026165
Iteration 12, loss = 5.43213601
Iteration 13, loss = 5.40065550
Iteration 14, loss = 5.39072719
Iteration 15, loss = 5.50066200
Iteration 16, loss = 5.46783379
Iteration 17, loss = 5.37156856
Iteration 18, loss = 5.28531946
Iteration 19, loss = 5.23859934
Iteration 20, loss = 5.19795806
Iteration 21, loss = 5.19114746
Iteration 22, loss = 5.18234831
Iteration 23, loss = 5.16844975
Iteration 24, loss = 5.08437395
Iteration 25, loss = 5.07645502
Iteration 26, loss = 5.04196022
Iteration 27, loss = 5.04081533
Iteration 28, loss = 5.10970521
Iteration 29, loss = 5.08484349
Iteration 30, loss = 5.07867702
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 0.96463156242895964, 'fit': 0.024375000000000001, 'time': 168.48999999999978})
Iteration 1, loss = 6.22197683
Iteration 2, loss = 6.00888884
Iteration 3, loss = 5.87113787
Iteration 4, loss = 5.81618176
Iteration 5, loss = 5.79871570
Iteration 6, loss = 5.78350799
Iteration 7, loss = 5.76608544
Iteration 8, loss = 5.79461815
Iteration 9, loss = 5.78524588
Iteration 10, loss = 5.83962179
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 2.3941296435859889, 'fit': 0.018249999999999999, 'time': 56.69000000000051})
Iteration 1, loss = 6.21281597
Iteration 2, loss = 5.79764008
Iteration 3, loss = 5.71616664
Iteration 4, loss = 5.59930613
Iteration 5, loss = 5.65174369
Iteration 6, loss = 5.66765666
Iteration 7, loss = 5.61512495
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 1.7159552636646238, 'fit': 0.021624999999999998, 'time': 40.780000000000655})
Iteration 1, loss = 6.21727968
Iteration 2, loss = 5.96433822
Iteration 3, loss = 5.91268507
Iteration 4, loss = 5.90243888
Iteration 5, loss = 5.88384989
Iteration 6, loss = 5.88950664
Iteration 7, loss = 5.90851708
Iteration 8, loss = 5.90028192
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.58577812795758, 'fit': 0.018249999999999999, 'time': 46.92000000000007})
Iteration 1, loss = 6.18886497
Iteration 2, loss = 5.74484939
Iteration 3, loss = 5.67603920
Iteration 4, loss = 5.69265969
Iteration 5, loss = 5.62394552
Iteration 6, loss = 5.61480850
Iteration 7, loss = 5.57907302
Iteration 8, loss = 5.62007815
Iteration 9, loss = 5.71514255
Iteration 10, loss = 5.73441893
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 1.5938044000139733, 'fit': 0.0195, 'time': 61.31999999999971})
Iteration 1, loss = 6.24119554
Iteration 2, loss = 6.05097243
Iteration 3, loss = 6.00688341
Iteration 4, loss = 5.93148270
Iteration 5, loss = 5.92184500
Iteration 6, loss = 5.90222404
Iteration 7, loss = 5.87368633
Iteration 8, loss = 5.86238917
Iteration 9, loss = 5.92298086
Iteration 10, loss = 5.96943078
Iteration 11, loss = 5.95278074
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.4045646494478889, 'fit': 0.017125000000000001, 'time': 68.42000000000007})
Iteration 1, loss = 6.25141044
Iteration 2, loss = 6.00361622
Iteration 3, loss = 5.91350903
Iteration 4, loss = 5.90337508
Iteration 5, loss = 5.88481811
Iteration 6, loss = 5.87468710
Iteration 7, loss = 5.87897107
Iteration 8, loss = 5.86925029
Iteration 9, loss = 5.86494808
Iteration 10, loss = 5.86325089
Iteration 11, loss = 5.86134542
Iteration 12, loss = 5.86157042
Iteration 13, loss = 5.85924108
Iteration 14, loss = 5.85872871
Iteration 15, loss = 5.85767658
Iteration 16, loss = 5.85842062
Iteration 17, loss = 5.85944093
Iteration 18, loss = 5.85668990
Iteration 19, loss = 5.85740539
Iteration 20, loss = 5.85514969
Iteration 21, loss = 5.85717204
Iteration 22, loss = 5.85465459
Iteration 23, loss = 5.85475739
Iteration 24, loss = 5.85475022
Iteration 25, loss = 5.85135019
Iteration 26, loss = 5.85457482
Iteration 27, loss = 5.85368538
Iteration 28, loss = 5.85112039
Iteration 29, loss = 5.85342288
Iteration 30, loss = 5.85368572
Iteration 31, loss = 5.85249992
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 2.3972301956759616, 'fit': 0.017999999999999999, 'time': 197.79000000000087})
Iteration 1, loss = 6.25959218
Iteration 2, loss = 6.05221134
Iteration 3, loss = 5.94136651
Iteration 4, loss = 5.91702137
Iteration 5, loss = 5.88496858
Iteration 6, loss = 5.92808916
Iteration 7, loss = 5.96046883
Iteration 8, loss = 5.92060000
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 2.8091724815770385, 'fit': 0.017999999999999999, 'time': 51.899999999999636})
Iteration 1, loss = 6.25682708
Iteration 2, loss = 5.97586639
Iteration 3, loss = 5.87436274
Iteration 4, loss = 5.84400349
Iteration 5, loss = 5.85117044
Iteration 6, loss = 5.84375872
Iteration 7, loss = 5.83858537
Iteration 8, loss = 5.83466010
Iteration 9, loss = 5.83463920
Iteration 10, loss = 5.83124038
Iteration 11, loss = 5.84036827
Iteration 12, loss = 5.83597841
Iteration 13, loss = 5.83144284
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 2.2376437395090969, 'fit': 0.018124999999999999, 'time': 86.80000000000109})
Iteration 1, loss = 6.24213250
Iteration 2, loss = 6.04380713
Iteration 3, loss = 6.08132378
Iteration 4, loss = 6.06281680
Iteration 5, loss = 5.91751734
Iteration 6, loss = 5.87282333
Iteration 7, loss = 5.91980482
Iteration 8, loss = 5.91734436
Iteration 9, loss = 5.91184603
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.6503455498786526, 'fit': 0.016125, 'time': 61.18000000000029})
Iteration 1, loss = 6.24459516
Iteration 2, loss = 6.04596645
Iteration 3, loss = 6.01075583
Iteration 4, loss = 5.90784190
Iteration 5, loss = 5.83494924
Iteration 6, loss = 5.79402391
Iteration 7, loss = 5.80675668
Iteration 8, loss = 5.90615975
Iteration 9, loss = 5.93041620
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.4271286103408429, 'fit': 0.019875, 'time': 62.54999999999927})
Iteration 1, loss = 6.24554458
Iteration 2, loss = 6.04566816
Iteration 3, loss = 6.03748459
Iteration 4, loss = 6.03517693
Iteration 5, loss = 6.03399844
Iteration 6, loss = 6.03513695
Iteration 7, loss = 6.03331328
Iteration 8, loss = 6.03304192
Iteration 9, loss = 6.03511423
Iteration 10, loss = 6.03140003
Iteration 11, loss = 6.03054679
Iteration 12, loss = 6.15943536
Iteration 13, loss = 6.03794999
Iteration 14, loss = 6.03112070
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.6503455498786526, 'fit': 0.016125, 'time': 98.60000000000036})
Iteration 1, loss = 6.27713611
Iteration 2, loss = 6.02251043
Iteration 3, loss = 5.94291940
Iteration 4, loss = 5.94974702
Iteration 5, loss = 5.88899301
Iteration 6, loss = 5.86845790
Iteration 7, loss = 5.86119571
Iteration 8, loss = 5.85966207
Iteration 9, loss = 5.85505121
Iteration 10, loss = 5.85386684
Iteration 11, loss = 5.85286756
Iteration 12, loss = 5.85368344
Iteration 13, loss = 5.85102226
Iteration 14, loss = 5.85098212
Iteration 15, loss = 5.85108867
Iteration 16, loss = 5.85004990
Iteration 17, loss = 5.87773704
Iteration 18, loss = 5.87869793
Iteration 19, loss = 5.87036765
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 2.6776815869580037, 'fit': 0.017125000000000001, 'time': 138.42000000000007})
Iteration 1, loss = 6.25054007
Iteration 2, loss = 6.04332015
Iteration 3, loss = 6.03692098
Iteration 4, loss = 6.03483646
Iteration 5, loss = 6.06047027
Iteration 6, loss = 6.03378465
Iteration 7, loss = 5.99942970
Iteration 8, loss = 5.96598951
Iteration 9, loss = 5.94830585
Iteration 10, loss = 5.93843641
Iteration 11, loss = 5.87963912
Iteration 12, loss = 5.84880573
Iteration 13, loss = 5.84215517
Iteration 14, loss = 5.83313972
Iteration 15, loss = 5.83114525
Iteration 16, loss = 5.82665465
Iteration 17, loss = 5.82377954
Iteration 18, loss = 5.82526109
Iteration 19, loss = 5.82224207
Iteration 20, loss = 5.82175023
Iteration 21, loss = 5.82065552
Iteration 22, loss = 5.82000994
Iteration 23, loss = 5.81909537
Iteration 24, loss = 5.81902205
Iteration 25, loss = 5.81901275
Iteration 26, loss = 5.81973437
Iteration 27, loss = 5.81911684
Iteration 28, loss = 5.81810184
Iteration 29, loss = 5.81715078
Iteration 30, loss = 5.81516458
Iteration 31, loss = 5.81648090
Iteration 32, loss = 5.81519302
Iteration 33, loss = 5.81674183
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 1.9353157266487631, 'fit': 0.017624999999999998, 'time': 246.10000000000036})
Iteration 1, loss = 6.24983397
Iteration 2, loss = 6.04819423
Iteration 3, loss = 6.03755263
Iteration 4, loss = 6.03372546
Iteration 5, loss = 6.03531242
Iteration 6, loss = 6.03184852
Iteration 7, loss = 6.03428038
Iteration 8, loss = 6.03179816
Iteration 9, loss = 6.03052823
Iteration 10, loss = 6.03156281
Iteration 11, loss = 6.03026357
Iteration 12, loss = 6.02890721
Iteration 13, loss = 6.02770440
Iteration 14, loss = 6.03146672
Iteration 15, loss = 6.02950355
Iteration 16, loss = 6.02797077
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 4.1291809042091216, 'fit': 0.015375, 'time': 120.14000000000124})
Iteration 1, loss = 6.24896328
Iteration 2, loss = 6.09613840
Iteration 3, loss = 6.00812290
Iteration 4, loss = 5.96516683
Iteration 5, loss = 5.93792018
Iteration 6, loss = 5.89290252
Iteration 7, loss = 5.84937603
Iteration 8, loss = 5.84218028
Iteration 9, loss = 5.83166862
Iteration 10, loss = 5.82924018
Iteration 11, loss = 5.82419167
Iteration 12, loss = 5.82145259
Iteration 13, loss = 5.82174747
Iteration 14, loss = 5.82051206
Iteration 15, loss = 5.81790905
Iteration 16, loss = 5.81688002
Iteration 17, loss = 5.81577702
Iteration 18, loss = 5.81542437
Iteration 19, loss = 5.81745535
Iteration 20, loss = 5.81856966
Iteration 21, loss = 5.81990753
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 2.4719061983275008, 'fit': 0.017999999999999999, 'time': 162.75})
Iteration 1, loss = 6.22821483
Iteration 2, loss = 6.00305109
Iteration 3, loss = 5.94644043
Iteration 4, loss = 5.92337535
Iteration 5, loss = 5.91158071
Iteration 6, loss = 5.90305611
Iteration 7, loss = 5.89954866
Iteration 8, loss = 5.89522987
Iteration 9, loss = 5.89396475
Iteration 10, loss = 5.89520078
Iteration 11, loss = 5.89397478
Iteration 12, loss = 5.89260009
Iteration 13, loss = 5.89201275
Iteration 14, loss = 5.89162319
Iteration 15, loss = 5.88803318
Iteration 16, loss = 5.88946779
Iteration 17, loss = 5.88903186
Iteration 18, loss = 5.88728379
Iteration 19, loss = 5.88576049
Iteration 20, loss = 5.88408500
Iteration 21, loss = 5.88466441
Iteration 22, loss = 5.88548885
Iteration 23, loss = 5.88375518
Iteration 24, loss = 5.88254014
Iteration 25, loss = 5.88488667
Iteration 26, loss = 5.88317885
Iteration 27, loss = 5.88259222
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.4908538727975738, 'fit': 0.018749999999999999, 'time': 211.02999999999884})
Iteration 1, loss = 6.22453285
Iteration 2, loss = 5.94769068
Iteration 3, loss = 5.93207948
Iteration 4, loss = 5.92492091
Iteration 5, loss = 5.93048542
Iteration 6, loss = 5.92642648
Iteration 7, loss = 6.02282379
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.3634823661996145, 'fit': 0.012999999999999999, 'time': 55.530000000000655})
Iteration 1, loss = 6.24270273
Iteration 2, loss = 6.08490135
Iteration 3, loss = 6.03904773
Iteration 4, loss = 6.03814534
Iteration 5, loss = 6.03193293
Iteration 6, loss = 6.03634599
Iteration 7, loss = 6.03462377
Iteration 8, loss = 6.03000869
Iteration 9, loss = 6.03153313
Iteration 10, loss = 6.03074455
Iteration 11, loss = 6.02933353
Iteration 12, loss = 6.02781404
Iteration 13, loss = 6.02625336
Iteration 14, loss = 6.02712749
Iteration 15, loss = 6.02966924
Iteration 16, loss = 6.02637342
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.8585056800337765, 'fit': 0.016500000000000001, 'time': 129.78000000000065})
Iteration 1, loss = 6.24042808
Iteration 2, loss = 6.06735169
Iteration 3, loss = 6.03848108
Iteration 4, loss = 6.03567763
Iteration 5, loss = 6.03539730
Iteration 6, loss = 6.03506722
Iteration 7, loss = 6.03491486
Iteration 8, loss = 6.03078758
Iteration 9, loss = 6.03098606
Iteration 10, loss = 6.02961369
Iteration 11, loss = 6.03179462
Iteration 12, loss = 6.02972887
Iteration 13, loss = 6.02942671
Iteration 14, loss = 6.03006794
Iteration 15, loss = 6.02774993
Iteration 16, loss = 6.02941069
Iteration 17, loss = 6.02827904
Iteration 18, loss = 6.02651490
Iteration 19, loss = 6.02867502
Iteration 20, loss = 6.02688298
Iteration 21, loss = 6.02942927
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.8585056800337765, 'fit': 0.016500000000000001, 'time': 173.84999999999854})
Iteration 1, loss = 6.23561401
Iteration 2, loss = 6.04693887
Iteration 3, loss = 6.03656071
Iteration 4, loss = 6.03660407
Iteration 5, loss = 6.03372591
Iteration 6, loss = 6.03485149
Iteration 7, loss = 6.03117758
Iteration 8, loss = 6.03252728
Iteration 9, loss = 6.03143291
Iteration 10, loss = 6.03131251
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.8585056800337765, 'fit': 0.016500000000000001, 'time': 83.22999999999956})
Iteration 1, loss = 6.21747921
Iteration 2, loss = 5.90447023
Iteration 3, loss = 5.87011446
Iteration 4, loss = 5.81768907
Iteration 5, loss = 5.79596670
Iteration 6, loss = 5.78961822
Iteration 7, loss = 5.78853079
Iteration 8, loss = 5.78710608
Iteration 9, loss = 5.78488934
Iteration 10, loss = 5.78583338
Iteration 11, loss = 5.78559170
Iteration 12, loss = 5.78348333
Iteration 13, loss = 5.79140895
Iteration 14, loss = 5.78944305
Iteration 15, loss = 5.78909289
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 2.242183239318142, 'fit': 0.019, 'time': 129.13000000000102})
Iteration 1, loss = 6.24596369
Iteration 2, loss = 6.04307159
Iteration 3, loss = 6.03807805
Iteration 4, loss = 6.03515297
Iteration 5, loss = 6.03530749
Iteration 6, loss = 6.03168086
Iteration 7, loss = 6.03200465
Iteration 8, loss = 6.03347796
Iteration 9, loss = 6.03184710
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.8585056800337765, 'fit': 0.016500000000000001, 'time': 77.40999999999985})
Iteration 1, loss = 6.23710529
Iteration 2, loss = 6.04374561
Iteration 3, loss = 6.03862522
Iteration 4, loss = 6.03350757
Iteration 5, loss = 6.03396454
Iteration 6, loss = 6.03245450
Iteration 7, loss = 6.03432093
Iteration 8, loss = 6.03258291
Iteration 9, loss = 6.02988824
Iteration 10, loss = 6.03104656
Iteration 11, loss = 6.03215285
Iteration 12, loss = 6.35795980
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.9866532810170701, 'fit': 0.018124999999999999, 'time': 105.22999999999956})
Iteration 1, loss = 6.24423632
Iteration 2, loss = 6.04412335
Iteration 3, loss = 6.03924336
Iteration 4, loss = 6.03870197
Iteration 5, loss = 6.03365772
Iteration 6, loss = 6.03217360
Iteration 7, loss = 6.03245702
Iteration 8, loss = 6.03296313
Iteration 9, loss = 6.03358427
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 4.1291809042091216, 'fit': 0.015375, 'time': 80.19000000000233})
Iteration 1, loss = 6.24651655
Iteration 2, loss = 6.04622144
Iteration 3, loss = 6.03790793
Iteration 4, loss = 6.03508282
Iteration 5, loss = 6.03480116
Iteration 6, loss = 6.03302633
Iteration 7, loss = 6.03275027
Iteration 8, loss = 6.03107653
Iteration 9, loss = 6.03104926
Iteration 10, loss = 6.03257716
Iteration 11, loss = 6.03228434
Iteration 12, loss = 6.03153056
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 4.1291809042091216, 'fit': 0.015375, 'time': 108.12999999999738})
Iteration 1, loss = 6.23791706
Iteration 2, loss = 6.04562374
Iteration 3, loss = 6.03885306
Iteration 4, loss = 6.03481172
Iteration 5, loss = 6.03473798
Iteration 6, loss = 6.08235214
Iteration 7, loss = 6.03874659
Iteration 8, loss = 6.03399806
Iteration 9, loss = 6.03181969
Iteration 10, loss = 6.06911351
Iteration 11, loss = 6.03191750
Iteration 12, loss = 6.03185541
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.6503455498786526, 'fit': 0.016125, 'time': 111.54000000000087})
Iteration 1, loss = 6.24795471
Iteration 2, loss = 6.04605053
Iteration 3, loss = 6.03952501
Iteration 4, loss = 6.03631198
Iteration 5, loss = 6.05397126
Iteration 6, loss = 6.05247070
Iteration 7, loss = 6.03290787
Iteration 8, loss = 6.03035500
Iteration 9, loss = 6.02676755
Iteration 10, loss = 6.01650873
Iteration 11, loss = 6.00657872
Iteration 12, loss = 5.99814856
Iteration 13, loss = 5.99184432
Iteration 14, loss = 5.98746496
Iteration 15, loss = 5.98497914
Iteration 16, loss = 5.98470091
Iteration 17, loss = 5.98645598
Iteration 18, loss = 5.99044680
Iteration 19, loss = 5.98709021
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 4.3004341038901952, 'fit': 0.0155, 'time': 180.27999999999884})
Iteration 1, loss = 6.26150043
Iteration 2, loss = 6.04584853
Iteration 3, loss = 6.09832095
Iteration 4, loss = 6.01880940
Iteration 5, loss = 5.95648081
Iteration 6, loss = 5.91751786
Iteration 7, loss = 5.89806147
Iteration 8, loss = 5.89216149
Iteration 9, loss = 5.88766812
Iteration 10, loss = 5.88687828
Iteration 11, loss = 5.89046171
Iteration 12, loss = 5.88757654
Iteration 13, loss = 5.88957236
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.7321536607512673, 'fit': 0.016375000000000001, 'time': 124.77000000000044})
Iteration 1, loss = 6.23429733
Iteration 2, loss = 6.03915168
Iteration 3, loss = 5.98955809
Iteration 4, loss = 5.96207154
Iteration 5, loss = 5.94948740
Iteration 6, loss = 5.94085643
Iteration 7, loss = 5.93531142
Iteration 8, loss = 5.93350507
Iteration 9, loss = 5.93140003
Iteration 10, loss = 5.92997234
Iteration 11, loss = 5.93057529
Iteration 12, loss = 5.92941790
Iteration 13, loss = 5.92831181
Iteration 14, loss = 5.92938972
Iteration 15, loss = 5.92718184
Iteration 16, loss = 5.92513285
Iteration 17, loss = 5.92477364
Iteration 18, loss = 5.92301002
Iteration 19, loss = 5.91904346
Iteration 20, loss = 5.91321265
Iteration 21, loss = 5.90843614
Iteration 22, loss = 5.90640214
Iteration 23, loss = 5.90651089
Iteration 24, loss = 5.90400772
Iteration 25, loss = 5.90216249
Iteration 26, loss = 5.89870679
Iteration 27, loss = 5.88442466
Iteration 28, loss = 5.87809886
Iteration 29, loss = 5.87369480
Iteration 30, loss = 5.87168140
Iteration 31, loss = 5.86894168
Iteration 32, loss = 5.86795918
Iteration 33, loss = 5.86770272
Iteration 34, loss = 5.86487186
Iteration 35, loss = 5.86487779
Iteration 36, loss = 5.86527984
Iteration 37, loss = 5.86326864
Iteration 38, loss = 5.86565044
Iteration 39, loss = 5.86387847
Iteration 40, loss = 5.86322341
Iteration 41, loss = 5.86251586
Iteration 42, loss = 5.86146991
Iteration 43, loss = 5.86325353
Iteration 44, loss = 5.86128973
Iteration 45, loss = 5.86072373
Iteration 46, loss = 5.86140709
Iteration 47, loss = 5.86234544
Iteration 48, loss = 5.86534880
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 2.9709180088243903, 'fit': 0.017749999999999998, 'time': 470.21000000000276})
Iteration 1, loss = 6.25569295
Iteration 2, loss = 6.04967975
Iteration 3, loss = 6.02567661
Iteration 4, loss = 5.97132642
Iteration 5, loss = 6.03390432
Iteration 6, loss = 6.03598095
Iteration 7, loss = 6.06821980
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.0047170046592195, 'fit': 0.0066249999999999998, 'time': 69.26000000000204})
Iteration 1, loss = 6.22823322
Iteration 2, loss = 6.04370992
Iteration 3, loss = 6.03690195
Iteration 4, loss = 6.03790910
Iteration 5, loss = 6.03589581
Iteration 6, loss = 6.03375807
Iteration 7, loss = 6.03428480
Iteration 8, loss = 6.03172585
Iteration 9, loss = 6.03087671
Iteration 10, loss = 6.03123558
Iteration 11, loss = 6.03032422
Iteration 12, loss = 6.03374532
Iteration 13, loss = 6.02980821
Iteration 14, loss = 6.02801273
Iteration 15, loss = 6.02897220
Iteration 16, loss = 6.02918661
Iteration 17, loss = 6.02904516
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.4816444642968407, 'fit': 0.016250000000000001, 'time': 168.04000000000087})
Iteration 1, loss = 6.24677550
Iteration 2, loss = 6.04562329
Iteration 3, loss = 6.03835464
Iteration 4, loss = 6.03843197
Iteration 5, loss = 6.03388519
Iteration 6, loss = 6.03192104
Iteration 7, loss = 6.03348328
Iteration 8, loss = 6.03245793
Iteration 9, loss = 6.07603463
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.3401266578523576, 'fit': 0.012999999999999999, 'time': 89.88000000000102})
Iteration 1, loss = 6.24969608
Iteration 2, loss = 6.04550166
Iteration 3, loss = 6.03652691
Iteration 4, loss = 6.03831354
Iteration 5, loss = 6.03420745
Iteration 6, loss = 6.03407881
Iteration 7, loss = 6.02999994
Iteration 8, loss = 6.03285433
Iteration 9, loss = 6.03239125
Iteration 10, loss = 6.03166055
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.8585056800337765, 'fit': 0.016500000000000001, 'time': 103.88999999999942})
Iteration 1, loss = 6.25504449
Iteration 2, loss = 6.04575005
Iteration 3, loss = 6.09683001
Iteration 4, loss = 6.01239124
Iteration 5, loss = 5.98275169
Iteration 6, loss = 5.97610823
Iteration 7, loss = 5.97036570
Iteration 8, loss = 5.96134345
Iteration 9, loss = 5.96166801
Iteration 10, loss = 5.95744026
Iteration 11, loss = 5.95450169
Iteration 12, loss = 5.95539740
Iteration 13, loss = 5.95281972
Iteration 14, loss = 5.95291221
Iteration 15, loss = 5.95044488
Iteration 16, loss = 5.95073832
Iteration 17, loss = 5.94789476
Iteration 18, loss = 5.94887064
Iteration 19, loss = 5.94693712
Iteration 20, loss = 5.94829416
Iteration 21, loss = 5.94771637
Iteration 22, loss = 5.94736758
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.6767655842433902, 'fit': 0.016875000000000001, 'time': 231.7599999999984})
Iteration 1, loss = 6.26192140
Iteration 2, loss = 6.04611115
Iteration 3, loss = 6.03830315
Iteration 4, loss = 6.03589367
Iteration 5, loss = 6.03166606
Iteration 6, loss = 6.03578560
Iteration 7, loss = 6.03302610
Iteration 8, loss = 6.36345481
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.5121045791956198, 'fit': 0.020125000000000001, 'time': 83.77000000000044})
Iteration 1, loss = 6.24902088
Iteration 2, loss = 6.04535877
Iteration 3, loss = 6.03778419
Iteration 4, loss = 6.03398552
Iteration 5, loss = 6.03526487
Iteration 6, loss = 6.10876776
Iteration 7, loss = 6.03298263
Iteration 8, loss = 6.03153670
Iteration 9, loss = 6.02959024
Iteration 10, loss = 6.03160680
Iteration 11, loss = 6.03152852
Iteration 12, loss = 6.03069254
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 4.5142493107681325, 'fit': 0.012874999999999999, 'time': 128.54999999999927})
Iteration 1, loss = 6.25190352
Iteration 2, loss = 6.04548070
Iteration 3, loss = 6.03810521
Iteration 4, loss = 6.03738530
Iteration 5, loss = 6.03439099
Iteration 6, loss = 6.03715514
Iteration 7, loss = 6.03421671
Iteration 8, loss = 6.03113596
Iteration 9, loss = 6.03276180
Iteration 10, loss = 6.03361812
Iteration 11, loss = 6.03023051
Iteration 12, loss = 6.02937191
Iteration 13, loss = 6.02937969
Iteration 14, loss = 6.03036362
Iteration 15, loss = 6.02857467
Iteration 16, loss = 6.02680890
Iteration 17, loss = 6.02745727
Iteration 18, loss = 6.02666839
Iteration 19, loss = 6.02855718
Iteration 20, loss = 6.02471714
Iteration 21, loss = 6.02661219
Iteration 22, loss = 6.02482866
Iteration 23, loss = 6.02692376
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.4816444642968407, 'fit': 0.016250000000000001, 'time': 245.8199999999997})
Iteration 1, loss = 6.22748959
Iteration 2, loss = 6.04514188
Iteration 3, loss = 6.03988796
Iteration 4, loss = 6.03588640
Iteration 5, loss = 6.03569694
Iteration 6, loss = 6.03539762
Iteration 7, loss = 6.03501519
Iteration 8, loss = 6.03158772
Iteration 9, loss = 6.03188588
Iteration 10, loss = 6.03103711
Iteration 11, loss = 6.03160758
Iteration 12, loss = 6.03011981
Iteration 13, loss = 6.02996646
Iteration 14, loss = 6.03085711
Iteration 15, loss = 6.02926716
Iteration 16, loss = 6.02642159
Iteration 17, loss = 6.02738565
Iteration 18, loss = 6.02788331
Iteration 19, loss = 6.02862884
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 4.1291809042091216, 'fit': 0.015375, 'time': 203.51000000000204})
Iteration 1, loss = 6.26154247
Iteration 2, loss = 6.04485450
Iteration 3, loss = 6.03601538
Iteration 4, loss = 6.03613626
Iteration 5, loss = 6.03590746
Iteration 6, loss = 6.03660833
Iteration 7, loss = 6.03427773
Iteration 8, loss = 6.03259455
Iteration 9, loss = 6.03008431
Iteration 10, loss = 6.03134342
Iteration 11, loss = 6.03058221
Iteration 12, loss = 6.03171375
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.4816444642968407, 'fit': 0.016250000000000001, 'time': 131.35000000000218})
Iteration 1, loss = 6.24369309
Iteration 2, loss = 6.04461526
Iteration 3, loss = 6.03746149
Iteration 4, loss = 6.03616199
Iteration 5, loss = 6.03451885
Iteration 6, loss = 6.03349203
Iteration 7, loss = 6.15679106
Iteration 8, loss = 6.15143312
Iteration 9, loss = 6.03913577
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.4816444642968407, 'fit': 0.016250000000000001, 'time': 100.63999999999942})
Iteration 1, loss = 6.24469357
Iteration 2, loss = 6.04649896
Iteration 3, loss = 6.03493526
Iteration 4, loss = 6.03476661
Iteration 5, loss = 6.03564604
Iteration 6, loss = 6.03459466
Iteration 7, loss = 6.03245300
Iteration 8, loss = 6.03272373
Iteration 9, loss = 6.03253163
Iteration 10, loss = 6.03063100
Iteration 11, loss = 6.03288598
Iteration 12, loss = 6.03061592
Iteration 13, loss = 6.02912962
Iteration 14, loss = 6.02948899
Iteration 15, loss = 6.02814204
Iteration 16, loss = 6.02904460
Iteration 17, loss = 6.02743529
Iteration 18, loss = 6.02772144
Iteration 19, loss = 6.02699931
Iteration 20, loss = 6.02602031
Iteration 21, loss = 6.02586373
Iteration 22, loss = 6.02600255
Iteration 23, loss = 6.02679646
Iteration 24, loss = 6.02647430
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.8585056800337765, 'fit': 0.016500000000000001, 'time': 267.27000000000044})
Iteration 1, loss = 6.25147265
Iteration 2, loss = 6.04692619
Iteration 3, loss = 6.07673785
Iteration 4, loss = 6.00113333
Iteration 5, loss = 5.97257691
Iteration 6, loss = 5.95806531
Iteration 7, loss = 5.95025175
Iteration 8, loss = 5.94492625
Iteration 9, loss = 5.94318692
Iteration 10, loss = 5.94780173
Iteration 11, loss = 5.94379935
Iteration 12, loss = 5.94214817
Iteration 13, loss = 5.94124425
Iteration 14, loss = 5.94034699
Iteration 15, loss = 5.93883808
Iteration 16, loss = 5.93691236
Iteration 17, loss = 5.93491960
Iteration 18, loss = 5.93735772
Iteration 19, loss = 5.93769164
Iteration 20, loss = 5.93624913
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.8773274565567655, 'fit': 0.018874999999999999, 'time': 234.70999999999913})
Iteration 1, loss = 6.24872846
Iteration 2, loss = 6.04588801
Iteration 3, loss = 6.03859107
Iteration 4, loss = 6.03621797
Iteration 5, loss = 6.03444746
Iteration 6, loss = 6.03605743
Iteration 7, loss = 6.03694657
Iteration 8, loss = 6.03256566
Iteration 9, loss = 6.03134853
Iteration 10, loss = 6.02919430
Iteration 11, loss = 6.02908435
Iteration 12, loss = 6.03034421
Iteration 13, loss = 6.03106637
Iteration 14, loss = 6.02991632
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 4.1291809042091216, 'fit': 0.015375, 'time': 160.95999999999913})
Iteration 1, loss = 6.24001580
Iteration 2, loss = 6.04472156
Iteration 3, loss = 6.03755310
Iteration 4, loss = 6.03726977
Iteration 5, loss = 6.03526417
Iteration 6, loss = 6.03550427
Iteration 7, loss = 6.03197411
Iteration 8, loss = 6.03147299
Iteration 9, loss = 6.03084588
Iteration 10, loss = 6.02991050
Iteration 11, loss = 6.03196545
Iteration 12, loss = 6.02992091
Iteration 13, loss = 6.02825848
Iteration 14, loss = 6.02965132
Iteration 15, loss = 6.02789939
Iteration 16, loss = 6.03055228
Iteration 17, loss = 6.02745997
Iteration 18, loss = 6.02590641
Iteration 19, loss = 6.02904825
Iteration 20, loss = 6.02579009
Iteration 21, loss = 6.02644268
Iteration 22, loss = 6.02483209
Iteration 23, loss = 6.02716246
Iteration 24, loss = 6.02402733
Iteration 25, loss = 6.02846930
Iteration 26, loss = 6.02475204
Iteration 27, loss = 6.02310157
Iteration 28, loss = 6.02592196
Iteration 29, loss = 6.02549262
Iteration 30, loss = 6.02257444
Iteration 31, loss = 6.02270481
Iteration 32, loss = 6.02325342
Iteration 33, loss = 6.02383016
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 4.1291809042091216, 'fit': 0.015375, 'time': 374.4599999999991})
Iteration 1, loss = 6.26001721
Iteration 2, loss = 6.04650111
Iteration 3, loss = 6.03865826
Iteration 4, loss = 6.03591639
Iteration 5, loss = 6.03540726
Iteration 6, loss = 6.03297700
Iteration 7, loss = 6.03260992
Iteration 8, loss = 6.03226395
Iteration 9, loss = 6.02997100
Iteration 10, loss = 6.03134321
Iteration 11, loss = 6.03144128
Iteration 12, loss = 6.03085772
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.8585056800337765, 'fit': 0.016500000000000001, 'time': 140.63000000000102})
Iteration 1, loss = 6.25890008
Iteration 2, loss = 6.04778632
Iteration 3, loss = 6.03828488
Iteration 4, loss = 6.11385908
Iteration 5, loss = 6.04435227
Iteration 6, loss = 6.03493948
Iteration 7, loss = 6.03268731
Iteration 8, loss = 6.03366676
Iteration 9, loss = 6.03042998
Iteration 10, loss = 6.03214147
Iteration 11, loss = 6.03069034
Iteration 12, loss = 6.03129495
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.8585056800337765, 'fit': 0.016500000000000001, 'time': 146.39999999999782})
Iteration 1, loss = 6.23819774
Iteration 2, loss = 6.04601084
Iteration 3, loss = 6.03603180
Iteration 4, loss = 6.03689946
Iteration 5, loss = 6.03300345
Iteration 6, loss = 6.03346258
Iteration 7, loss = 6.03369628
Iteration 8, loss = 6.03189829
Iteration 9, loss = 6.03147883
Iteration 10, loss = 6.03193999
Iteration 11, loss = 6.02975058
Iteration 12, loss = 6.02873568
Iteration 13, loss = 6.03085453
Iteration 14, loss = 6.02895839
Iteration 15, loss = 6.02714453
Iteration 16, loss = 6.02846083
Iteration 17, loss = 6.02725286
Iteration 18, loss = 6.02691990
Iteration 19, loss = 6.02448851
Iteration 20, loss = 6.02821563
Iteration 21, loss = 6.02696366
Iteration 22, loss = 6.02541923
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.8585056800337765, 'fit': 0.016500000000000001, 'time': 262.880000000001})
Iteration 1, loss = 6.23963456
Iteration 2, loss = 6.04466822
Iteration 3, loss = 6.03881860
Iteration 4, loss = 6.03475918
Iteration 5, loss = 6.03680423
Iteration 6, loss = 6.03489079
Iteration 7, loss = 6.03423270
Iteration 8, loss = 6.03272716
Iteration 9, loss = 6.03293283
Iteration 10, loss = 6.03031904
Iteration 11, loss = 6.03178748
Iteration 12, loss = 6.03211338
Iteration 13, loss = 6.03023789
Iteration 14, loss = 6.02985829
Iteration 15, loss = 6.02751830
Iteration 16, loss = 6.02575942
Iteration 17, loss = 6.03016207
Iteration 18, loss = 6.02693721
Iteration 19, loss = 6.02926924
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.8585056800337765, 'fit': 0.016500000000000001, 'time': 229.33000000000175})
Iteration 1, loss = 6.24187466
Iteration 2, loss = 6.04527366
Iteration 3, loss = 6.03616045
Iteration 4, loss = 6.03462132
Iteration 5, loss = 6.03489841
Iteration 6, loss = 6.03483907
Iteration 7, loss = 6.03371300
Iteration 8, loss = 6.03197913
Iteration 9, loss = 6.03086726
Iteration 10, loss = 6.03097546
Iteration 11, loss = 6.02869046
Iteration 12, loss = 6.02986162
Iteration 13, loss = 6.02822435
Iteration 14, loss = 6.02835561
Iteration 15, loss = 6.02775119
Iteration 16, loss = 6.02910986
Iteration 17, loss = 6.02773745
Iteration 18, loss = 6.02793343
Iteration 19, loss = 6.02691281
Iteration 20, loss = 6.02687342
Iteration 21, loss = 6.02572783
Iteration 22, loss = 6.02927449
Iteration 23, loss = 6.02566038
Iteration 24, loss = 6.02589743
Iteration 25, loss = 6.02351187
Iteration 26, loss = 6.02609202
Iteration 27, loss = 6.02384170
Iteration 28, loss = 6.02432141
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 4.1291809042091216, 'fit': 0.015375, 'time': 336.4699999999975})
Iteration 1, loss = 6.23903315
Iteration 2, loss = 6.04307523
Iteration 3, loss = 6.03935413
Iteration 4, loss = 6.03559798
Iteration 5, loss = 6.03544933
Iteration 6, loss = 6.03485151
Iteration 7, loss = 6.03225835
Iteration 8, loss = 6.03255809
Iteration 9, loss = 6.03232259
Iteration 10, loss = 6.03085102
Iteration 11, loss = 6.02968838
Iteration 12, loss = 6.03018331
Iteration 13, loss = 6.02959083
Iteration 14, loss = 6.03078248
Iteration 15, loss = 6.02769915
Iteration 16, loss = 6.02862224
Iteration 17, loss = 6.02767816
Iteration 18, loss = 6.02656880
Iteration 19, loss = 6.02712051
Iteration 20, loss = 6.02927860
Iteration 21, loss = 6.02656488
Iteration 22, loss = 6.02781136
Iteration 23, loss = 6.02365793
Iteration 24, loss = 6.02600868
Iteration 25, loss = 6.02646833
Iteration 26, loss = 6.02647685
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.6503455498786526, 'fit': 0.016125, 'time': 319.27999999999884})
Iteration 1, loss = 6.23639307
Iteration 2, loss = 6.04442385
Iteration 3, loss = 6.04159889
Iteration 4, loss = 6.03860967
Iteration 5, loss = 6.03663535
Iteration 6, loss = 6.03332580
Iteration 7, loss = 6.03443906
Iteration 8, loss = 6.03320342
Iteration 9, loss = 6.03261709
Iteration 10, loss = 6.03054509
Iteration 11, loss = 6.03018500
Iteration 12, loss = 6.03096632
Iteration 13, loss = 6.02827913
Iteration 14, loss = 6.03127224
Iteration 15, loss = 6.02716891
Iteration 16, loss = 6.03100585
Iteration 17, loss = 6.02687829
Iteration 18, loss = 6.02649249
Iteration 19, loss = 6.02677451
Iteration 20, loss = 6.02616139
Iteration 21, loss = 6.02567555
Iteration 22, loss = 6.02737985
Iteration 23, loss = 6.02574919
Iteration 24, loss = 6.02709373
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.6503455498786526, 'fit': 0.016125, 'time': 295.880000000001})
Iteration 1, loss = 6.25838445
Iteration 2, loss = 6.07233194
Iteration 3, loss = 6.04188824
Iteration 4, loss = 6.03690382
Iteration 5, loss = 6.03503716
Iteration 6, loss = 6.03414242
Iteration 7, loss = 6.03265895
Iteration 8, loss = 6.03284853
Iteration 9, loss = 6.03362791
Iteration 10, loss = 6.03137134
Iteration 11, loss = 6.03185277
Iteration 12, loss = 6.03036123
Iteration 13, loss = 6.03215117
Iteration 14, loss = 6.02897025
Iteration 15, loss = 6.02915288
Iteration 16, loss = 6.03009199
Iteration 17, loss = 6.02956356
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 4.1291809042091216, 'fit': 0.015375, 'time': 220.04999999999927})
Iteration 1, loss = 6.23923640
Iteration 2, loss = 6.15155847
Iteration 3, loss = 6.04437707
Iteration 4, loss = 6.03617134
Iteration 5, loss = 6.03589482
Iteration 6, loss = 6.03367656
Iteration 7, loss = 6.03451736
Iteration 8, loss = 6.03175185
Iteration 9, loss = 6.03203896
Iteration 10, loss = 6.03138409
Iteration 11, loss = 6.02967507
Iteration 12, loss = 6.03147490
Iteration 13, loss = 6.03052378
Iteration 14, loss = 6.02974490
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 4.1291809042091216, 'fit': 0.015375, 'time': 180.20999999999913})
Iteration 1, loss = 6.26718275
Iteration 2, loss = 6.04535553
Iteration 3, loss = 6.03839599
Iteration 4, loss = 6.03393365
Iteration 5, loss = 6.03403002
Iteration 6, loss = 6.03314257
Iteration 7, loss = 6.03273300
Iteration 8, loss = 6.02990280
Iteration 9, loss = 6.02785313
Iteration 10, loss = 6.03144308
Iteration 11, loss = 6.02961148
Iteration 12, loss = 6.02992221
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.8585056800337765, 'fit': 0.016500000000000001, 'time': 156.20000000000073})
Iteration 1, loss = 6.24751639
Iteration 2, loss = 6.04646145
Iteration 3, loss = 6.03721043
Iteration 4, loss = 6.03619537
Iteration 5, loss = 6.03482808
Iteration 6, loss = 6.03316377
Iteration 7, loss = 6.03805686
Iteration 8, loss = 6.03394583
Iteration 9, loss = 6.03374944
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.6503455498786526, 'fit': 0.016125, 'time': 119.01000000000204})
Iteration 1, loss = 6.24509284
Iteration 2, loss = 6.04475616
Iteration 3, loss = 6.03889856
Iteration 4, loss = 6.03574841
Iteration 5, loss = 6.03547447
Iteration 6, loss = 6.03259014
Iteration 7, loss = 6.03265840
Iteration 8, loss = 6.03413080
Iteration 9, loss = 6.03235032
Iteration 10, loss = 6.03214294
Iteration 11, loss = 6.03156997
Iteration 12, loss = 6.02854063
Iteration 13, loss = 6.02965455
Iteration 14, loss = 6.03002133
Iteration 15, loss = 6.02765086
Iteration 16, loss = 6.03040347
Iteration 17, loss = 6.02869615
Iteration 18, loss = 6.02928052
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.3401266578523576, 'fit': 0.012999999999999999, 'time': 235.97000000000116})
Iteration 1, loss = 6.24762614
Iteration 2, loss = 6.04666150
Iteration 3, loss = 6.03905390
Iteration 4, loss = 6.21880470
Iteration 5, loss = 6.04135212
Iteration 6, loss = 6.03551355
Iteration 7, loss = 6.03190174
Iteration 8, loss = 6.03309957
Iteration 9, loss = 6.03540990
Iteration 10, loss = 6.03457682
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.8585056800337765, 'fit': 0.016500000000000001, 'time': 136.61000000000058})
Iteration 1, loss = 6.24576864
Iteration 2, loss = 6.04419427
Iteration 3, loss = 6.03704708
Iteration 4, loss = 6.03654628
Iteration 5, loss = 6.03430775
Iteration 6, loss = 6.03667811
Iteration 7, loss = 6.03357946
Iteration 8, loss = 6.03175965
Iteration 9, loss = 6.03306788
Iteration 10, loss = 6.03155696
Iteration 11, loss = 6.03179257
Iteration 12, loss = 6.02946177
Iteration 13, loss = 6.03103874
Iteration 14, loss = 6.02811699
Iteration 15, loss = 6.02752514
Iteration 16, loss = 6.02797661
Iteration 17, loss = 6.02768520
Iteration 18, loss = 6.02674408
Iteration 19, loss = 6.03054289
Iteration 20, loss = 6.02805174
Iteration 21, loss = 6.02542235
Iteration 22, loss = 6.02601674
Iteration 23, loss = 6.02709578
Iteration 24, loss = 6.02444349
Iteration 25, loss = 6.02684264
Iteration 26, loss = 6.02672590
Iteration 27, loss = 6.02680217
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.4816444642968407, 'fit': 0.016250000000000001, 'time': 354.9900000000016})
Iteration 1, loss = 6.23127877
Iteration 2, loss = 6.04517723
Iteration 3, loss = 6.04040002
Iteration 4, loss = 6.03722451
Iteration 5, loss = 6.03489098
Iteration 6, loss = 6.03683741
Iteration 7, loss = 6.03500218
Iteration 8, loss = 6.03276622
Iteration 9, loss = 6.03337628
Iteration 10, loss = 6.03106843
Iteration 11, loss = 6.03047203
Iteration 12, loss = 6.03052470
Iteration 13, loss = 6.03200010
Iteration 14, loss = 6.02885662
Iteration 15, loss = 6.03161282
Iteration 16, loss = 6.02971846
Iteration 17, loss = 6.02600283
Iteration 18, loss = 6.02671551
Iteration 19, loss = 6.02695290
Iteration 20, loss = 6.02602262
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 4.1291809042091216, 'fit': 0.015375, 'time': 269.6000000000022})
Iteration 1, loss = 6.24957689
Iteration 2, loss = 6.04645733
Iteration 3, loss = 6.03645910
Iteration 4, loss = 6.03410891
Iteration 5, loss = 6.03474286
Iteration 6, loss = 6.03422912
Iteration 7, loss = 6.03053274
Iteration 8, loss = 6.03163210
Iteration 9, loss = 6.03127699
Iteration 10, loss = 6.02988614
Iteration 11, loss = 6.03320514
Iteration 12, loss = 6.02758667
Iteration 13, loss = 6.02873189
Iteration 14, loss = 6.03116354
Iteration 15, loss = 6.02882787
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.6503455498786526, 'fit': 0.016125, 'time': 206.21000000000276})
Iteration 1, loss = 6.23859489
Iteration 2, loss = 6.04502513
Iteration 3, loss = 6.03786552
Iteration 4, loss = 6.03541056
Iteration 5, loss = 6.03254419
Iteration 6, loss = 6.03400959
Iteration 7, loss = 6.03385809
Iteration 8, loss = 6.03148976
Iteration 9, loss = 6.03325264
Iteration 10, loss = 6.03204898
Iteration 11, loss = 6.03197750
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 4.1291809042091216, 'fit': 0.015375, 'time': 153.47000000000116})
Iteration 1, loss = 6.26825307
Iteration 2, loss = 6.04777754
Iteration 3, loss = 6.04051466
Iteration 4, loss = 6.03653125
Iteration 5, loss = 6.03571906
Iteration 6, loss = 6.03237340
Iteration 7, loss = 6.03364107
Iteration 8, loss = 6.02966377
Iteration 9, loss = 6.03298706
Iteration 10, loss = 6.03278654
Iteration 11, loss = 6.03269831
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.6503455498786526, 'fit': 0.016125, 'time': 155.55999999999767})
Iteration 1, loss = 6.25165942
Iteration 2, loss = 6.04588007
Iteration 3, loss = 6.03780771
Iteration 4, loss = 6.03611228
Iteration 5, loss = 6.03359037
Iteration 6, loss = 6.03550596
Iteration 7, loss = 6.03265333
Iteration 8, loss = 6.03129169
Iteration 9, loss = 6.03032340
Iteration 10, loss = 6.03231978
Iteration 11, loss = 6.02947905
Iteration 12, loss = 6.02985975
Iteration 13, loss = 6.02958933
Iteration 14, loss = 6.02928008
Iteration 15, loss = 6.02705922
Iteration 16, loss = 6.02944560
Iteration 17, loss = 6.02668354
Iteration 18, loss = 6.02625047
Iteration 19, loss = 6.02730199
Iteration 20, loss = 6.02861825
Iteration 21, loss = 6.02682229
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.4816444642968407, 'fit': 0.016250000000000001, 'time': 293.40000000000146})
Iteration 1, loss = 6.24535988
Iteration 2, loss = 6.04546510
Iteration 3, loss = 6.03864838
Iteration 4, loss = 6.03678085
Iteration 5, loss = 6.03492668
Iteration 6, loss = 6.03239826
Iteration 7, loss = 6.03395534
Iteration 8, loss = 6.03404521
Iteration 9, loss = 6.02965228
Iteration 10, loss = 6.03310128
Iteration 11, loss = 6.03108479
Iteration 12, loss = 6.03118781
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.8585056800337765, 'fit': 0.016500000000000001, 'time': 170.7400000000016})
Iteration 1, loss = 6.24822571
Iteration 2, loss = 6.11689018
Iteration 3, loss = 6.03878313
Iteration 4, loss = 6.03622602
Iteration 5, loss = 6.03634779
Iteration 6, loss = 6.03388286
Iteration 7, loss = 6.03249970
Iteration 8, loss = 6.03332329
Iteration 9, loss = 6.03093775
Iteration 10, loss = 6.03540782
Iteration 11, loss = 6.03173652
Iteration 12, loss = 6.03099255
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.6503455498786526, 'fit': 0.016125, 'time': 176.61999999999898})
Iteration 1, loss = 6.23903274
Iteration 2, loss = 6.04669277
Iteration 3, loss = 6.03843438
Iteration 4, loss = 6.04152160
Iteration 5, loss = 6.12975114
Iteration 6, loss = 6.05629839
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.6503455498786526, 'fit': 0.016125, 'time': 88.54000000000087})
Iteration 1, loss = 6.23543851
Iteration 2, loss = 6.04542059
Iteration 3, loss = 6.03955757
Iteration 4, loss = 6.03618688
Iteration 5, loss = 6.03594680
Iteration 6, loss = 6.03527221
Iteration 7, loss = 6.03352612
Iteration 8, loss = 6.03303365
Iteration 9, loss = 6.03250753
Iteration 10, loss = 6.03300591
Iteration 11, loss = 6.03065533
Iteration 12, loss = 6.03001406
Iteration 13, loss = 6.02898791
Iteration 14, loss = 6.02933960
Iteration 15, loss = 6.02788712
Iteration 16, loss = 6.02817544
Iteration 17, loss = 6.02751243
Iteration 18, loss = 6.02676614
Iteration 19, loss = 6.02815997
Iteration 20, loss = 6.02639441
Iteration 21, loss = 6.02868576
Iteration 22, loss = 6.02780687
Iteration 23, loss = 6.02672281
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.8585056800337765, 'fit': 0.016500000000000001, 'time': 334.9599999999991})
Iteration 1, loss = 6.27062146
Iteration 2, loss = 6.04429850
Iteration 3, loss = 6.03547644
Iteration 4, loss = 6.03857133
Iteration 5, loss = 6.03462982
Iteration 6, loss = 6.03138396
Iteration 7, loss = 6.03423375
Iteration 8, loss = 6.03071636
Iteration 9, loss = 6.03036908
Iteration 10, loss = 6.03098243
Iteration 11, loss = 6.02986770
Iteration 12, loss = 6.02825276
Iteration 13, loss = 6.02912194
Iteration 14, loss = 6.02666264
Iteration 15, loss = 6.02920456
Iteration 16, loss = 6.02633580
Iteration 17, loss = 6.02659428
Iteration 18, loss = 6.02867250
Iteration 19, loss = 6.02868362
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.6503455498786526, 'fit': 0.016125, 'time': 280.52000000000044})
Iteration 1, loss = 6.25577851
Iteration 2, loss = 6.04402650
Iteration 3, loss = 6.04195619
Iteration 4, loss = 6.03647189
Iteration 5, loss = 6.03486425
Iteration 6, loss = 6.03328373
Iteration 7, loss = 6.03346746
Iteration 8, loss = 6.02961572
Iteration 9, loss = 6.03083707
Iteration 10, loss = 6.03317302
Iteration 11, loss = 6.03001265
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 4.1291809042091216, 'fit': 0.015375, 'time': 168.44000000000233})
Iteration 1, loss = 6.34248533
Iteration 2, loss = 6.05571208
Iteration 3, loss = 6.04187843
Iteration 4, loss = 6.03754277
Iteration 5, loss = 6.03383331
Iteration 6, loss = 6.03464623
Iteration 7, loss = 6.03433819
Iteration 8, loss = 6.03413005
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.8585056800337765, 'fit': 0.016500000000000001, 'time': 125.70000000000073})
Iteration 1, loss = 6.23766766
Iteration 2, loss = 6.04381286
Iteration 3, loss = 6.03639364
Iteration 4, loss = 6.03644163
Iteration 5, loss = 6.03436488
Iteration 6, loss = 6.03200813
Iteration 7, loss = 6.02998408
Iteration 8, loss = 6.03115203
Iteration 9, loss = 6.03113427
Iteration 10, loss = 6.03248819
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.6503455498786526, 'fit': 0.016125, 'time': 155.28000000000247})
Iteration 1, loss = 6.25386578
Iteration 2, loss = 6.04911007
Iteration 3, loss = 6.04058526
Iteration 4, loss = 6.03599502
Iteration 5, loss = 6.03425041
Iteration 6, loss = 6.03347440
Iteration 7, loss = 6.03364571
Iteration 8, loss = 6.03245480
Iteration 9, loss = 6.03066314
Iteration 10, loss = 6.02850510
Iteration 11, loss = 6.03239902
Iteration 12, loss = 6.03094712
Iteration 13, loss = 6.02982601
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
((100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100), {'error': 3.6503455498786526, 'fit': 0.016125, 'time': 203.7699999999968})
