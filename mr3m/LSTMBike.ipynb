{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, TimeDistributed\n",
    "from keras.layers import LSTM, SimpleRNN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy as scipy\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.regularizers import l1, activity_l1\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from sklearn import preprocessing  \n",
    "#from datetime \n",
    "import datetime \n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "import pickle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../train.csv\")\n",
    "df_test = pd.read_csv(\"../test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train['month'] = pd.DatetimeIndex(df_train.datetime).month\n",
    "df_train['day'] = pd.DatetimeIndex(df_train.datetime).dayofweek\n",
    "df_train['hour'] = pd.DatetimeIndex(df_train.datetime).hour\n",
    "df_train['year'] = pd.DatetimeIndex(df_train.datetime).year\n",
    "\n",
    "df_test['month'] = pd.DatetimeIndex(df_test.datetime).month\n",
    "df_test['day'] = pd.DatetimeIndex(df_test.datetime).dayofweek\n",
    "df_test['hour'] = pd.DatetimeIndex(df_test.datetime).hour\n",
    "df_test['year'] = pd.DatetimeIndex(df_test.datetime).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine train and test data into one df\n",
    "df = df_train.append(df_test)\n",
    "\n",
    "# lowercase column names\n",
    "#df.columns = map(str.lower, df.columns)\n",
    "\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df.index = df['datetime']\n",
    "\n",
    "#df['month'] = pd.DatetimeIndex(df.datetime).month\n",
    "#df['day'] = pd.DatetimeIndex(df.datetime).dayofweek\n",
    "#df['hour'] = pd.DatetimeIndex(df.datetime).hour\n",
    "#df['year'] = pd.DatetimeIndex(df.datetime).year\n",
    "\n",
    "df = df.drop('datetime', axis=1)\n",
    "drange = pd.date_range('2011-01-01 00:00:00', '2012-12-31 23:00:00', freq='H')\n",
    "df = df.reindex(drange)\n",
    "\n",
    "df = df[df_train.drop(['datetime', 'casual', 'registered','count'],axis=1).columns]\n",
    "\n",
    "# parse datetime colum & add new time related columns\n",
    "#dt = pd.DatetimeIndex(df['datetime'])\n",
    "#df.set_index(dt, inplace=True)\n",
    "\n",
    "dt = pd.DatetimeIndex(df_train['datetime'])\n",
    "df_train.set_index(dt, inplace=True)\n",
    "dt = pd.DatetimeIndex(df_test['datetime'])\n",
    "df_test.set_index(dt, inplace=True)\n",
    "\n",
    "df[\"weather\"] = df[\"weather\"].interpolate(method='time').apply(np.round)\n",
    "df[\"temp\"] = df[\"temp\"].interpolate(method='time')\n",
    "df[\"atemp\"] = df[\"atemp\"].interpolate(method='time')\n",
    "df[\"humidity\"] = df[\"humidity\"].interpolate(method='time').apply(np.round)\n",
    "df[\"windspeed\"] = df[\"windspeed\"].interpolate(method='time')\n",
    "\n",
    "df[\"season\"] = df[\"season\"].interpolate(method='time').apply(np.round)\n",
    "df[\"holiday\"] = df[\"holiday\"].interpolate(method='time').apply(np.round)\n",
    "df[\"workingday\"] = df[\"workingday\"].interpolate(method='time').apply(np.round)\n",
    "\n",
    "df[\"month\"] = df[\"month\"].interpolate(method='time').apply(np.round)\n",
    "df[\"day\"] = df[\"day\"].interpolate(method='time').apply(np.round)\n",
    "df[\"hour\"] = df[\"hour\"].interpolate(method='time').apply(np.round)\n",
    "df[\"year\"] = df[\"year\"].interpolate(method='time').apply(np.round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_day(day_start):\n",
    "    day_end = day_start + pd.offsets.DateOffset(hours=23)\n",
    "    return pd.date_range(day_start, day_end, freq=\"H\")\n",
    "\n",
    "# tax day\n",
    "df_train.loc[get_day(pd.datetime(2011, 4, 15)), \"workingday\"] = 1\n",
    "df_train.loc[get_day(pd.datetime(2012, 4, 16)), \"workingday\"] = 1\n",
    "# thanksgiving friday\n",
    "df_test.loc[get_day(pd.datetime(2011, 11, 25)), \"workingday\"] = 0\n",
    "df_test.loc[get_day(pd.datetime(2012, 11, 23)), \"workingday\"] = 0\n",
    "# tax day\n",
    "df_train.loc[get_day(pd.datetime(2011, 4, 15)), \"holiday\"] = 0\n",
    "df_train.loc[get_day(pd.datetime(2012, 4, 16)), \"holiday\"] = 0\n",
    "\n",
    "# thanksgiving friday\n",
    "df_test.loc[get_day(pd.datetime(2011, 11, 25)), \"holiday\"] = 1\n",
    "df_test.loc[get_day(pd.datetime(2012, 11, 23)), \"holiday\"] = 1\n",
    "\n",
    "#storms\n",
    "df_test.loc[get_day(pd.datetime(2012, 5, 21)), \"holiday\"] = 1\n",
    "#tornado\n",
    "df_train.loc[get_day(pd.datetime(2012, 6, 1)), \"holiday\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_train = df_train.drop(['datetime','casual','registered','count'], axis = 1)\n",
    "#df_train_train = df_train_train.drop(['season','workingday','weather','holiday'], axis = 1)\n",
    "df_test_train = df_train['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df_test_train.values.astype('float32').reshape(-1,1))\n",
    "scalerx = StandardScaler()\n",
    "scalerx.fit(df_train_train.values.astype('float32'))\n",
    "df = df.apply(lambda x: scalerx.fit_transform(x.reshape(-1,1)).reshape(1,-1)[0] )\n",
    "timesteps = 2\n",
    "def preprocess_nn(datax, datay):\n",
    "    indexx = pd.DatetimeIndex(datax['datetime']).day \n",
    "    testindex = indexx == 1#np.logical_or(np.logical_or(indexx == 19,indexx == 18),np.logical_or(indexx == 17, indexx == 16))\n",
    "    datax[datax.drop('datetime',axis=1).columns] = datax[datax.drop('datetime',axis=1).columns].apply(lambda x: scalerx.fit_transform(x.reshape(-1,1)).reshape(1,-1)[0] )\n",
    "    datay[datay.drop('datetime',axis=1).columns] = scaler.transform(datay[datay.drop('datetime',axis=1).columns])\n",
    "    #train_Y_n = scaler.transform(datay.reshape(-1,1))\n",
    "    train_x = datax[~testindex]#.astype('float32')\n",
    "    train_y = datay[~testindex]#.astype('float32')\n",
    "    test_x = datax[testindex]#.astype('float32')\n",
    "    test_y = datay[testindex]#.astype('float32')\n",
    "    return train_x, train_y, test_x, test_y\n",
    "#def preshape_nn(data_x):\n",
    "#    data_x = numpy.reshape(data_x, (data_x.shape[0]/timesteps, timesteps, data_x.shape[1]))\n",
    "#    return data_x\n",
    "def preshape_nn(data_set, look_back = timesteps):\n",
    "    out1= []\n",
    "    for val in range(len(data_set) - look_back + 1):\n",
    "        end1 = datetime.datetime.strptime(data_set['datetime'].values[look_back - 1 + val],\"%Y-%m-%d %H:%M:%S\" )\n",
    "        timesteps_array = []\n",
    "        for it in range(look_back): \n",
    "            timesteps_array.append(np.datetime64(end1))\n",
    "            end1 = end1 - datetime.timedelta(hours = 1)\n",
    "        timesteps_array = timesteps_array[::-1]\n",
    "        timestep_unit = df.loc[timesteps_array].values.astype('float32')\n",
    "        #if val == 0:\n",
    "         #   out = timestep_unit\n",
    "        #    print out.shape\n",
    "        #timestep_unit = np.array(timestep_unit,type('float32'))\n",
    "        #out = np.stack([out,timestep_unit], axis=0)\n",
    "        out1.append(timestep_unit)\n",
    "    datax = np.stack(out1, axis=0)\n",
    "    return datax\n",
    "def preshape_nn_test(data_set, look_back = timesteps):\n",
    "    out1= []\n",
    "    for val in range(len(data_set)):\n",
    "        end1 = datetime.datetime.strptime(data_set['datetime'].values[val],\"%Y-%m-%d %H:%M:%S\" )\n",
    "        timesteps_array = []\n",
    "        for it in range(look_back): \n",
    "            timesteps_array.append(np.datetime64(end1))\n",
    "            end1 = end1 - datetime.timedelta(hours = 1)\n",
    "        timesteps_array = timesteps_array[::-1]\n",
    "        timestep_unit = df.loc[timesteps_array].values.astype('float32')\n",
    "        #if val == 0:\n",
    "         #   out = timestep_unit\n",
    "        #    print out.shape\n",
    "        #timestep_unit = np.array(timestep_unit,type('float32'))\n",
    "        #out = np.stack([out,timestep_unit], axis=0)\n",
    "        out1.append(timestep_unit)\n",
    "    datax = np.stack(out1, axis=0)\n",
    "    return datax\n",
    "def preshape_nn_y(data_set, look_back = timesteps ):\n",
    "    return data_set.drop('datetime',axis = 1).values[look_back - 1:]\n",
    "def learn_nn(train_x, train_y, test_x, test_y, string):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM, Dense\n",
    "    import numpy as np    \n",
    "\n",
    "    data_dim = train_x.shape[-1] -1\n",
    "    \n",
    "    #LSTM\n",
    "    # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(100))\n",
    "    r = 0.00001\n",
    "    model.add(LSTM(256, return_sequences=True,\n",
    "                   input_shape=(timesteps, data_dim)))#, W_regularizer=l2(r)))\n",
    "    #model.add(LSTM(50, return_sequences=True))#, W_regularizer=l2(r)))\n",
    "    #model.add(LSTM(25 return_sequences=True))#, W_regularizer=l2(r)))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(LSTM(16, return_sequences=True))\n",
    "    model.add(LSTM(8, return_sequences=True)) \n",
    "    model.add(LSTM(4, return_sequences=True))\n",
    "    model.add(LSTM(2))  \n",
    "    model.add(Dense(1, activation='linear'))    \n",
    "    \n",
    "    \n",
    "    #####\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer='rmsprop')\n",
    "    \n",
    "    x_train = preshape_nn(train_x)\n",
    "    y_train  = preshape_nn_y(train_y) #scaler.transform(train_y)\n",
    "\n",
    "\n",
    "    x_val = preshape_nn(test_x)\n",
    "    y_val = preshape_nn_y(test_y)#scaler.transform(test_y)\n",
    "    # generate dummy validation data\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    print now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=100, nb_epoch=100, verbose = 0,\n",
    "              validation_data=(x_val, y_val),\n",
    "              callbacks=[TensorBoard(log_dir='logs1/' + now.strftime(\"%Y%m%d-%H%M%S\") + string + \"/\")])\n",
    "    model.save(\"models/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \".h5\")\n",
    "    return model\n",
    "def model_eval_nn(model, prepro):\n",
    "    trainScore = math.sqrt(mean_squared_error(scaler.inverse_transform(model.predict(preshape_nn(prepro[0]))), scaler.inverse_transform(preshape_nn_y(prepro[1]))))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore =  math.sqrt(mean_squared_error(scaler.inverse_transform(model.predict(preshape_nn(prepro[2]))), scaler.inverse_transform(preshape_nn_y(prepro[3]))))\n",
    "    print('Test Score: %.2f RMSE' % (testScore)) \n",
    "    plt.plot(scaler.inverse_transform(model.predict(preshape_nn(prepro[2]))).reshape(-1,1) - scaler.inverse_transform(preshape_nn_y(prepro[3])).reshape(-1,1),'.')\n",
    "def get_rmsle(model, prepro):\n",
    "    diff = pd.DataFrame(np.log(scaler.inverse_transform(model.predict(preshape_nn(prepro[2]))).astype('float32') + 1) - np.log(scaler.inverse_transform(preshape_nn_y(prepro[3])).astype('float32') + 1))\n",
    "    mean_error = np.square(diff).mean()\n",
    "    return np.sqrt(mean_error).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/root/anaconda2/lib/python2.7/site-packages/pandas/core/indexing.py:465: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "datax = df_train.drop(['casual','registered','count'], axis = 1)\n",
    "datay = df_train[['datetime','count']]\n",
    "pre1 = preprocess_nn(datax,datay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20170114-171126\n"
     ]
    }
   ],
   "source": [
    "m1 = learn_nn(pre1[0],pre1[1],pre1[2],pre1[3],'MG')\n",
    "model_eval_nn(m1,pre1)\n",
    "get_rmsle(m1,pre1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test_test = df_test.copy()#.drop(['datetime'], axis = 1)\n",
    "#df_test_test = df_test_test.drop(['season','workingday','weather','holiday'], axis = 1)\n",
    "#df_test_t = df_test_test.apply(lambda x: scalerx.fit_transform(x.reshape(-1,1)).reshape(1,-1)[0] )\n",
    "df_test_test[df_test_test.drop('datetime',axis=1).columns] = df_test_test[df_test_test.drop('datetime',axis=1).columns].apply(lambda x: scalerx.fit_transform(x.reshape(-1,1)).reshape(1,-1)[0] )\n",
    "df_test_r = preshape_nn_test(df_test_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = m1.predict(df_test_r)\n",
    "out = scaler.inverse_transform(pred).astype(int)\n",
    "#from keras.models import load_model\n",
    "#mr = load_model('models/20170113-190313.h5')\n",
    "#df_test_r_r = preshape_nn(df_test_test,1)\n",
    "#predr = scaler.inverse_transform(mr.predict(df_test_r_r)).astype(int)[range(timesteps-1)]\n",
    "#out = np.append(predr,out)\n",
    "out[out<=0] = 0\n",
    "out = out[:,0]\n",
    "pred = pd.DataFrame({'datetime': df_test['datetime'],'count': out})\n",
    "pred = pred[['datetime','count']]\n",
    "pred.to_csv(\"pred.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(scaler.inverse_transform(m1.predict(preshape_nn(pre1[0])))-scaler.inverse_transform(preshape_nn_y(pre1[1])).reshape(-1,1),'.')\n",
    "print scipy.stats.pearsonr(scaler.inverse_transform(m1.predict(preshape_nn(pre1[0]))),scaler.inverse_transform(preshape_nn_y(pre1[1])).reshape(-1,1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = m1\n",
    "prepro = pre1\n",
    "target = scaler.inverse_transform(model.predict(preshape_nn(prepro[2]))).reshape(-1,1) - scaler.inverse_transform(preshape_nn_y(prepro[3])).reshape(-1,1)\n",
    "target = target.reshape(1,-1)\n",
    "target = target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepro[2]['datetime'][abs(target)>200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#indexx = pd.DatetimeIndex(df_train['datetime'][df_train['year'] == 2011]).day \n",
    "#testindex = np.logical_or(np.logical_or(indexx == 19,indexx == 18),np.logical_or(indexx == 17, indexx == 16))\n",
    "pre2 = preprocess_nn(df_train.drop(['casual','registered','count'], axis = 1)[df_train['year'] == 2011],df_train[['datetime','casual']][df_train['year'] == 2011])\n",
    "m2 = learn_nn(pre2[0],pre2[1],pre2[2],pre2[3],'MAC')\n",
    "model_eval_nn(m2,pre2)\n",
    "get_rmsle(m2,pre2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre3 = preprocess_nn(df_train.drop(['casual','registered','count'], axis = 1)[df_train['year'] == 2011],df_train[['datetime','registered']][df_train['year'] == 2011])\n",
    "m3 = learn_nn(pre3[0],pre3[1],pre3[2],pre3[3],'MAR')\n",
    "model_eval_nn(m3,pre3)\n",
    "get_rmsle(m3,pre3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#indexx = pd.DatetimeIndex(df_train['datetime'][df_train['year'] == 2012]).day \n",
    "#testindex = np.logical_or(np.logical_or(indexx == 19,indexx == 18),np.logical_or(indexx == 17, indexx == 16))\n",
    "pre4 = preprocess_nn(df_train.drop(['casual','registered','count'], axis = 1)[df_train['year'] == 2012],df_train[['datetime','casual']][df_train['year'] == 2012])\n",
    "m4 = learn_nn(pre4[0],pre4[1],pre4[2],pre4[3],'MBC')\n",
    "model_eval_nn(m4,pre4)\n",
    "get_rmsle(m4,pre4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre5 = preprocess_nn(df_train.drop(['casual','registered','count'], axis = 1)[df_train['year'] == 2012],df_train[['datetime','registered']][df_train['year'] == 2012])\n",
    "m5 = learn_nn(pre5[0],pre5[1],pre5[2],pre5[3],'MBR')\n",
    "model_eval_nn(m5,pre5)\n",
    "get_rmsle(m5,pre5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out2 = scaler.inverse_transform(m2.predict(preshape_nn_test(df_test[df_test['year']==2011]))).astype(int)\n",
    "out3 = scaler.inverse_transform(m3.predict(preshape_nn_test(df_test[df_test['year']==2011]))).astype(int)\n",
    "out4 = scaler.inverse_transform(m4.predict(preshape_nn_test(df_test[df_test['year']==2012]))).astype(int)\n",
    "out5 = scaler.inverse_transform(m5.predict(preshape_nn_test(df_test[df_test['year']==2012]))).astype(int)\n",
    "out2[out2<0] = 0\n",
    "out3[out3<0] = 0\n",
    "out4[out4<0] = 0\n",
    "out5[out5<0] = 0\n",
    "out2 = out2[:,0]\n",
    "out3 = out3[:,0]\n",
    "out4 = out4[:,0]\n",
    "out5 = out5[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outb = np.concatenate((np.array(out2 + out3),np.array(out4 + out5)), axis = 0)\n",
    "pred = pd.DataFrame({'datetime': df_test['datetime'],'count': outb})\n",
    "pred = pred[['datetime','count']]\n",
    "pred.to_csv(\"pred2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(outb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(out-outb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
