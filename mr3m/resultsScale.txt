Iteration 1, loss = 1735.37923220
Iteration 2, loss = 1699.22726041
Iteration 3, loss = 1659.76708314
Iteration 4, loss = 1622.81513142
Iteration 5, loss = 1594.69519825
Iteration 6, loss = 1572.64462517
Iteration 7, loss = 1553.86686909
Iteration 8, loss = 1536.82303110
Iteration 9, loss = 1520.99420731
Iteration 10, loss = 1506.06206430
Iteration 11, loss = 1491.82921909
Iteration 12, loss = 1478.15580802
Iteration 13, loss = 1464.91391788
Iteration 14, loss = 1452.10841463
Iteration 15, loss = 1439.63139155
Iteration 16, loss = 1427.45307327
Iteration 17, loss = 1415.65457064
Iteration 18, loss = 1404.03794898
Iteration 19, loss = 1392.76106198
Iteration 20, loss = 1381.59219665
Iteration 21, loss = 1370.65325045
Iteration 22, loss = 1359.87774306
Iteration 23, loss = 1349.24138090
Iteration 24, loss = 1338.75972054
Iteration 25, loss = 1328.39782410
Iteration 26, loss = 1318.09627195
Iteration 27, loss = 1307.85203234
Iteration 28, loss = 1297.75676474
Iteration 29, loss = 1287.66666393
Iteration 30, loss = 1277.77023280
Iteration 31, loss = 1267.95437882
Iteration 32, loss = 1258.45638025
Iteration 33, loss = 1248.88736948
Iteration 34, loss = 1239.50295574
Iteration 35, loss = 1230.26690229
Iteration 36, loss = 1220.93870046
Iteration 37, loss = 1211.61563077
Iteration 38, loss = 1202.61588872
Iteration 39, loss = 1193.68432578
Iteration 40, loss = 1184.89269896
Iteration 41, loss = 1176.02802675
Iteration 42, loss = 1167.44978449
Iteration 43, loss = 1158.71237184
Iteration 44, loss = 1150.30731807
Iteration 45, loss = 1141.79190410
Iteration 46, loss = 1133.46011116
Iteration 47, loss = 1125.22783663
Iteration 48, loss = 1117.12713015
Iteration 49, loss = 1109.13153944
Iteration 50, loss = 1101.20514144
Iteration 51, loss = 1093.41339353
Iteration 52, loss = 1085.71658806
Iteration 53, loss = 1078.04511355
Iteration 54, loss = 1070.45330954
Iteration 55, loss = 1062.86176225
Iteration 56, loss = 1055.80428022
Iteration 57, loss = 1048.54019588
Iteration 58, loss = 1041.28820665
Iteration 59, loss = 1033.96304024
Iteration 60, loss = 1026.87155469
Iteration 61, loss = 1019.65744703
Iteration 62, loss = 1012.81792679
Iteration 63, loss = 1005.97741492
Iteration 64, loss = 999.17569001
Iteration 65, loss = 992.65726647
Iteration 66, loss = 985.98128622
Iteration 67, loss = 979.26809494
Iteration 68, loss = 972.95655439
Iteration 69, loss = 966.49623497
Iteration 70, loss = 960.27780042
Iteration 71, loss = 953.95355038
Iteration 72, loss = 948.07318453
Iteration 73, loss = 941.57141659
Iteration 74, loss = 935.69241891
Iteration 75, loss = 929.61795114
Iteration 76, loss = 924.01144172
Iteration 77, loss = 917.75770173
Iteration 78, loss = 912.27106452
Iteration 79, loss = 906.78491577
Iteration 80, loss = 900.89509869
Iteration 81, loss = 895.67639688
Iteration 82, loss = 889.82765350
Iteration 83, loss = 884.21702757
Iteration 84, loss = 879.36002694
Iteration 85, loss = 873.54147810
Iteration 86, loss = 868.54828143
Iteration 87, loss = 863.59828535
Iteration 88, loss = 858.42707162
Iteration 89, loss = 852.89374887
Iteration 90, loss = 847.72166588
Iteration 91, loss = 842.90585180
Iteration 92, loss = 838.03318679
Iteration 93, loss = 832.60757031
Iteration 94, loss = 827.89592172
Iteration 95, loss = 823.15345301
Iteration 96, loss = 818.85275487
Iteration 97, loss = 813.93860515
Iteration 98, loss = 809.21826123
Iteration 99, loss = 804.72407948
Iteration 100, loss = 799.81175975
Iteration 101, loss = 795.37565290
Iteration 102, loss = 790.46115739
Iteration 103, loss = 786.97332521
Iteration 104, loss = 782.00575461
Iteration 105, loss = 777.66348481
Iteration 106, loss = 773.38241256
Iteration 107, loss = 768.59249034
Iteration 108, loss = 764.29868299
Iteration 109, loss = 760.50470475
Iteration 110, loss = 756.20483840
Iteration 111, loss = 752.05261524
Iteration 112, loss = 748.28515021
Iteration 113, loss = 743.59714549
Iteration 114, loss = 739.60759100
Iteration 115, loss = 735.86237179
Iteration 116, loss = 731.78615544
Iteration 117, loss = 727.66386946
Iteration 118, loss = 724.34640930
Iteration 119, loss = 720.68063321
Iteration 120, loss = 716.32286205
Iteration 121, loss = 712.67471506
Iteration 122, loss = 708.78957382
Iteration 123, loss = 706.34690644
Iteration 124, loss = 701.95420409
Iteration 125, loss = 698.24277322
Iteration 126, loss = 694.78754233
Iteration 127, loss = 691.52389125
Iteration 128, loss = 688.17251493
Iteration 129, loss = 684.49207579
Iteration 130, loss = 681.03781658
Iteration 131, loss = 677.81884706
Iteration 132, loss = 674.05947743
Iteration 133, loss = 671.30505040
Iteration 134, loss = 667.41558142
Iteration 135, loss = 665.09861924
Iteration 136, loss = 661.26035557
Iteration 137, loss = 657.66230827
Iteration 138, loss = 654.36940001
Iteration 139, loss = 651.45696836
Iteration 140, loss = 647.86812505
Iteration 141, loss = 645.08685144
Iteration 142, loss = 642.68968713
Iteration 143, loss = 639.23965443
Iteration 144, loss = 636.18838195
Iteration 145, loss = 632.80178531
Iteration 146, loss = 629.92254297
Iteration 147, loss = 627.05681630
Iteration 148, loss = 624.35720118
Iteration 149, loss = 621.67170006
Iteration 150, loss = 618.14737052
Iteration 151, loss = 615.36238979
Iteration 152, loss = 612.53896955
Iteration 153, loss = 609.55367772
Iteration 154, loss = 606.65450430
Iteration 155, loss = 604.62130551
Iteration 156, loss = 602.27683714
Iteration 157, loss = 598.58214688
Iteration 158, loss = 595.70817601
Iteration 159, loss = 592.90119904
Iteration 160, loss = 590.54517352
Iteration 161, loss = 587.81395430
Iteration 162, loss = 584.54703497
Iteration 163, loss = 581.75562393
Iteration 164, loss = 579.22660627
Iteration 165, loss = 576.37828905
Iteration 166, loss = 575.70105434
Iteration 167, loss = 571.39750439
Iteration 168, loss = 569.36611112
Iteration 169, loss = 567.07832424
Iteration 170, loss = 563.87305072
Iteration 171, loss = 561.31037723
Iteration 172, loss = 559.66899011
Iteration 173, loss = 556.30788097
Iteration 174, loss = 553.53511519
Iteration 175, loss = 552.35146667
Iteration 176, loss = 548.89015756
Iteration 177, loss = 546.24304375
Iteration 178, loss = 544.39035210
Iteration 179, loss = 540.67037501
Iteration 180, loss = 539.27482174
Iteration 181, loss = 536.56799076
Iteration 182, loss = 534.86553553
Iteration 183, loss = 531.26709106
Iteration 184, loss = 529.08243408
Iteration 185, loss = 526.17289907
Iteration 186, loss = 525.28166824
Iteration 187, loss = 522.02054636
Iteration 188, loss = 520.02849205
Iteration 189, loss = 517.10890522
Iteration 190, loss = 514.41499020
Iteration 191, loss = 511.77037221
Iteration 192, loss = 509.05807995
Iteration 193, loss = 507.06677887
Iteration 194, loss = 504.06286265
Iteration 195, loss = 502.46472718
Iteration 196, loss = 500.39272668
Iteration 197, loss = 498.11281258
Iteration 198, loss = 495.45265280
Iteration 199, loss = 493.66918105
Iteration 200, loss = 492.23105090
Iteration 201, loss = 488.16914754
Iteration 202, loss = 486.15817819
Iteration 203, loss = 484.07152114
Iteration 204, loss = 482.26972923
Iteration 205, loss = 479.46577007
Iteration 206, loss = 477.90575587
Iteration 207, loss = 476.43397878
Iteration 208, loss = 474.27235777
Iteration 209, loss = 471.78384441
Iteration 210, loss = 469.12053843
Iteration 211, loss = 467.76988516
Iteration 212, loss = 465.17989834
Iteration 213, loss = 463.54547522
Iteration 214, loss = 462.05716133
Iteration 215, loss = 459.79842341
Iteration 216, loss = 458.25490586
Iteration 217, loss = 455.27230467
Iteration 218, loss = 453.10124812
Iteration 219, loss = 450.48221127
Iteration 220, loss = 449.33746649
Iteration 221, loss = 449.24956698
Iteration 222, loss = 446.53685033
Iteration 223, loss = 444.40337530
Iteration 224, loss = 442.77118333
Iteration 225, loss = 439.42093081
Iteration 226, loss = 437.72023269
Iteration 227, loss = 435.79717273
Iteration 228, loss = 435.10076481
Iteration 229, loss = 432.45070561
Iteration 230, loss = 430.33263255
Iteration 231, loss = 429.31128299
Iteration 232, loss = 426.75777807
Iteration 233, loss = 424.64927207
Iteration 234, loss = 423.07258228
Iteration 235, loss = 421.66056912
Iteration 236, loss = 419.39729610
Iteration 237, loss = 419.34316788
Iteration 238, loss = 416.76564918
Iteration 239, loss = 414.55804372
Iteration 240, loss = 413.02178746
Iteration 241, loss = 411.01222814
Iteration 242, loss = 409.13249465
Iteration 243, loss = 408.08856335
Iteration 244, loss = 406.14044174
Iteration 245, loss = 404.78303234
Iteration 246, loss = 403.51215330
Iteration 247, loss = 402.37801697
Iteration 248, loss = 399.68549746
Iteration 249, loss = 398.48489091
Iteration 250, loss = 398.11304847
Iteration 251, loss = 396.85173956
Iteration 252, loss = 392.46384516
Iteration 253, loss = 391.99586305
Iteration 254, loss = 389.80153087
Iteration 255, loss = 390.25700487
Iteration 256, loss = 388.69509431
Iteration 257, loss = 387.80128704
Iteration 258, loss = 384.53764469
Iteration 259, loss = 382.69852524
Iteration 260, loss = 381.97037982
Iteration 261, loss = 381.19558270
Iteration 262, loss = 378.55184306
Iteration 263, loss = 377.04995105
Iteration 264, loss = 377.94933321
Iteration 265, loss = 373.67651930
Iteration 266, loss = 372.51099004
Iteration 267, loss = 371.86414493
Iteration 268, loss = 370.01562921
Iteration 269, loss = 369.18540512
Iteration 270, loss = 367.94941433
Iteration 271, loss = 365.77181330
Iteration 272, loss = 365.11320681
Iteration 273, loss = 365.03484573
Iteration 274, loss = 362.08577755
Iteration 275, loss = 361.01474865
Iteration 276, loss = 358.89028407
Iteration 277, loss = 360.02177563
Iteration 278, loss = 358.21782361
Iteration 279, loss = 357.28362234
Iteration 280, loss = 353.83402440
Iteration 281, loss = 354.29497054
Iteration 282, loss = 351.22110673
Iteration 283, loss = 350.45027592
Iteration 284, loss = 349.38653071
Iteration 285, loss = 347.38237215
Iteration 286, loss = 346.97889718
Iteration 287, loss = 345.75961796
Iteration 288, loss = 344.90303645
Iteration 289, loss = 346.93530184
Iteration 290, loss = 341.65641000
Iteration 291, loss = 342.86247288
Iteration 292, loss = 340.14792381
Iteration 293, loss = 338.65064760
Iteration 294, loss = 337.22998531
Iteration 295, loss = 337.56140619
Iteration 296, loss = 336.66780936
Iteration 297, loss = 335.51304973
Iteration 298, loss = 334.92985328
Iteration 299, loss = 332.07960246
Iteration 300, loss = 330.69471305
Iteration 301, loss = 330.54080243
Iteration 302, loss = 329.58697066
Iteration 303, loss = 327.60516361
Iteration 304, loss = 326.59096256
Iteration 305, loss = 326.51852908
Iteration 306, loss = 326.91640332
Iteration 307, loss = 325.17326286
Iteration 308, loss = 322.75275359
Iteration 309, loss = 321.11614540
Iteration 310, loss = 321.46655450
Iteration 311, loss = 321.47184761
Iteration 312, loss = 318.10013194
Iteration 313, loss = 318.07150899
Iteration 314, loss = 319.53415539
Iteration 315, loss = 316.10899763
Iteration 316, loss = 315.48175635
Iteration 317, loss = 316.15496451
Iteration 318, loss = 313.48593149
Iteration 319, loss = 312.33972697
Iteration 320, loss = 315.09936203
Iteration 321, loss = 310.69647127
Iteration 322, loss = 307.96449187
Iteration 323, loss = 308.21839561
Iteration 324, loss = 307.38271235
Iteration 325, loss = 305.70829896
Iteration 326, loss = 307.97271169
Iteration 327, loss = 305.15402313
Iteration 328, loss = 305.34269239
Iteration 329, loss = 302.59102757
Iteration 330, loss = 302.04034471
Iteration 331, loss = 302.84678369
Iteration 332, loss = 302.08274711
Iteration 333, loss = 300.78333936
Iteration 334, loss = 300.07369694
Iteration 335, loss = 300.37343677
Iteration 336, loss = 296.84257782
Iteration 337, loss = 295.50632727
Iteration 338, loss = 295.69439075
Iteration 339, loss = 296.29539547
Iteration 340, loss = 294.31084798
Iteration 341, loss = 293.81540206
Iteration 342, loss = 292.94767642
Iteration 343, loss = 294.09799709
Iteration 344, loss = 292.23305538
Iteration 345, loss = 291.32720216
Iteration 346, loss = 289.91539751
Iteration 347, loss = 290.44979739
Iteration 348, loss = 289.67890656
Iteration 349, loss = 290.95508417
Iteration 350, loss = 287.38053023
Iteration 351, loss = 287.32262105
Iteration 352, loss = 285.68698781
Iteration 353, loss = 286.69541222
Iteration 354, loss = 287.64058522
Iteration 355, loss = 285.66227071
Iteration 356, loss = 282.92866141
Iteration 357, loss = 285.24071650
Iteration 358, loss = 284.16096423
Iteration 359, loss = 282.96288475
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
{'ann': MLPRegressor(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(10, 10), learning_rate='adaptive',
       learning_rate_init=0.001, max_iter=10000, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=None,
       shuffle=True, solver='adam', tol=1e-30, validation_fraction=0.1,
       verbose=True, warm_start=False), 'error': 0.68244761216916339, 'fit': 0.75679373319321797, 'time': 6.800000000000001}
Iteration 1, loss = 19020.30249744
Iteration 2, loss = 18789.65747335
Iteration 3, loss = 18628.35412121
Iteration 4, loss = 18515.38942063
Iteration 5, loss = 18419.61177608
Iteration 6, loss = 18332.73249449
Iteration 7, loss = 18251.94659256
Iteration 8, loss = 18174.98582640
Iteration 9, loss = 18101.09322077
Iteration 10, loss = 18029.10896177
Iteration 11, loss = 17959.07961663
Iteration 12, loss = 17890.39414283
Iteration 13, loss = 17822.78251741
Iteration 14, loss = 17756.37333034
Iteration 15, loss = 17690.60494928
Iteration 16, loss = 17625.72291787
Iteration 17, loss = 17561.62881904
Iteration 18, loss = 17498.01173548
Iteration 19, loss = 17435.19546057
Iteration 20, loss = 17372.96549046
Iteration 21, loss = 17311.07941368
Iteration 22, loss = 17249.97041748
Iteration 23, loss = 17189.18925738
Iteration 24, loss = 17128.93441151
Iteration 25, loss = 17069.10427916
Iteration 26, loss = 17009.75602768
Iteration 27, loss = 16950.81145659
Iteration 28, loss = 16892.29312013
Iteration 29, loss = 16834.18220216
Iteration 30, loss = 16776.23142439
Iteration 31, loss = 16718.76515027
Iteration 32, loss = 16661.69260535
Iteration 33, loss = 16604.86634449
Iteration 34, loss = 16548.55271234
Iteration 35, loss = 16492.41347727
Iteration 36, loss = 16436.74135592
Iteration 37, loss = 16381.29852355
Iteration 38, loss = 16326.30328673
Iteration 39, loss = 16271.66110194
Iteration 40, loss = 16217.04365945
Iteration 41, loss = 16162.90416834
Iteration 42, loss = 16109.19039375
Iteration 43, loss = 16055.61505361
Iteration 44, loss = 16002.28481144
Iteration 45, loss = 15949.36839957
Iteration 46, loss = 15896.60475132
Iteration 47, loss = 15844.41879356
Iteration 48, loss = 15792.26214781
Iteration 49, loss = 15740.71845503
Iteration 50, loss = 15689.03914718
Iteration 51, loss = 15637.89358324
Iteration 52, loss = 15587.02256179
Iteration 53, loss = 15536.34728325
Iteration 54, loss = 15486.06102027
Iteration 55, loss = 15436.02684678
Iteration 56, loss = 15386.07591340
Iteration 57, loss = 15336.54937663
Iteration 58, loss = 15287.08662340
Iteration 59, loss = 15238.01772358
Iteration 60, loss = 15189.15757537
Iteration 61, loss = 15140.62006156
Iteration 62, loss = 15092.17483810
Iteration 63, loss = 15044.09434728
Iteration 64, loss = 14996.24361606
Iteration 65, loss = 14948.77168861
Iteration 66, loss = 14901.37871644
Iteration 67, loss = 14854.32691047
Iteration 68, loss = 14807.45737897
Iteration 69, loss = 14760.88016878
Iteration 70, loss = 14714.52955129
Iteration 71, loss = 14668.27262776
Iteration 72, loss = 14622.38523816
Iteration 73, loss = 14576.53521749
Iteration 74, loss = 14530.42033148
Iteration 75, loss = 14483.45437714
Iteration 76, loss = 14434.42834048
Iteration 77, loss = 14383.79053390
Iteration 78, loss = 14334.10542294
Iteration 79, loss = 14285.08988590
Iteration 80, loss = 14229.64087463
Iteration 81, loss = 14173.36542975
Iteration 82, loss = 14121.81588232
Iteration 83, loss = 14072.02897074
Iteration 84, loss = 14024.66272694
Iteration 85, loss = 13975.81281194
Iteration 86, loss = 13927.58521261
Iteration 87, loss = 13879.91304559
Iteration 88, loss = 13832.08346252
Iteration 89, loss = 13784.48167020
Iteration 90, loss = 13737.66879460
Iteration 91, loss = 13690.62880224
Iteration 92, loss = 13643.37519728
Iteration 93, loss = 13598.18273034
Iteration 94, loss = 13551.64227416
Iteration 95, loss = 13505.47269020
Iteration 96, loss = 13459.05886191
Iteration 97, loss = 13413.46162362
Iteration 98, loss = 13367.81100510
Iteration 99, loss = 13322.38462907
Iteration 100, loss = 13276.76444111
Iteration 101, loss = 13231.71723679
Iteration 102, loss = 13187.51839887
Iteration 103, loss = 13143.29038373
Iteration 104, loss = 13095.83442759
Iteration 105, loss = 13048.81290339
Iteration 106, loss = 13002.82891346
Iteration 107, loss = 12955.25135488
Iteration 108, loss = 12909.74962457
Iteration 109, loss = 12863.64296249
Iteration 110, loss = 12818.99347961
Iteration 111, loss = 12773.89480021
Iteration 112, loss = 12730.52336228
Iteration 113, loss = 12686.91553094
Iteration 114, loss = 12644.88237986
Iteration 115, loss = 12600.90016604
Iteration 116, loss = 12558.49269861
Iteration 117, loss = 12516.15800725
Iteration 118, loss = 12474.33149933
Iteration 119, loss = 12432.66461486
Iteration 120, loss = 12391.01721927
Iteration 121, loss = 12350.18453487
Iteration 122, loss = 12308.21404966
Iteration 123, loss = 12267.99721445
Iteration 124, loss = 12227.03563489
Iteration 125, loss = 12187.90942047
Iteration 126, loss = 12146.35002504
Iteration 127, loss = 12105.95555826
Iteration 128, loss = 12067.72339356
Iteration 129, loss = 12026.95122424
Iteration 130, loss = 11987.66239513
Iteration 131, loss = 11947.69440224
Iteration 132, loss = 11910.59840467
Iteration 133, loss = 11868.66507452
Iteration 134, loss = 11831.21452965
Iteration 135, loss = 11791.03489216
Iteration 136, loss = 11752.67500347
Iteration 137, loss = 11714.75295821
Iteration 138, loss = 11675.80558561
Iteration 139, loss = 11638.58415220
Iteration 140, loss = 11599.94270016
Iteration 141, loss = 11562.79166333
Iteration 142, loss = 11525.03071324
Iteration 143, loss = 11488.33057512
Iteration 144, loss = 11453.41212686
Iteration 145, loss = 11412.57498847
Iteration 146, loss = 11375.30558196
Iteration 147, loss = 11338.99558615
Iteration 148, loss = 11303.75491128
Iteration 149, loss = 11266.65259810
Iteration 150, loss = 11229.55400984
Iteration 151, loss = 11197.05861844
Iteration 152, loss = 11157.49134058
Iteration 153, loss = 11121.07957550
Iteration 154, loss = 11086.28107155
Iteration 155, loss = 11051.78818476
Iteration 156, loss = 11017.03120867
Iteration 157, loss = 10978.80902490
Iteration 158, loss = 10944.14214702
Iteration 159, loss = 10908.87974734
Iteration 160, loss = 10874.65074298
Iteration 161, loss = 10839.38286606
Iteration 162, loss = 10804.90988855
Iteration 163, loss = 10770.96373197
Iteration 164, loss = 10735.61630197
Iteration 165, loss = 10702.22753616
Iteration 166, loss = 10668.47287380
Iteration 167, loss = 10634.36081070
Iteration 168, loss = 10599.78798304
Iteration 169, loss = 10566.52037998
Iteration 170, loss = 10532.87330799
Iteration 171, loss = 10499.63602909
Iteration 172, loss = 10467.65744912
Iteration 173, loss = 10434.07663353
Iteration 174, loss = 10400.68239553
Iteration 175, loss = 10369.67328185
Iteration 176, loss = 10337.81265447
Iteration 177, loss = 10302.79462284
Iteration 178, loss = 10269.40595021
Iteration 179, loss = 10238.78454640
Iteration 180, loss = 10206.57380494
Iteration 181, loss = 10173.69357486
Iteration 182, loss = 10141.53469889
Iteration 183, loss = 10109.89252525
Iteration 184, loss = 10077.28380412
Iteration 185, loss = 10047.95376351
Iteration 186, loss = 10014.66360353
Iteration 187, loss = 9985.36102415
Iteration 188, loss = 9953.07477414
Iteration 189, loss = 9920.49066656
Iteration 190, loss = 9890.96040532
Iteration 191, loss = 9857.60739258
Iteration 192, loss = 9826.85346676
Iteration 193, loss = 9797.20618310
Iteration 194, loss = 9768.75244007
Iteration 195, loss = 9740.34967275
Iteration 196, loss = 9708.50838742
Iteration 197, loss = 9676.84442300
Iteration 198, loss = 9646.14024785
Iteration 199, loss = 9617.78328894
Iteration 200, loss = 9587.64955671
Iteration 201, loss = 9558.17254330
Iteration 202, loss = 9531.11797861
Iteration 203, loss = 9501.78689279
Iteration 204, loss = 9469.97065945
Iteration 205, loss = 9441.40894942
Iteration 206, loss = 9412.90656809
Iteration 207, loss = 9382.97187480
Iteration 208, loss = 9355.43008856
Iteration 209, loss = 9327.67190907
Iteration 210, loss = 9305.27654762
Iteration 211, loss = 9273.55631401
Iteration 212, loss = 9243.85187667
Iteration 213, loss = 9214.85060198
Iteration 214, loss = 9190.03460989
Iteration 215, loss = 9162.91387724
Iteration 216, loss = 9133.72609810
Iteration 217, loss = 9105.00604991
Iteration 218, loss = 9077.90002115
Iteration 219, loss = 9049.68734802
Iteration 220, loss = 9018.81291814
Iteration 221, loss = 8987.95605816
Iteration 222, loss = 8960.95511829
Iteration 223, loss = 8929.59995636
Iteration 224, loss = 8901.68170778
Iteration 225, loss = 8876.61245290
Iteration 226, loss = 8845.25167424
Iteration 227, loss = 8817.29901948
Iteration 228, loss = 8791.94748664
Iteration 229, loss = 8766.47168831
Iteration 230, loss = 8736.94511645
Iteration 231, loss = 8711.43308223
Iteration 232, loss = 8682.49382662
Iteration 233, loss = 8657.32509136
Iteration 234, loss = 8633.56950991
Iteration 235, loss = 8603.37949783
Iteration 236, loss = 8580.28738297
Iteration 237, loss = 8554.35720142
Iteration 238, loss = 8526.75281161
Iteration 239, loss = 8501.65545054
Iteration 240, loss = 8475.86677546
Iteration 241, loss = 8448.98727914
Iteration 242, loss = 8421.27976947
Iteration 243, loss = 8396.88346557
Iteration 244, loss = 8368.76771299
Iteration 245, loss = 8345.95474948
Iteration 246, loss = 8322.05455732
Iteration 247, loss = 8295.97385298
Iteration 248, loss = 8270.57189776
Iteration 249, loss = 8246.91584161
Iteration 250, loss = 8221.14213750
Iteration 251, loss = 8196.25469974
Iteration 252, loss = 8176.41917597
Iteration 253, loss = 8154.46944143
Iteration 254, loss = 8127.44365873
Iteration 255, loss = 8103.71241281
Iteration 256, loss = 8079.04723075
Iteration 257, loss = 8054.77565918
Iteration 258, loss = 8030.05307848
Iteration 259, loss = 8006.82096975
Iteration 260, loss = 7983.97247811
Iteration 261, loss = 7959.73544243
Iteration 262, loss = 7938.10753062
Iteration 263, loss = 7916.56621957
Iteration 264, loss = 7893.80141778
Iteration 265, loss = 7867.97945264
Iteration 266, loss = 7843.93819050
Iteration 267, loss = 7821.64577379
Iteration 268, loss = 7802.32651032
Iteration 269, loss = 7777.49115754
Iteration 270, loss = 7753.74544211
Iteration 271, loss = 7735.38041038
Iteration 272, loss = 7713.11875240
Iteration 273, loss = 7686.60338994
Iteration 274, loss = 7670.02046816
Iteration 275, loss = 7646.13599094
Iteration 276, loss = 7621.42566312
Iteration 277, loss = 7601.18351952
Iteration 278, loss = 7581.45013103
Iteration 279, loss = 7564.63752641
Iteration 280, loss = 7537.92241124
Iteration 281, loss = 7515.14792335
Iteration 282, loss = 7492.27419097
Iteration 283, loss = 7473.36875008
Iteration 284, loss = 7450.96093963
Iteration 285, loss = 7432.51065950
Iteration 286, loss = 7410.71176912
Iteration 287, loss = 7389.79410180
Iteration 288, loss = 7370.05564328
Iteration 289, loss = 7347.24922655
Iteration 290, loss = 7325.44413525
Iteration 291, loss = 7309.87542895
Iteration 292, loss = 7284.32385450
Iteration 293, loss = 7268.17214752
Iteration 294, loss = 7252.49278486
Iteration 295, loss = 7222.64643158
Iteration 296, loss = 7204.47729752
Iteration 297, loss = 7180.45922369
Iteration 298, loss = 7164.89412306
Iteration 299, loss = 7148.04030999
Iteration 300, loss = 7122.76527959
Iteration 301, loss = 7102.36027293
Iteration 302, loss = 7084.95730421
Iteration 303, loss = 7068.74782653
Iteration 304, loss = 7050.28742808
Iteration 305, loss = 7026.06773339
Iteration 306, loss = 7005.78300823
Iteration 307, loss = 6987.00005727
Iteration 308, loss = 6967.30120393
Iteration 309, loss = 6950.27314409
Iteration 310, loss = 6933.02465267
Iteration 311, loss = 6916.40938337
Iteration 312, loss = 6892.89870214
Iteration 313, loss = 6873.13180859
Iteration 314, loss = 6857.14675155
Iteration 315, loss = 6843.21822314
Iteration 316, loss = 6825.08239001
Iteration 317, loss = 6803.60462199
Iteration 318, loss = 6784.35967044
Iteration 319, loss = 6766.94466797
Iteration 320, loss = 6746.88851914
Iteration 321, loss = 6733.07039004
Iteration 322, loss = 6710.96107585
Iteration 323, loss = 6699.78295188
Iteration 324, loss = 6682.60467166
Iteration 325, loss = 6663.76130209
Iteration 326, loss = 6648.55170828
Iteration 327, loss = 6631.90022484
Iteration 328, loss = 6613.04421026
Iteration 329, loss = 6597.29705735
Iteration 330, loss = 6574.34037146
Iteration 331, loss = 6562.93761492
Iteration 332, loss = 6550.98433119
Iteration 333, loss = 6534.93663038
Iteration 334, loss = 6512.84035169
Iteration 335, loss = 6500.46042053
Iteration 336, loss = 6479.65820534
Iteration 337, loss = 6462.29910191
Iteration 338, loss = 6448.28566136
Iteration 339, loss = 6432.39441402
Iteration 340, loss = 6427.01234442
Iteration 341, loss = 6403.50991257
Iteration 342, loss = 6389.93313926
Iteration 343, loss = 6378.61299834
Iteration 344, loss = 6355.98289962
Iteration 345, loss = 6339.67982669
Iteration 346, loss = 6328.70923080
Iteration 347, loss = 6313.08234182
Iteration 348, loss = 6301.99073816
Iteration 349, loss = 6288.84063962
Iteration 350, loss = 6270.06748470
Iteration 351, loss = 6254.81608360
Iteration 352, loss = 6243.70781865
Iteration 353, loss = 6231.22445188
Iteration 354, loss = 6221.70906436
Iteration 355, loss = 6203.60181813
Iteration 356, loss = 6185.09091585
Iteration 357, loss = 6173.57472884
Iteration 358, loss = 6163.35153781
Iteration 359, loss = 6146.27791962
Iteration 360, loss = 6131.39785705
Iteration 361, loss = 6120.42244405
Iteration 362, loss = 6111.38964660
Iteration 363, loss = 6095.11523483
Iteration 364, loss = 6080.91637371
Iteration 365, loss = 6072.19788163
Iteration 366, loss = 6054.22647781
Iteration 367, loss = 6041.41317669
Iteration 368, loss = 6034.28427442
Iteration 369, loss = 6021.81397678
Iteration 370, loss = 6005.92811612
Iteration 371, loss = 5996.65765404
Iteration 372, loss = 5980.79692771
Iteration 373, loss = 5975.63207145
Iteration 374, loss = 5957.32573981
Iteration 375, loss = 5946.93587457
Iteration 376, loss = 5932.46277812
Iteration 377, loss = 5928.26365371
Iteration 378, loss = 5910.35130960
Iteration 379, loss = 5899.58629945
Iteration 380, loss = 5888.24973102
Iteration 381, loss = 5877.68163641
Iteration 382, loss = 5863.92566449
Iteration 383, loss = 5854.08336176
Iteration 384, loss = 5846.36919675
Iteration 385, loss = 5832.21241879
Iteration 386, loss = 5823.17821187
Iteration 387, loss = 5811.84487078
Iteration 388, loss = 5803.71372804
Iteration 389, loss = 5799.17260477
Iteration 390, loss = 5776.26464730
Iteration 391, loss = 5778.96738701
Iteration 392, loss = 5764.93747898
Iteration 393, loss = 5748.84070340
Iteration 394, loss = 5734.85306233
Iteration 395, loss = 5726.96468337
Iteration 396, loss = 5715.26579209
Iteration 397, loss = 5702.56632253
Iteration 398, loss = 5693.58524010
Iteration 399, loss = 5687.19517857
Iteration 400, loss = 5675.83525769
Iteration 401, loss = 5664.68172166
Iteration 402, loss = 5659.89479375
Iteration 403, loss = 5656.47842737
Iteration 404, loss = 5638.60056727
Iteration 405, loss = 5632.05458272
Iteration 406, loss = 5627.42156039
Iteration 407, loss = 5609.71945883
Iteration 408, loss = 5606.54225475
Iteration 409, loss = 5597.33819056
Iteration 410, loss = 5591.90225758
Iteration 411, loss = 5584.50686110
Iteration 412, loss = 5579.71394397
Iteration 413, loss = 5564.87411456
Iteration 414, loss = 5552.79768732
Iteration 415, loss = 5543.25245125
Iteration 416, loss = 5537.69809901
Iteration 417, loss = 5524.27500296
Iteration 418, loss = 5515.67072453
Iteration 419, loss = 5515.51189955
Iteration 420, loss = 5499.34695842
Iteration 421, loss = 5501.39660624
Iteration 422, loss = 5496.09131909
Iteration 423, loss = 5492.36486292
Iteration 424, loss = 5476.80540849
Iteration 425, loss = 5466.69255424
Iteration 426, loss = 5463.77608174
Iteration 427, loss = 5452.33895505
Iteration 428, loss = 5453.11607775
Iteration 429, loss = 5445.77744724
Iteration 430, loss = 5436.33868806
Iteration 431, loss = 5435.04903992
Iteration 432, loss = 5411.65802127
Iteration 433, loss = 5406.16453638
Iteration 434, loss = 5407.80603949
Iteration 435, loss = 5397.77913794
Iteration 436, loss = 5387.56243707
Iteration 437, loss = 5381.33679885
Iteration 438, loss = 5377.04353714
Iteration 439, loss = 5366.74494487
Iteration 440, loss = 5359.08576819
Iteration 441, loss = 5351.57233593
Iteration 442, loss = 5347.37800114
Iteration 443, loss = 5350.65110028
Iteration 444, loss = 5342.01990331
Iteration 445, loss = 5323.86181793
Iteration 446, loss = 5322.26595070
Iteration 447, loss = 5325.09013846
Iteration 448, loss = 5309.40350484
Iteration 449, loss = 5310.00114507
Iteration 450, loss = 5299.06198243
Iteration 451, loss = 5289.71395695
Iteration 452, loss = 5283.21820146
Iteration 453, loss = 5279.74851223
Iteration 454, loss = 5276.14747896
Iteration 455, loss = 5274.07754218
Iteration 456, loss = 5266.20366126
Iteration 457, loss = 5256.90671027
Iteration 458, loss = 5257.93013466
Iteration 459, loss = 5247.14873652
Iteration 460, loss = 5237.95135908
Iteration 461, loss = 5231.16878244
Iteration 462, loss = 5227.60046762
Iteration 463, loss = 5222.84284961
Iteration 464, loss = 5211.45870127
Iteration 465, loss = 5205.12569961
Iteration 466, loss = 5198.96914587
Iteration 467, loss = 5191.53669434
Iteration 468, loss = 5183.14195742
Iteration 469, loss = 5184.38995725
Iteration 470, loss = 5190.42782002
Iteration 471, loss = 5169.53124548
Iteration 472, loss = 5158.10320647
Iteration 473, loss = 5163.47062849
Iteration 474, loss = 5147.02963431
Iteration 475, loss = 5145.99776507
Iteration 476, loss = 5136.25912317
Iteration 477, loss = 5138.24184432
Iteration 478, loss = 5123.95051710
Iteration 479, loss = 5119.31632608
Iteration 480, loss = 5117.68588168
Iteration 481, loss = 5105.86511012
Iteration 482, loss = 5111.22211975
Iteration 483, loss = 5107.29582285
Iteration 484, loss = 5089.28323666
Iteration 485, loss = 5100.11034914
Iteration 486, loss = 5088.85921550
Iteration 487, loss = 5083.90800147
Iteration 488, loss = 5075.56359647
Iteration 489, loss = 5081.94552079
Iteration 490, loss = 5070.71532739
Iteration 491, loss = 5056.57104148
Iteration 492, loss = 5060.95430464
Iteration 493, loss = 5053.99863837
Iteration 494, loss = 5044.86508830
Iteration 495, loss = 5042.89152438
Iteration 496, loss = 5057.98522756
Iteration 497, loss = 5039.52974856
Iteration 498, loss = 5025.50899157
Iteration 499, loss = 5023.47589617
Iteration 500, loss = 5018.42617864
Iteration 501, loss = 5019.58764947
Iteration 502, loss = 5009.65099229
Iteration 503, loss = 5005.19430164
Iteration 504, loss = 5008.43279838
Iteration 505, loss = 4996.64508636
Iteration 506, loss = 4998.05270354
Iteration 507, loss = 5005.14283302
Iteration 508, loss = 4990.79989001
Iteration 509, loss = 4990.99521018
Iteration 510, loss = 4979.13142974
Iteration 511, loss = 4970.63427103
Iteration 512, loss = 4979.31683302
Iteration 513, loss = 4972.64931914
Iteration 514, loss = 4968.62957504
Iteration 515, loss = 4960.85099145
Iteration 516, loss = 4972.81770717
Iteration 517, loss = 4960.86392913
Iteration 518, loss = 4953.24353559
Iteration 519, loss = 4949.40435304
Iteration 520, loss = 4949.30638052
Iteration 521, loss = 4945.92537671
Iteration 522, loss = 4942.09269930
Iteration 523, loss = 4937.18005810
Iteration 524, loss = 4954.75412220
Iteration 525, loss = 4948.33844082
Iteration 526, loss = 4937.72393878
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
{'ann': MLPRegressor(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(10, 10), learning_rate='adaptive',
       learning_rate_init=0.001, max_iter=10000, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=None,
       shuffle=True, solver='adam', tol=1e-30, validation_fraction=0.1,
       verbose=True, warm_start=False), 'error': 0.61725383498934572, 'fit': 0.47303076584786746, 'time': 10.579999999999998}
