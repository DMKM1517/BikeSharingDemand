{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, TimeDistributed\n",
    "from keras.layers import LSTM, SimpleRNN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy as scipy\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.regularizers import l1, activity_l1\n",
    "from keras.regularizers import l2, activity_l2\n",
    "from sklearn import preprocessing  \n",
    "#from datetime \n",
    "import datetime \n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "import pickle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../train.csv\")\n",
    "df_test = pd.read_csv(\"../test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train['month'] = pd.DatetimeIndex(df_train.datetime).month\n",
    "df_train['day'] = pd.DatetimeIndex(df_train.datetime).dayofweek\n",
    "df_train['hour'] = pd.DatetimeIndex(df_train.datetime).hour\n",
    "df_train['year'] = pd.DatetimeIndex(df_train.datetime).year\n",
    "\n",
    "df_test['month'] = pd.DatetimeIndex(df_test.datetime).month\n",
    "df_test['day'] = pd.DatetimeIndex(df_test.datetime).dayofweek\n",
    "df_test['hour'] = pd.DatetimeIndex(df_test.datetime).hour\n",
    "df_test['year'] = pd.DatetimeIndex(df_test.datetime).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine train and test data into one df\n",
    "df = df_train.append(df_test)\n",
    "\n",
    "# lowercase column names\n",
    "#df.columns = map(str.lower, df.columns)\n",
    "\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df.index = df['datetime']\n",
    "\n",
    "#df['month'] = pd.DatetimeIndex(df.datetime).month\n",
    "#df['day'] = pd.DatetimeIndex(df.datetime).dayofweek\n",
    "#df['hour'] = pd.DatetimeIndex(df.datetime).hour\n",
    "#df['year'] = pd.DatetimeIndex(df.datetime).year\n",
    "\n",
    "df = df.drop('datetime', axis=1)\n",
    "drange = pd.date_range('2011-01-01 00:00:00', '2012-12-31 23:00:00', freq='H')\n",
    "df = df.reindex(drange)\n",
    "\n",
    "df = df[df_train.drop(['datetime', 'casual', 'registered','count'],axis=1).columns]\n",
    "\n",
    "# parse datetime colum & add new time related columns\n",
    "#dt = pd.DatetimeIndex(df['datetime'])\n",
    "#df.set_index(dt, inplace=True)\n",
    "\n",
    "dt = pd.DatetimeIndex(df_train['datetime'])\n",
    "df_train.set_index(dt, inplace=True)\n",
    "dt = pd.DatetimeIndex(df_test['datetime'])\n",
    "df_test.set_index(dt, inplace=True)\n",
    "\n",
    "df[\"weather\"] = df[\"weather\"].interpolate(method='time').apply(np.round)\n",
    "df[\"temp\"] = df[\"temp\"].interpolate(method='time')\n",
    "df[\"atemp\"] = df[\"atemp\"].interpolate(method='time')\n",
    "df[\"humidity\"] = df[\"humidity\"].interpolate(method='time').apply(np.round)\n",
    "df[\"windspeed\"] = df[\"windspeed\"].interpolate(method='time')\n",
    "\n",
    "df[\"season\"] = df[\"season\"].interpolate(method='time').apply(np.round)\n",
    "df[\"holiday\"] = df[\"holiday\"].interpolate(method='time').apply(np.round)\n",
    "df[\"workingday\"] = df[\"workingday\"].interpolate(method='time').apply(np.round)\n",
    "\n",
    "df[\"month\"] = df[\"month\"].interpolate(method='time').apply(np.round)\n",
    "df[\"day\"] = df[\"day\"].interpolate(method='time').apply(np.round)\n",
    "df[\"hour\"] = df[\"hour\"].interpolate(method='time').apply(np.round)\n",
    "df[\"year\"] = df[\"year\"].interpolate(method='time').apply(np.round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_day(day_start):\n",
    "    day_end = day_start + pd.offsets.DateOffset(hours=23)\n",
    "    return pd.date_range(day_start, day_end, freq=\"H\")\n",
    "\n",
    "# tax day\n",
    "df_train.loc[get_day(pd.datetime(2011, 4, 15)), \"workingday\"] = 1\n",
    "df_train.loc[get_day(pd.datetime(2012, 4, 16)), \"workingday\"] = 1\n",
    "# thanksgiving friday\n",
    "df_test.loc[get_day(pd.datetime(2011, 11, 25)), \"workingday\"] = 0\n",
    "df_test.loc[get_day(pd.datetime(2012, 11, 23)), \"workingday\"] = 0\n",
    "# tax day\n",
    "df_train.loc[get_day(pd.datetime(2011, 4, 15)), \"holiday\"] = 0\n",
    "df_train.loc[get_day(pd.datetime(2012, 4, 16)), \"holiday\"] = 0\n",
    "\n",
    "# thanksgiving friday\n",
    "df_test.loc[get_day(pd.datetime(2011, 11, 25)), \"holiday\"] = 1\n",
    "df_test.loc[get_day(pd.datetime(2012, 11, 23)), \"holiday\"] = 1\n",
    "\n",
    "#storms\n",
    "df_test.loc[get_day(pd.datetime(2012, 5, 21)), \"holiday\"] = 1\n",
    "#tornado\n",
    "df_train.loc[get_day(pd.datetime(2012, 6, 1)), \"holiday\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_train = df_train.drop(['datetime','casual','registered','count'], axis = 1)\n",
    "#df_train_train = df_train_train.drop(['season','workingday','weather','holiday'], axis = 1)\n",
    "df_test_train = df_train['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler.fit(df_test_train.values.astype('float32').reshape(-1,1))\n",
    "scalerx = MinMaxScaler(feature_range=(-1, 1))\n",
    "scalerx.fit(df_train_train.values.astype('float32'))\n",
    "df = df.apply(lambda x: scalerx.fit_transform(x.reshape(-1,1)).reshape(1,-1)[0] )\n",
    "timesteps = 2\n",
    "def preprocess_nn(datax, datay):\n",
    "    indexx = pd.DatetimeIndex(datax['datetime']).day \n",
    "    testindex = np.logical_or(np.logical_or(indexx == 19,indexx == 18),np.logical_or(indexx == 17, indexx == 16))\n",
    "    datax[datax.drop('datetime',axis=1).columns] = datax[datax.drop('datetime',axis=1).columns].apply(lambda x: scalerx.fit_transform(x.reshape(-1,1)).reshape(1,-1)[0] )\n",
    "    datay['count'] = scaler.transform(datay['count'])\n",
    "    #train_Y_n = scaler.transform(datay.reshape(-1,1))\n",
    "    train_x = datax[~testindex]#.astype('float32')\n",
    "    train_y = datay[~testindex]#.astype('float32')\n",
    "    test_x = datax[testindex]#.astype('float32')\n",
    "    test_y = datay[testindex]#.astype('float32')\n",
    "    return train_x, train_y, test_x, test_y\n",
    "#def preshape_nn(data_x):\n",
    "#    data_x = numpy.reshape(data_x, (data_x.shape[0]/timesteps, timesteps, data_x.shape[1]))\n",
    "#    return data_x\n",
    "def preshape_nn(data_set, look_back = timesteps):\n",
    "    out1= []\n",
    "    for val in range(len(data_set) - look_back + 1):\n",
    "        end1 = datetime.datetime.strptime(data_set['datetime'].values[look_back - 1 + val],\"%Y-%m-%d %H:%M:%S\" )\n",
    "        timesteps_array = []\n",
    "        for it in range(look_back): \n",
    "            timesteps_array.append(np.datetime64(end1))\n",
    "            end1 = end1 - datetime.timedelta(hours = 1)\n",
    "        timesteps_array = timesteps_array[::-1]\n",
    "        timestep_unit = df.loc[timesteps_array].values.astype('float32')\n",
    "        #if val == 0:\n",
    "         #   out = timestep_unit\n",
    "        #    print out.shape\n",
    "        #timestep_unit = np.array(timestep_unit,type('float32'))\n",
    "        #out = np.stack([out,timestep_unit], axis=0)\n",
    "        out1.append(timestep_unit)\n",
    "    datax = np.stack(out1, axis=0)\n",
    "    return datax\n",
    "def preshape_nn_y(data_set, look_back = timesteps ):\n",
    "    return data_set['count'].values[look_back - 1:]\n",
    "def learn_nn(train_x, train_y, test_x, test_y, string):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM, Dense\n",
    "    import numpy as np    \n",
    "\n",
    "    data_dim = train_x.shape[-1] -1\n",
    "    \n",
    "    #LSTM\n",
    "    # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(100))\n",
    "    model.add(LSTM(100, return_sequences=True,\n",
    "                   input_shape=(timesteps, data_dim)))#, W_regularizer=l2(0.00001)))\n",
    "    model.add(LSTM(100, return_sequences=True))#, W_regularizer=l2(0.00001)))\n",
    "    model.add(LSTM(10))  \n",
    "    model.add(Dense(1, activation='linear'))    \n",
    "    \n",
    "    \n",
    "    #####\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer='rmsprop')\n",
    "    \n",
    "    x_train = preshape_nn(train_x)\n",
    "    y_train  = preshape_nn_y(train_y) #scaler.transform(train_y)\n",
    "\n",
    "\n",
    "    x_val = preshape_nn(test_x)\n",
    "    y_val = preshape_nn_y(test_y)#scaler.transform(test_y)\n",
    "    # generate dummy validation data\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    print now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=100, nb_epoch=100, verbose = 0,\n",
    "              validation_data=(x_val, y_val),\n",
    "              callbacks=[TensorBoard(log_dir='logs1/' + now.strftime(\"%Y%m%d-%H%M%S\") + string + \"/\")])\n",
    "    model.save(\"models/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \".h5\")\n",
    "    return model\n",
    "def model_eval_nn(model, prepro):\n",
    "    trainScore = math.sqrt(mean_squared_error(scaler.inverse_transform(model.predict(preshape_nn(prepro[0]))), scaler.inverse_transform(preshape_nn_y(prepro[1]))))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore =  math.sqrt(mean_squared_error(scaler.inverse_transform(model.predict(preshape_nn(prepro[2]))), scaler.inverse_transform(preshape_nn_y(prepro[3]))))\n",
    "    print('Test Score: %.2f RMSE' % (testScore)) \n",
    "    plt.plot(scaler.inverse_transform(model.predict(preshape_nn(prepro[2]))) - scaler.inverse_transform(preshape_nn_y(prepro[3])),'.')\n",
    "def get_rmsle(model, prepro):\n",
    "    diff = pd.DataFrame(np.log(scaler.inverse_transform(model.predict(preshape_nn(prepro[2]))).astype('float32') + 1) - np.log(scaler.inverse_transform(preshape_nn_y(prepro[3])).astype('float32') + 1))\n",
    "    mean_error = np.square(diff).mean()\n",
    "    return np.sqrt(mean_error).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:359: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/root/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "datax = df_train.drop(['casual','registered','count'], axis = 1)\n",
    "datay = df_train[['datetime','count']]\n",
    "pre1 = preprocess_nn(datax,datay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20170113-191611\n"
     ]
    }
   ],
   "source": [
    "m1 = learn_nn(pre1[0],pre1[1],pre1[2],pre1[3],'MG')\n",
    "model_eval_nn(m1,pre1)\n",
    "get_rmsle(m1,pre1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test_test = df_test#.drop(['datetime'], axis = 1)\n",
    "#df_test_test = df_test_test.drop(['season','workingday','weather','holiday'], axis = 1)\n",
    "#df_test_t = df_test_test.apply(lambda x: scalerx.fit_transform(x.reshape(-1,1)).reshape(1,-1)[0] )\n",
    "df_test_test[df_test_test.drop('datetime',axis=1).columns] = df_test_test[df_test_test.drop('datetime',axis=1).columns].apply(lambda x: scalerx.fit_transform(x.reshape(-1,1)).reshape(1,-1)[0] )\n",
    "df_test_r = preshape_nn(df_test_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = m1.predict(df_test_r)\n",
    "out = scaler.inverse_transform(pred).astype(int)\n",
    "out[out<0] = 0\n",
    "out = out[:,0]\n",
    "pred = pd.DataFrame({'datetime': df_test['datetime'],'count': out})\n",
    "pred = pred[['datetime','count']]\n",
    "pred.to_csv(\"pred.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(scaler.inverse_transform(m1.predict(preshape_nn(pre1[0])))-scaler.inverse_transform(pre1[1]),'.')\n",
    "print scipy.stats.pearsonr(scaler.inverse_transform(m1.predict(preshape_nn(pre1[0]))),scaler.inverse_transform(pre1[1]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexx = pd.DatetimeIndex(df_train['datetime'][df_train['year'] == 2011]).day \n",
    "testindex = np.logical_or(np.logical_or(indexx == 19,indexx == 18),np.logical_or(indexx == 17, indexx == 16))\n",
    "pre2 = preprocess_nn(df_train_train[df_train['year'] == 2011],df_train['casual'][df_train['year'] == 2011])\n",
    "m2 = learn_nn(pre2[0],pre2[1],pre2[2],pre2[3],'MAC')\n",
    "model_eval_nn(m2,pre2)\n",
    "get_rmsle(m2,pre2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre3 = preprocess_nn(df_train_train[df_train['year'] == 2011],df_train['registered'][df_train['year'] == 2011])\n",
    "m3 = learn_nn(pre3[0],pre3[1],pre3[2],pre3[3],'MAR')\n",
    "model_eval_nn(m3,pre3)\n",
    "get_rmsle(m3,pre3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexx = pd.DatetimeIndex(df_train['datetime'][df_train['year'] == 2012]).day \n",
    "testindex = np.logical_or(np.logical_or(indexx == 19,indexx == 18),np.logical_or(indexx == 17, indexx == 16))\n",
    "pre4 = preprocess_nn(df_train_train[df_train['year'] == 2012],df_train['casual'][df_train['year'] == 2012])\n",
    "m4 = learn_nn(pre4[0],pre4[1],pre4[2],pre4[3],'MBC')\n",
    "model_eval_nn(m4,pre4)\n",
    "get_rmsle(m4,pre4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre5 = preprocess_nn(df_train_train[df_train['year'] == 2012],df_train['registered'][df_train['year'] == 2012])\n",
    "m5 = learn_nn(pre5[0],pre5[1],pre5[2],pre5[3],'MBR')\n",
    "model_eval_nn(m5,pre5)\n",
    "get_rmsle(m5,pre5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out2 = scaler.inverse_transform(m2.predict(preshape_nn(preprocessing.scale(df_test_test[df_test['year']==2011].values.astype('float32'))))).astype(int)\n",
    "out3 = scaler.inverse_transform(m3.predict(preshape_nn(preprocessing.scale(df_test_test[df_test['year']==2011].values.astype('float32'))))).astype(int)\n",
    "out4 = scaler.inverse_transform(m4.predict(preshape_nn(preprocessing.scale(df_test_test[df_test['year']==2012].values.astype('float32'))))).astype(int)\n",
    "out5 = scaler.inverse_transform(m5.predict(preshape_nn(preprocessing.scale(df_test_test[df_test['year']==2012].values.astype('float32'))))).astype(int)\n",
    "out2[out2<0] = 0\n",
    "out3[out3<0] = 0\n",
    "out4[out4<0] = 0\n",
    "out5[out5<0] = 0\n",
    "out2 = out2[:,0]\n",
    "out3 = out3[:,0]\n",
    "out4 = out4[:,0]\n",
    "out5 = out5[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outb = np.concatenate((np.array(out2 + out3),np.array(out4 + out5)), axis = 0)\n",
    "pred = pd.DataFrame({'datetime': df_test['datetime'],'count': outb})\n",
    "pred = pred[['datetime','count']]\n",
    "pred.to_csv(\"pred2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(outb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(out-outb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 7s - loss: 11.4680 - acc: 0.0920 - val_loss: 11.5496 - val_acc: 0.0900\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 3s - loss: 11.4662 - acc: 0.1090 - val_loss: 11.5488 - val_acc: 0.0900\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 4s - loss: 11.4660 - acc: 0.1100 - val_loss: 11.5489 - val_acc: 0.0700\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 4s - loss: 11.4655 - acc: 0.1270 - val_loss: 11.5481 - val_acc: 0.0600\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 3s - loss: 11.4652 - acc: 0.1280 - val_loss: 11.5505 - val_acc: 0.0700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x113e2fc50>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "data_dim = 16\n",
    "timesteps = 2\n",
    "nb_classes = 10\n",
    "\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True,\n",
    "               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32))  # return a single vector of dimension 32\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# generate dummy training data\n",
    "x_train = np.random.random((1000, timesteps, data_dim))\n",
    "y_train = np.random.random((1000, nb_classes))\n",
    "\n",
    "# generate dummy validation data\n",
    "x_val = np.random.random((100, timesteps, data_dim))\n",
    "y_val = np.random.random((100, nb_classes))\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64, nb_epoch=5,\n",
    "          validation_data=(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
